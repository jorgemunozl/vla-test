{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fourth Test\n",
        "\n",
        "This time we visualize another more probable tokens, tweaking the temperature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "9afbca9880cb4969bc13764f5b6d715e",
            "671cbd380f8f47abbd52c56bcf3f3cec",
            "7288cec4470a4b3396716326fe9c16e1",
            "03ced8628d9c422ba31cd3deefbffa3e",
            "330a744c61864587b4f25a96fcdc9e46",
            "d2634243dfee496da2e433ff99f1ba08",
            "6b3cb969da214b55a601bb2632065d5b",
            "c08646bdd1f446ffa06e62187f413fa3",
            "09d24dddf93b41bb9769916ea031e4b4",
            "dcf030c1e28e4d939eea04ec182d66c3",
            "86d67298962a437886c82fec2e9095b8",
            "f4d781486ec743fd9a1eb0e26d89014a",
            "4d3c71094f00441bbf7f937df6982a4b",
            "c8770dacca394e7fb6597aaa58ecdd41",
            "5aaeab307dd247aaac2e36781eef85dc",
            "0c7a973c53ac41cb89eb5873fb5139a5",
            "e1db3569f909456ca1ed63f2f2c9d32e",
            "e95b677548c542f4adcee867dbc68d3e",
            "f23561568c824c9fb4874ebe19a972bd",
            "e201b77d53c740e8aac3549970715737"
          ]
        },
        "id": "0aM4ig9gOJ3D",
        "outputId": "167891e7-79e6-4eae-dc94-8e3cf09de920"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9afbca9880cb4969bc13764f5b6d715e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcqFen2xOUZh",
        "outputId": "8b93e364-8616-4637-fc09-6274869d4c0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m109 packages\u001b[0m \u001b[2min 29.90s\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m29 packages\u001b[0m \u001b[2min 19.27s\u001b[0m\u001b[0m\n",
            "\u001b[2mUninstalled \u001b[1m11 packages\u001b[0m \u001b[2min 1.04s\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m29 packages\u001b[0m \u001b[2min 248ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mav\u001b[0m\u001b[2m==15.1.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mdeepdiff\u001b[0m\u001b[2m==8.6.1\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mdiffusers\u001b[0m\u001b[2m==0.36.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mdiffusers\u001b[0m\u001b[2m==0.35.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mdraccus\u001b[0m\u001b[2m==0.10.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mevdev\u001b[0m\u001b[2m==1.9.2\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==0.36.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==0.35.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1minquirerpy\u001b[0m\u001b[2m==0.3.4\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjsonlines\u001b[0m\u001b[2m==4.0.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlerobot\u001b[0m\u001b[2m==0.4.3 (from git+https://github.com/huggingface/lerobot.git@fc296548cb6438b3036d046b43fe91951c87ea9a)\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmergedeep\u001b[0m\u001b[2m==1.3.4\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmypy-extensions\u001b[0m\u001b[2m==1.1.0\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.10.2.21\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.5.1.17\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnvidia-cusparselt-cu12\u001b[0m\u001b[2m==0.7.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparselt-cu12\u001b[0m\u001b[2m==0.6.3\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnvidia-nccl-cu12\u001b[0m\u001b[2m==2.27.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-nccl-cu12\u001b[0m\u001b[2m==2.26.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1morderly-set\u001b[0m\u001b[2m==5.5.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpfzy\u001b[0m\u001b[2m==0.3.4\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpynput\u001b[0m\u001b[2m==1.8.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpyserial\u001b[0m\u001b[2m==3.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpython-xlib\u001b[0m\u001b[2m==0.33\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpyyaml-include\u001b[0m\u001b[2m==1.4.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mrerun-sdk\u001b[0m\u001b[2m==0.26.2\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.22.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.21.4\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.9.0+cu126\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.7.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtorchcodec\u001b[0m\u001b[2m==0.5\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.24.0+cu126\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.22.1\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.57.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.53.3 (from git+https://github.com/huggingface/transformers.git@dcddb970176382c0fcf4521b0c0e6fc15894dfe0)\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==3.5.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==3.3.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtyping-inspect\u001b[0m\u001b[2m==0.9.0\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mwandb\u001b[0m\u001b[2m==0.23.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mwandb\u001b[0m\u001b[2m==0.21.4\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!uv pip install \"git+https://github.com/huggingface/transformers.git@fix/lerobot_openpi\" \"lerobot @ git+https://github.com/huggingface/lerobot.git\" opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_OY5jHpoOY9S"
      },
      "outputs": [],
      "source": [
        "import builtins\n",
        "import logging\n",
        "import math\n",
        "from collections import deque\n",
        "from pathlib import Path\n",
        "from typing import TYPE_CHECKING, Literal, TypedDict\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F  # noqa: N812\n",
        "from torch import Tensor, nn\n",
        "from typing_extensions import Unpack\n",
        "\n",
        "from lerobot.utils.import_utils import _transformers_available\n",
        "\n",
        "# Conditional import for type checking and lazy loading\n",
        "if TYPE_CHECKING or _transformers_available:\n",
        "    from transformers.models.auto import CONFIG_MAPPING\n",
        "    from transformers.models.gemma import modeling_gemma\n",
        "    from transformers.models.gemma.modeling_gemma import GemmaForCausalLM\n",
        "    from transformers.models.paligemma.modeling_paligemma import (\n",
        "        PaliGemmaForConditionalGeneration,\n",
        "    )\n",
        "else:\n",
        "    CONFIG_MAPPING = None\n",
        "    modeling_gemma = None\n",
        "    GemmaForCausalLM = None\n",
        "    PaliGemmaForConditionalGeneration = None\n",
        "\n",
        "from lerobot.configs.policies import PreTrainedConfig\n",
        "from lerobot.policies.pi05.configuration_pi05 import PI05Config\n",
        "from lerobot.policies.pretrained import PreTrainedPolicy, T\n",
        "from lerobot.policies.rtc.modeling_rtc import RTCProcessor\n",
        "from lerobot.utils.constants import (\n",
        "    ACTION,\n",
        "    OBS_LANGUAGE_ATTENTION_MASK,\n",
        "    OBS_LANGUAGE_TOKENS,\n",
        "    OPENPI_ATTENTION_MASK_VALUE,\n",
        ")\n",
        "\n",
        "\n",
        "class ActionSelectKwargs(TypedDict, total=False):\n",
        "    inference_delay: int | None\n",
        "    prev_chunk_left_over: Tensor | None\n",
        "    execution_horizon: int | None\n",
        "\n",
        "\n",
        "def get_safe_dtype(target_dtype, device_type):\n",
        "    \"\"\"Get a safe dtype for the given device type.\"\"\"\n",
        "    if device_type == \"mps\" and target_dtype == torch.float64:\n",
        "        return torch.float32\n",
        "    if device_type == \"cpu\":\n",
        "        # CPU doesn't support bfloat16, use float32 instead\n",
        "        if target_dtype == torch.bfloat16:\n",
        "            return torch.float32\n",
        "        if target_dtype == torch.float64:\n",
        "            return torch.float64\n",
        "    return target_dtype\n",
        "\n",
        "\n",
        "# Positional Embedding for time action embedding\n",
        "def create_sinusoidal_pos_embedding(\n",
        "    time: torch.Tensor, dimension: int,\n",
        "    min_period: float, max_period: float, device=\"cpu\"\n",
        ") -> Tensor:\n",
        "    \"\"\"Computes sine-cosine positional embedding\n",
        "    vectors for scalar positions.\"\"\"\n",
        "    if dimension % 2 != 0:\n",
        "        raise ValueError(f\"dimension ({dimension}) must be divisible by 2\")\n",
        "\n",
        "    if time.ndim != 1:\n",
        "        raise ValueError(\"The time tensor is expected \" +\n",
        "                         \"to be of shape `(batch_size, )`.\")\n",
        "\n",
        "    dtype = get_safe_dtype(torch.float64, device.type)\n",
        "    fraction = torch.linspace(0.0, 1.0, dimension // 2,\n",
        "                              dtype=dtype, device=device)\n",
        "    period = min_period * (max_period / min_period) ** fraction\n",
        "\n",
        "    # Compute the outer product\n",
        "    scaling_factor = 1.0 / period * 2 * math.pi\n",
        "    sin_input = scaling_factor[None, :] * time[:, None]\n",
        "    return torch.cat([torch.sin(sin_input), torch.cos(sin_input)], dim=1)\n",
        "\n",
        "\n",
        "# For training\n",
        "def sample_beta(alpha, beta, bsize, device):\n",
        "    alpha_t = torch.as_tensor(alpha, dtype=torch.float32, device=device)\n",
        "    beta_t = torch.as_tensor(beta, dtype=torch.float32, device=device)\n",
        "    dist = torch.distributions.Beta(alpha_t, beta_t)\n",
        "    return dist.sample((bsize,))\n",
        "\n",
        "\n",
        "def make_att_2d_masks(pad_masks, att_masks):\n",
        "    \"\"\"\n",
        "    Tokens can attend to valid inputs tokens which have a cumulative mask_ar\n",
        "    smaller or equal to theirs. This way `mask_ar` int[B, N] can be used to\n",
        "    setup several types of attention, for example:\n",
        "\n",
        "      [[1 1 1 1 1 1]]: pure causal attention.\n",
        "\n",
        "      [[0 0 0 1 1 1]]: prefix-lm attention. The first 3 tokens can attend\n",
        "        between\n",
        "          themselves and the last 3 tokens have a causal attention. The first\n",
        "          entry could also be a 1 without changing behaviour.\n",
        "\n",
        "      [[1 0 1 0 1 0 0 1 0 0]]: causal attention between 4 blocks. Tokens of a\n",
        "          block can attend all previous blocks and all\n",
        "          tokens on the same block.\n",
        "\n",
        "    Args:\n",
        "        N: int - number of tokens in the sequence\n",
        "        pad_masks: bool[B, N] true if its part of the input,\n",
        "            false if padding.\n",
        "        att_masks: int[B, N] mask that's 1 where previous tokens\n",
        "            cannot depend on it and 0 where it shares the same\n",
        "            attention mask as the previous token.\n",
        "\n",
        "    Returns:\n",
        "        att_2d_masks: bool[B, N, N] 2D attention mask\n",
        "    \"\"\"\n",
        "    if att_masks.ndim != 2:\n",
        "        raise ValueError(att_masks.ndim)\n",
        "    if pad_masks.ndim != 2:\n",
        "        raise ValueError(pad_masks.ndim)\n",
        "\n",
        "    # cumsum shape: (B, N)\n",
        "    cumsum = torch.cumsum(att_masks, dim=1)\n",
        "    # att_2d_masks shape: (B, N, N)\n",
        "    att_2d_masks = cumsum[:, None, :] <= cumsum[:, :, None]\n",
        "    # pad_2d_masks shape: (B, N, N)\n",
        "    pad_2d_masks = pad_masks[:, None, :] * pad_masks[:, :, None]\n",
        "    # att_2d_masks & pad_2d_masks shape: (B, N, N)\n",
        "    return att_2d_masks & pad_2d_masks\n",
        "\n",
        "\n",
        "def pad_vector(vector, new_dim):\n",
        "    \"\"\"Pad the last dimension of a vector to new_dim with zeros.\n",
        "\n",
        "    Can be (batch_size x sequence_length x features_dimension)\n",
        "    or (batch_size x features_dimension)\n",
        "    \"\"\"\n",
        "    if vector.shape[-1] >= new_dim:\n",
        "        return vector\n",
        "    return F.pad(vector, (0, new_dim - vector.shape[-1]))\n",
        "\n",
        "\n",
        "def resize_with_pad_torch(\n",
        "    images: torch.Tensor,\n",
        "    height: int,\n",
        "    width: int,\n",
        "    mode: str = \"bilinear\",\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"PyTorch version of resize_with_pad. Resizes an image\n",
        "      to a target height and width without distortion\n",
        "    by padding with black. If the image is float32,\n",
        "      it must be in the range [-1, 1].\n",
        "\n",
        "    Args:\n",
        "        images: Tensor of shape [*b, h, w, c] or [*b, c, h, w]\n",
        "        height: Target height\n",
        "        width: Target width\n",
        "        mode: Interpolation mode ('bilinear', 'nearest', etc.)\n",
        "\n",
        "    Returns:\n",
        "        Resized and padded tensor with same shape format as input\n",
        "    \"\"\"\n",
        "    # Check if input is in channels-last format\n",
        "    # [*b, h, w, c] or channels-first [*b, c, h, w]\n",
        "    if images.shape[-1] <= 4:  # Assume channels-last format\n",
        "        channels_last = True\n",
        "        if images.dim() == 3:\n",
        "            images = images.unsqueeze(0)  # Add batch dimension\n",
        "        images = images.permute(0, 3, 1, 2)  # [b, h, w, c] -> [b, c, h, w]\n",
        "    else:\n",
        "        channels_last = False\n",
        "        if images.dim() == 3:\n",
        "            images = images.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "    batch_size, channels, cur_height, cur_width = images.shape\n",
        "\n",
        "    # Calculate resize ratio\n",
        "    ratio = max(cur_width / width, cur_height / height)\n",
        "    resized_height = int(cur_height / ratio)\n",
        "    resized_width = int(cur_width / ratio)\n",
        "\n",
        "    # Resize\n",
        "    resized_images = F.interpolate(\n",
        "        images,\n",
        "        size=(resized_height, resized_width),\n",
        "        mode=mode,\n",
        "        align_corners=False if mode == \"bilinear\" else None,\n",
        "    )\n",
        "\n",
        "    # Handle dtype-specific clipping\n",
        "    if images.dtype == torch.uint8:\n",
        "        resized_images = torch.round(resized_images)\n",
        "        resized_images = resized_images.clamp(0, 255).to(torch.uint8)\n",
        "    elif images.dtype == torch.float32:\n",
        "        resized_images = resized_images.clamp(-1.0, 1.0)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported image dtype: {images.dtype}\")\n",
        "\n",
        "    # Calculate padding\n",
        "    pad_h0, remainder_h = divmod(height - resized_height, 2)\n",
        "    pad_h1 = pad_h0 + remainder_h\n",
        "    pad_w0, remainder_w = divmod(width - resized_width, 2)\n",
        "    pad_w1 = pad_w0 + remainder_w\n",
        "\n",
        "    # Pad\n",
        "    constant_value = 0 if images.dtype == torch.uint8 else -1.0\n",
        "    padded_images = F.pad(\n",
        "        resized_images,\n",
        "        (pad_w0, pad_w1, pad_h0, pad_h1),  # left, right, top, bottom\n",
        "        mode=\"constant\",\n",
        "        value=constant_value,\n",
        "    )\n",
        "\n",
        "    # Convert back to original format if needed\n",
        "    if channels_last:\n",
        "        # [b, c, h, w] -> [b, h, w, c]\n",
        "        padded_images = padded_images.permute(0, 2, 3, 1)\n",
        "\n",
        "    return padded_images\n",
        "\n",
        "\n",
        "# Define the complete layer computation function for gradient checkpointing\n",
        "def compute_layer_complete(\n",
        "    layer_idx, inputs_embeds, attention_mask,\n",
        "    position_ids, adarms_cond, paligemma, gemma_expert\n",
        "):\n",
        "    models = [paligemma.language_model, gemma_expert.model]\n",
        "    query_states = []\n",
        "    key_states = []\n",
        "    value_states = []\n",
        "    gates = []\n",
        "    for i, hidden_states in enumerate(inputs_embeds):\n",
        "        layer = models[i].layers[layer_idx]\n",
        "        hidden_states, gate = layer.input_layernorm(hidden_states,\n",
        "                                                    cond=adarms_cond[i])\n",
        "        gates.append(gate)\n",
        "        input_shape = hidden_states.shape[:-1]\n",
        "        hidden_shape = (*input_shape, -1, layer.self_attn.head_dim)\n",
        "        query_state = layer.self_attn.q_proj(hidden_states)\n",
        "        query_state = query_state.view(hidden_shape).transpose(1, 2)\n",
        "        key_state = layer.self_attn.k_proj(hidden_states)\n",
        "        key_state = key_state.view(hidden_shape).transpose(1, 2)\n",
        "        value_state = layer.self_attn.v_proj(hidden_states)\n",
        "        value_state = value_state.view(hidden_shape).transpose(1, 2)\n",
        "        query_states.append(query_state)\n",
        "        key_states.append(key_state)\n",
        "        value_states.append(value_state)\n",
        "    # Concatenate and process attention\n",
        "    query_states = torch.cat(query_states, dim=2)\n",
        "    key_states = torch.cat(key_states, dim=2)\n",
        "    value_states = torch.cat(value_states, dim=2)\n",
        "    dummy_tensor = torch.zeros(\n",
        "        query_states.shape[0],\n",
        "        query_states.shape[2],\n",
        "        query_states.shape[-1],\n",
        "        device=query_states.device,\n",
        "        dtype=query_states.dtype,\n",
        "    )\n",
        "    cos, sin = paligemma.model.language_model.rotary_emb(dummy_tensor,\n",
        "                                                         position_ids)\n",
        "    query_states, key_states = modeling_gemma.apply_rotary_pos_emb(\n",
        "        query_states, key_states, cos, sin, unsqueeze_dim=1\n",
        "    )\n",
        "    batch_size = query_states.shape[0]\n",
        "    scaling = paligemma.language_model.layers[layer_idx].self_attn.scaling\n",
        "    # Attention computation\n",
        "    att_output, _ = modeling_gemma.eager_attention_forward(\n",
        "        paligemma.language_model.layers[layer_idx].self_attn,\n",
        "        query_states,\n",
        "        key_states,\n",
        "        value_states,\n",
        "        attention_mask,\n",
        "        scaling,\n",
        "    )\n",
        "    # Get head_dim from the current layer, not from the model\n",
        "    head_dim = paligemma.language_model.layers[layer_idx].self_attn.head_dim\n",
        "    att_output = att_output.reshape(batch_size, -1, 1 * 8 * head_dim)\n",
        "    # Process layer outputs\n",
        "    outputs_embeds = []\n",
        "    start_pos = 0\n",
        "    for i, hidden_states in enumerate(inputs_embeds):\n",
        "        layer = models[i].layers[layer_idx]\n",
        "        end_pos = start_pos + hidden_states.shape[1]\n",
        "        if att_output.dtype != layer.self_attn.o_proj.weight.dtype:\n",
        "            att_output = att_output.to(layer.self_attn.o_proj.weight.dtype)\n",
        "        out_emb = layer.self_attn.o_proj(att_output[:, start_pos:end_pos])\n",
        "        # first residual\n",
        "        out_emb = modeling_gemma._gated_residual(hidden_states,\n",
        "                                                 out_emb, gates[i])\n",
        "        after_first_residual = out_emb.clone()\n",
        "        out_emb, gate = layer.post_attention_layernorm(out_emb,\n",
        "                                                       cond=adarms_cond[i])\n",
        "        # Convert to bfloat16 if the next layer (mlp) uses bfloat16\n",
        "        if layer.mlp.up_proj.weight.dtype == torch.bfloat16:\n",
        "            out_emb = out_emb.to(dtype=torch.bfloat16)\n",
        "        out_emb = layer.mlp(out_emb)\n",
        "        # second residual\n",
        "        out_emb = modeling_gemma._gated_residual(after_first_residual,\n",
        "                                                 out_emb, gate)\n",
        "        outputs_embeds.append(out_emb)\n",
        "        start_pos = end_pos\n",
        "    return outputs_embeds\n",
        "\n",
        "\n",
        "class GemmaConfig:  # see openpi `gemma.py: Config`\n",
        "    \"\"\"Configuration for Gemma model variants.\"\"\"\n",
        "\n",
        "    def __init__(self, width, depth, mlp_dim,\n",
        "                 num_heads, num_kv_heads, head_dim):\n",
        "        self.width = width\n",
        "        self.depth = depth\n",
        "        self.mlp_dim = mlp_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.num_kv_heads = num_kv_heads\n",
        "        self.head_dim = head_dim\n",
        "\n",
        "\n",
        "def get_gemma_config(variant: str) -> GemmaConfig:\n",
        "    \"\"\"Returns config for specified gemma variant.\"\"\"\n",
        "    # This is the config for the action expert\n",
        "    if variant == \"gemma_300m\":\n",
        "        return GemmaConfig(\n",
        "            width=1024,\n",
        "            depth=18,\n",
        "            mlp_dim=4096,\n",
        "            num_heads=8,\n",
        "            num_kv_heads=1,\n",
        "            head_dim=256,\n",
        "        )\n",
        "    # Vison Language Config\n",
        "    elif variant == \"gemma_2b\":\n",
        "        return GemmaConfig(\n",
        "            width=2048,\n",
        "            depth=18,\n",
        "            mlp_dim=16_384,\n",
        "            num_heads=8,\n",
        "            num_kv_heads=1,\n",
        "            head_dim=256,\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown variant: {variant}\")\n",
        "\n",
        "\n",
        "class PaliGemmaWithExpertModel(nn.Module):\n",
        "    \"\"\"PaliGemma model with action expert for PI05.\"\"\"\n",
        "    def __init__(\n",
        "        self, vlm_config, action_expert_config, use_adarms=None,\n",
        "        precision: Literal[\"bfloat16\", \"float32\"] = \"bfloat16\",\n",
        "    ):\n",
        "        if use_adarms is None:\n",
        "            use_adarms = [False, False]\n",
        "        super().__init__()\n",
        "\n",
        "        # Configuration from the VLM PALIGEMMA\n",
        "        vlm_config_hf = CONFIG_MAPPING[\"paligemma\"]()\n",
        "        vlm_config_hf._vocab_size = 257152  # noqa: SLF001\n",
        "        vlm_config_hf.image_token_index = 257152\n",
        "        vlm_config_hf.text_config.hidden_size = vlm_config.width\n",
        "        vlm_config_hf.text_config.intermediate_size = vlm_config.mlp_dim\n",
        "        vlm_config_hf.text_config.num_attention_heads = vlm_config.num_heads\n",
        "        vlm_config_hf.text_config.head_dim = vlm_config.head_dim\n",
        "        vlm_config_hf.text_config.num_hidden_layers = vlm_config.depth\n",
        "        vlm_config_hf.text_config.num_key_value_heads = vlm_config.num_kv_heads\n",
        "        vlm_config_hf.text_config.hidden_activation = \"gelu_pytorch_tanh\"\n",
        "        vlm_config_hf.text_config.torch_dtype = \"float32\"\n",
        "        vlm_config_hf.text_config.vocab_size = 257152\n",
        "        vlm_config_hf.text_config.use_adarms = use_adarms[0]\n",
        "        vlm_config_hf.text_config.adarms_cond_dim = (\n",
        "            vlm_config.width if use_adarms[0] else None\n",
        "        )\n",
        "        vlm_config_hf.vision_config.intermediate_size = 4304\n",
        "        vlm_config_hf.vision_config.projection_dim = 2048\n",
        "        vlm_config_hf.vision_config.projector_hidden_act = \"gelu_fast\"\n",
        "        vlm_config_hf.vision_config.torch_dtype = \"float32\"\n",
        "\n",
        "        # CONFIGURATION FOR THE ACTION EXPERT\n",
        "        action_expert_config_hf = CONFIG_MAPPING[\"gemma\"](\n",
        "            head_dim=action_expert_config.head_dim,\n",
        "            hidden_size=action_expert_config.width,\n",
        "            intermediate_size=action_expert_config.mlp_dim,\n",
        "            num_attention_heads=action_expert_config.num_heads,\n",
        "            num_hidden_layers=action_expert_config.depth,\n",
        "            num_key_value_heads=action_expert_config.num_kv_heads,\n",
        "            vocab_size=257152,\n",
        "            hidden_activation=\"gelu_pytorch_tanh\",\n",
        "            torch_dtype=\"float32\",\n",
        "            use_adarms=use_adarms[1],\n",
        "            adarms_cond_dim=(\n",
        "                action_expert_config.width if use_adarms[1] else None\n",
        "            ),\n",
        "        )\n",
        "        # VLM\n",
        "        self.paligemma = PaliGemmaForConditionalGeneration(\n",
        "            config=vlm_config_hf)\n",
        "\n",
        "        # Expert Architecture Initialized from a small Gemma Version\n",
        "        # From pretrained only loads the paligemma model\n",
        "        self.gemma_expert = GemmaForCausalLM(config=action_expert_config_hf)\n",
        "        self.gemma_expert.model.embed_tokens = None\n",
        "\n",
        "        self.to_bfloat16_for_selected_params(precision)\n",
        "\n",
        "    def to_bfloat16_for_selected_params(\n",
        "            self, precision: Literal[\"bfloat16\", \"float32\"] = \"bfloat16\"):\n",
        "        if precision == \"bfloat16\":\n",
        "            self.to(dtype=torch.bfloat16)\n",
        "        elif precision == \"float32\":\n",
        "            self.to(dtype=torch.float32)\n",
        "            return\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid precision: {precision}\")\n",
        "\n",
        "        params_to_keep_float32 = [\n",
        "            \"vision_tower.vision_model.embeddings.patch_embedding.weight\",\n",
        "            \"vision_tower.vision_model.embeddings.patch_embedding.bias\",\n",
        "            \"vision_tower.vision_model.embeddings.position_embedding.weight\",\n",
        "            \"input_layernorm\",\n",
        "            \"post_attention_layernorm\",\n",
        "            \"model.norm\",\n",
        "        ]\n",
        "\n",
        "        for name, param in self.named_parameters():\n",
        "            if any(selector in name for selector in params_to_keep_float32):\n",
        "                param.data = param.data.to(dtype=torch.float32)\n",
        "\n",
        "    def embed_image(self, image: torch.Tensor):\n",
        "        return self.paligemma.model.get_image_features(image)\n",
        "\n",
        "    def embed_language_tokens(self, tokens: torch.Tensor):\n",
        "        return self.paligemma.language_model.embed_tokens(tokens)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        attention_mask: torch.Tensor | None = None,\n",
        "        position_ids: torch.LongTensor | None = None,\n",
        "        past_key_values: list[torch.FloatTensor] | None = None,\n",
        "        inputs_embeds: list[torch.FloatTensor] | None = None,\n",
        "        use_cache: bool | None = None,\n",
        "        adarms_cond: list[torch.Tensor] | None = None,\n",
        "    ):\n",
        "        if adarms_cond is None:\n",
        "            adarms_cond = [None, None]\n",
        "\n",
        "        # The first step is give [preffix, None]\n",
        "        # then just the VLM return a suffix\n",
        "        if inputs_embeds[1] is None:\n",
        "            prefix_output = self.paligemma.language_model.forward(\n",
        "                inputs_embeds=inputs_embeds[0],\n",
        "                attention_mask=attention_mask,\n",
        "                position_ids=position_ids,\n",
        "                past_key_values=past_key_values,\n",
        "                use_cache=use_cache,\n",
        "                adarms_cond=(\n",
        "                    adarms_cond[0] if adarms_cond is not None else None\n",
        "                )\n",
        "            )\n",
        "            prefix_past_key_values = prefix_output.past_key_values\n",
        "            prefix_output = prefix_output.last_hidden_state\n",
        "            suffix_output = None\n",
        "\n",
        "        # When inputs_embeds=[None, suffix_embs] the Gemma_Expert Activates\n",
        "        elif inputs_embeds[0] is None:\n",
        "            suffix_output = self.gemma_expert.model.forward(\n",
        "                inputs_embeds=inputs_embeds[1],\n",
        "                attention_mask=attention_mask,\n",
        "                position_ids=position_ids,\n",
        "                past_key_values=past_key_values,\n",
        "                use_cache=use_cache,\n",
        "                adarms_cond=(\n",
        "                    adarms_cond[1] if adarms_cond is not None else None\n",
        "                )\n",
        "            )\n",
        "            suffix_output = suffix_output.last_hidden_state\n",
        "            prefix_output = None\n",
        "            prefix_past_key_values = None\n",
        "\n",
        "        # A complete forward for VLM and action expert.\n",
        "        else:\n",
        "            models = [self.paligemma.language_model, self.gemma_expert.model]\n",
        "            num_layers = self.paligemma.config.text_config.num_hidden_layers\n",
        "\n",
        "            # Check if gradient checkpointing is enabled for any of the models\n",
        "            use_gradient_checkpointing = (\n",
        "                hasattr(self.gemma_expert.model, \"gradient_checkpointing\")\n",
        "                and self.gemma_expert.model.gradient_checkpointing\n",
        "                and self.training\n",
        "            ) or (\n",
        "                hasattr(self, \"gradient_checkpointing\")\n",
        "                and self.gradient_checkpointing\n",
        "                and self.training\n",
        "            )\n",
        "\n",
        "            # Process all layers with gradient checkpointing if enabled\n",
        "            for layer_idx in range(num_layers):\n",
        "                if use_gradient_checkpointing:\n",
        "                    inputs_embeds = torch.utils.checkpoint.checkpoint(\n",
        "                        compute_layer_complete,\n",
        "                        layer_idx,\n",
        "                        inputs_embeds,\n",
        "                        attention_mask,\n",
        "                        position_ids,\n",
        "                        adarms_cond,\n",
        "                        use_reentrant=False,\n",
        "                        preserve_rng_state=False,\n",
        "                        paligemma=self.paligemma,\n",
        "                        gemma_expert=self.gemma_expert,\n",
        "                    )\n",
        "                else:\n",
        "                    inputs_embeds = compute_layer_complete(\n",
        "                        layer_idx,\n",
        "                        inputs_embeds,\n",
        "                        attention_mask,\n",
        "                        position_ids,\n",
        "                        adarms_cond,\n",
        "                        paligemma=self.paligemma,\n",
        "                        gemma_expert=self.gemma_expert,\n",
        "                    )\n",
        "\n",
        "            # final norm\n",
        "            def compute_final_norms(inputs_embeds, adarms_cond):\n",
        "                outputs_embeds = []\n",
        "                for i, hidden_states in enumerate(inputs_embeds):\n",
        "                    out_emb, _ = models[i].norm(hidden_states,\n",
        "                                                cond=adarms_cond[i])\n",
        "                    outputs_embeds.append(out_emb)\n",
        "                return outputs_embeds\n",
        "\n",
        "            # Apply gradient checkpointing to final norm if enabled\n",
        "            if use_gradient_checkpointing:\n",
        "                outputs_embeds = torch.utils.checkpoint.checkpoint(\n",
        "                    compute_final_norms,\n",
        "                    inputs_embeds,\n",
        "                    adarms_cond,\n",
        "                    use_reentrant=False,\n",
        "                    preserve_rng_state=False,\n",
        "                )\n",
        "            else:\n",
        "                outputs_embeds = compute_final_norms(inputs_embeds,\n",
        "                                                     adarms_cond)\n",
        "\n",
        "            prefix_output = outputs_embeds[0]\n",
        "            suffix_output = outputs_embeds[1]\n",
        "            prefix_past_key_values = None\n",
        "\n",
        "        # You only care about the suffix_output to denoise actions.\n",
        "        return [prefix_output, suffix_output], prefix_past_key_values\n",
        "\n",
        "\n",
        "class PI05Model(nn.Module):\n",
        "    \"\"\"Core PI05 Model.\"\"\"\n",
        "    def __init__(self, config: PI05Config,\n",
        "                 rtc_processor: RTCProcessor | None = None):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.rtc_processor = rtc_processor\n",
        "\n",
        "        paligemma_config = get_gemma_config(config.paligemma_variant)\n",
        "        action_expert_config = get_gemma_config(config.action_expert_variant)\n",
        "\n",
        "        self.paligemma_with_expert = PaliGemmaWithExpertModel(\n",
        "            paligemma_config,\n",
        "            action_expert_config,\n",
        "            use_adarms=[False, True],\n",
        "            precision=config.dtype,\n",
        "        )\n",
        "\n",
        "        # Embeds the noisy action\n",
        "        self.action_in_proj = nn.Linear(config.max_action_dim,\n",
        "                                        action_expert_config.width)\n",
        "\n",
        "        # Unmbed the denoisy action into the space action\n",
        "        self.action_out_proj = nn.Linear(action_expert_config.width,\n",
        "                                         config.max_action_dim)\n",
        "\n",
        "        # Time Embedding\n",
        "        self.time_mlp_in = nn.Linear(action_expert_config.width,\n",
        "                                     action_expert_config.width)\n",
        "        # Time Unmbedding\n",
        "        self.time_mlp_out = nn.Linear(action_expert_config.width,\n",
        "                                      action_expert_config.width)\n",
        "\n",
        "        # Initialize gradient checkpointing flag\n",
        "        self.gradient_checkpointing_enabled = False\n",
        "\n",
        "        # Compile model if requested\n",
        "        if config.compile_model:\n",
        "            torch.set_float32_matmul_precision(\"high\")\n",
        "            self.sample_actions = torch.compile(self.sample_actions,\n",
        "                                                mode=config.compile_mode)\n",
        "\n",
        "        msg = \"\"\"An incorrect transformer version is used, please create\n",
        "          an issue on https://github.com/huggingface/lerobot/issues\"\"\"\n",
        "\n",
        "        try:\n",
        "            from transformers.models.siglip import check\n",
        "\n",
        "            if not check.check_whether_transformers_replace_is_installed_correctly():  # noqa: E501\n",
        "                raise ValueError(msg)\n",
        "        except ImportError:\n",
        "            raise ValueError(msg) from None\n",
        "\n",
        "    def gradient_checkpointing_enable(self):\n",
        "        \"\"\"Enable gradient checkpointing for memory optimization.\"\"\"\n",
        "        self.gradient_checkpointing_enabled = True\n",
        "        self.paligemma_with_expert.paligemma.language_model.gradient_checkpointing = True  # noqa: E501\n",
        "        self.paligemma_with_expert.paligemma.vision_tower.gradient_checkpointing = True  # noqa: E501\n",
        "        self.paligemma_with_expert.gemma_expert.model.gradient_checkpointing = True  # noqa: E501\n",
        "        logging.info(\"Enabled gradient checkpointing for PI05Pytorch model\")\n",
        "\n",
        "    def gradient_checkpointing_disable(self):\n",
        "        \"\"\"Disable gradient checkpointing.\"\"\"\n",
        "        self.gradient_checkpointing_enabled = False\n",
        "        self.paligemma_with_expert.paligemma.language_model.gradient_checkpointing = False  # noqa: E501\n",
        "        self.paligemma_with_expert.paligemma.vision_tower.gradient_checkpointing = False  # noqa: E501\n",
        "        self.paligemma_with_expert.gemma_expert.model.gradient_checkpointing = False  # noqa: E501\n",
        "        logging.info(\"Disabled gradient checkpointing for PI05Pytorch model\")\n",
        "\n",
        "    def _rtc_enabled(self):\n",
        "        return self.config.rtc_config is not None and self.config.rtc_config.enabled  # noqa: E501\n",
        "\n",
        "    def _apply_checkpoint(self, func, *args, **kwargs):\n",
        "        \"\"\"Helper method to apply gradient checkpointing if enabled.\"\"\"\n",
        "        if self.gradient_checkpointing_enabled and self.training:\n",
        "            return torch.utils.checkpoint.checkpoint(\n",
        "                func, *args, use_reentrant=False,\n",
        "                preserve_rng_state=False, **kwargs\n",
        "            )\n",
        "        return func(*args, **kwargs)\n",
        "\n",
        "    def _prepare_attention_masks_4d(self, att_2d_masks):\n",
        "        \"\"\"\n",
        "        Helper method to prepare 4D attention masks for transformer.\n",
        "        \"\"\"\n",
        "        att_2d_masks_4d = att_2d_masks[:, None, :, :]\n",
        "        return torch.where(att_2d_masks_4d, 0.0, OPENPI_ATTENTION_MASK_VALUE)\n",
        "\n",
        "    def sample_noise(self, shape, device):\n",
        "        return torch.normal(\n",
        "            mean=0.0,\n",
        "            std=1.0,\n",
        "            size=shape,\n",
        "            dtype=torch.float32,\n",
        "            device=device,\n",
        "        )\n",
        "\n",
        "    def sample_time(self, bsize, device):\n",
        "        time_beta = sample_beta(\n",
        "            self.config.time_sampling_beta_alpha,\n",
        "            self.config.time_sampling_beta_beta, bsize, device\n",
        "        )\n",
        "        time = time_beta * self.config.time_sampling_scale + self.config.time_sampling_offset  # noqa:E501\n",
        "        return time.to(dtype=torch.float32, device=device)\n",
        "\n",
        "    def embed_prefix(self, images, img_masks, tokens, masks\n",
        "                     ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Embed images with SigLIP and language\n",
        "        tokens with embedding layer.\n",
        "\n",
        "        Args:\n",
        "            images: List of image tensors\n",
        "            img_masks: List of image masks\n",
        "            tokens: Tokenized prompt tokens (B, seq_len)\n",
        "            masks: Attention masks for tokens (B, seq_len)\n",
        "\n",
        "        Returns:\n",
        "            A tuple of tensors containing the embedded images,\n",
        "            pad masks, and attention masks.\n",
        "            embs: (B, seq_len, embd_dim)\n",
        "            pad_masks: (B, seq_len)\n",
        "            att_masks: (B, seq_len)\n",
        "        \"\"\"\n",
        "        embs = []\n",
        "        pad_masks = []\n",
        "        att_masks = []\n",
        "\n",
        "        # Process images\n",
        "        for img, img_mask in zip(images, img_masks, strict=True):\n",
        "            def image_embed_func(img):\n",
        "                return self.paligemma_with_expert.embed_image(img)\n",
        "\n",
        "            # img shape: (B, C, H, W)\n",
        "            # img_emb shape: (B, num_img_embs, embd_dim)\n",
        "            img_emb = self._apply_checkpoint(image_embed_func, img)\n",
        "            bsize, num_img_embs = img_emb.shape[:2]\n",
        "\n",
        "            embs.append(img_emb)\n",
        "\n",
        "            # img_pad_mask shape: (B, num_img_embs)\n",
        "            img_pad_mask = img_mask[:, None].expand(bsize, num_img_embs)\n",
        "            pad_masks.append(img_pad_mask)\n",
        "            att_masks += [0] * num_img_embs\n",
        "\n",
        "        # Process language tokens\n",
        "        def lang_embed_func(tokens):\n",
        "            lang_emb = self.paligemma_with_expert.embed_language_tokens(tokens)\n",
        "            lang_emb_dim = lang_emb.shape[-1]\n",
        "            return lang_emb * math.sqrt(lang_emb_dim)\n",
        "\n",
        "        lang_emb = self._apply_checkpoint(lang_embed_func, tokens)\n",
        "\n",
        "        # Image + Language\n",
        "        embs.append(lang_emb)\n",
        "\n",
        "        pad_masks.append(masks)\n",
        "\n",
        "        num_lang_embs = lang_emb.shape[1]\n",
        "        # Prefix attention masks are all 0 for language and image tokens\n",
        "        # att_masks shape: (num_images* num_img_embs + num_lang_embs)\n",
        "        att_masks += [0] * num_lang_embs\n",
        "\n",
        "        # Convert from list to tensor\n",
        "        embs = torch.cat(embs, dim=1)\n",
        "\n",
        "        # pad_masks shape: (B, num_images* num_img_embs + num_lang_embs)\n",
        "        pad_masks = torch.cat(pad_masks, dim=1)\n",
        "        # att_masks shape: (num_images* num_img_embs + num_lang_embs)\n",
        "        att_masks = torch.tensor(att_masks,\n",
        "                                 dtype=torch.bool, device=pad_masks.device)\n",
        "        bsize = pad_masks.shape[0]\n",
        "        # att_masks shape: (B, num_images* num_img_embs + num_lang_embs)\n",
        "        att_masks = att_masks[None, :].expand(bsize, len(att_masks))\n",
        "\n",
        "        return embs, pad_masks, att_masks\n",
        "\n",
        "    def embed_suffix(self, noisy_actions, timestep):\n",
        "        \"\"\"\n",
        "        Embeds noisy_actions,\n",
        "        timestep to prepare for Expert Gemma processing.\n",
        "        Args:\n",
        "            noisy_actions: The noisy actions to embed.\n",
        "                shape: (B, action_horizon, action_dim)\n",
        "            timestep: The timestep to embed.\n",
        "            Begin with in -1 and end with in 0.\n",
        "                shape: (B,)\n",
        "\n",
        "        Returns:\n",
        "            A tuple of tensors containing the embedded noisy\n",
        "            actions, pad masks, and attention masks.\n",
        "        \"\"\"\n",
        "        embs = []\n",
        "        pad_masks = []\n",
        "        att_masks = []\n",
        "\n",
        "        # Embed timestep using sine-cosine positional encoding\n",
        "        time_emb = create_sinusoidal_pos_embedding(\n",
        "            timestep,\n",
        "            self.action_in_proj.out_features,\n",
        "            min_period=self.config.min_period,\n",
        "            max_period=self.config.max_period,\n",
        "            device=timestep.device,\n",
        "        )\n",
        "        time_emb = time_emb.type(dtype=timestep.dtype)\n",
        "\n",
        "        # Fuse timestep + action information using an MLP\n",
        "        def action_proj_func(noisy_actions):\n",
        "            return self.action_in_proj(noisy_actions)\n",
        "\n",
        "        # (B, chunk_size, action_dim)\n",
        "        action_emb = self._apply_checkpoint(action_proj_func, noisy_actions)\n",
        "\n",
        "        def time_mlp_func(time_emb):\n",
        "            x = self.time_mlp_in(time_emb)\n",
        "            x = F.silu(x)\n",
        "            x = self.time_mlp_out(x)\n",
        "            return F.silu(x)\n",
        "\n",
        "        time_emb = self._apply_checkpoint(time_mlp_func, time_emb)\n",
        "        action_time_emb = action_emb\n",
        "        adarms_cond = time_emb\n",
        "\n",
        "        embs.append(action_time_emb)\n",
        "        bsize, action_time_dim = action_time_emb.shape[:2]\n",
        "        action_time_mask = torch.ones(bsize, action_time_dim, dtype=torch.bool,\n",
        "                                      device=timestep.device)\n",
        "        pad_masks.append(action_time_mask)\n",
        "\n",
        "        # Set attention masks so that image,\n",
        "        # language and state inputs do not attend to action tokens\n",
        "        att_masks += [1] + ([0] * (self.config.chunk_size - 1))\n",
        "\n",
        "        embs = torch.cat(embs, dim=1)\n",
        "        pad_masks = torch.cat(pad_masks, dim=1)\n",
        "        att_masks = torch.tensor(att_masks,\n",
        "                                 dtype=embs.dtype, device=embs.device)\n",
        "        att_masks = att_masks[None, :].expand(bsize, len(att_masks))\n",
        "\n",
        "        return embs, pad_masks, att_masks, adarms_cond\n",
        "\n",
        "    def forward(self, images, img_masks,\n",
        "                tokens, masks, actions, noise=None, time=None,\n",
        "                token_loss_mask=None, real_action_dim=None) -> Tensor:\n",
        "        \"\"\"\n",
        "        Do a full training forward pass and compute the loss.\n",
        "        Actions are data that comes from teleop or from the human.\n",
        "\n",
        "        Args:\n",
        "            images: List of image tensors\n",
        "            img_masks: List of image masks\n",
        "            tokens: Tokenized prompt tokens (B, seq_len)\n",
        "            masks: Attention masks for tokens (B, seq_len)\n",
        "            actions: Action tensors (B, action_horizon, action_dim)\n",
        "            noise: Optional noise tensor for flow matching\n",
        "            time: Optional time tensor for flow matching\n",
        "            token_loss_mask: Optional mask for which tokens to compute\n",
        "                CE loss (B, seq_len)\n",
        "            real_action_dim: Optional real action dimension\n",
        "                (for padding handling)\n",
        "        \"\"\"\n",
        "        if noise is None:\n",
        "            noise = self.sample_noise(actions.shape, actions.device)\n",
        "\n",
        "        if time is None:\n",
        "            time = self.sample_time(actions.shape[0], actions.device)\n",
        "\n",
        "        # Embed prefix (images + language tokens)\n",
        "        prefix_embs, prefix_pad_masks, prefix_att_masks = (\n",
        "            self.embed_prefix(images, img_masks, tokens, masks)\n",
        "        )\n",
        "\n",
        "        # Prepare attention masks for prefix\n",
        "        prefix_att_2d_masks = make_att_2d_masks(\n",
        "            prefix_pad_masks, prefix_att_masks\n",
        "        )\n",
        "        prefix_position_ids = torch.cumsum(prefix_pad_masks, dim=1) - 1\n",
        "        prefix_att_2d_masks_4d = self._prepare_attention_masks_4d(\n",
        "            prefix_att_2d_masks\n",
        "        )\n",
        "        # Convert prefix_embs to bfloat16 if needed\n",
        "        q_proj_dtype = (\n",
        "            self.paligemma_with_expert.paligemma.language_model.layers[0]\n",
        "            .self_attn.q_proj.weight.dtype\n",
        "        )\n",
        "        if q_proj_dtype == torch.bfloat16:\n",
        "            prefix_embs = prefix_embs.to(dtype=torch.bfloat16)\n",
        "\n",
        "        # Forward pass through prefix to get prefix output and KV cache\n",
        "        def prefix_forward_func(\n",
        "            prefix_embs, prefix_att_2d_masks_4d, prefix_position_ids\n",
        "        ):\n",
        "            (prefix_out, _), kv_cache = self.paligemma_with_expert.forward(\n",
        "                attention_mask=prefix_att_2d_masks_4d,\n",
        "                position_ids=prefix_position_ids,\n",
        "                past_key_values=None,\n",
        "                inputs_embeds=[prefix_embs, None],\n",
        "                use_cache=True,\n",
        "                adarms_cond=[None, None],\n",
        "            )\n",
        "            return prefix_out, kv_cache\n",
        "\n",
        "        prefix_out, kv_cache = self._apply_checkpoint(\n",
        "            prefix_forward_func,\n",
        "            prefix_embs,\n",
        "            prefix_att_2d_masks_4d,\n",
        "            prefix_position_ids\n",
        "        )\n",
        "\n",
        "        # Compute Cross-Entropy Loss for Subtask Generation\n",
        "        subtask_generation_loss = None\n",
        "        if token_loss_mask is not None and tokens is not None:\n",
        "            # Shift tokens by 1 for next-token prediction\n",
        "            # tokens shape: (B, seq_len)\n",
        "            targets = tokens[:, 1:]  # (B, seq_len - 1)\n",
        "            loss_mask = token_loss_mask[:, 1:]  # (B, seq_len - 1)\n",
        "\n",
        "            # prefix_out shape: (B, prefix_seq_len, embd_dim)\n",
        "            # The prefix contains: images + language tokens\n",
        "            # prefix_out[:, :-1] removes the last token\n",
        "            # (to align with shifted targets)\n",
        "            # We need to extract the language token part from the end\n",
        "            # Since language tokens are at the end of prefix,\n",
        "            # we take the last tokens.shape[1] - 1 tokens\n",
        "            prefix_out_shifted = prefix_out[:, :-1]\n",
        "            # (B, prefix_seq_len - 1, embd_dim)\n",
        "            token_seq_len = tokens.shape[1]\n",
        "\n",
        "            # Extract the language token embeddings\n",
        "            # (last token_seq_len - 1 tokens)\n",
        "            if prefix_out_shifted.shape[1] >= token_seq_len - 1:\n",
        "                lang_prefix_out = prefix_out_shifted[\n",
        "                    :, -(token_seq_len - 1):\n",
        "                ]  # (B, token_seq_len - 1, embd_dim)\n",
        "                # Ensure we have the right length\n",
        "                min_len = min(lang_prefix_out.shape[1], targets.shape[1])\n",
        "                lang_prefix_out = lang_prefix_out[:, :min_len]\n",
        "                targets = targets[:, :min_len]\n",
        "                loss_mask = loss_mask[:, :min_len]\n",
        "\n",
        "                # Convert embeddings to logits\n",
        "                def deembed_func(lang_prefix_out):\n",
        "                    return (\n",
        "                        self.paligemma_with_expert.paligemma.lm_head(\n",
        "                            lang_prefix_out\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "                logits = self._apply_checkpoint(deembed_func, lang_prefix_out)\n",
        "                # logits shape: (B, min_len, vocab_size)\n",
        "\n",
        "                # Compute log probabilities\n",
        "                log_probs = F.log_softmax(logits, dim=-1)\n",
        "\n",
        "                # Get the log probability of the target token\n",
        "                target_log_probs = torch.gather(\n",
        "                    log_probs, dim=-1, index=targets.unsqueeze(-1)\n",
        "                ).squeeze(-1)  # (B, min_len)\n",
        "\n",
        "                # Apply loss mask and compute mean\n",
        "                masked_loss = -target_log_probs * loss_mask.float()\n",
        "                # Sum over sequence dimension and divide by\n",
        "                # number of valid tokens\n",
        "                num_valid_tokens = loss_mask.float().sum(dim=-1).clamp(min=1.0)\n",
        "                subtask_generation_loss = (\n",
        "                    masked_loss.sum(dim=-1) / num_valid_tokens\n",
        "                )  # (B,)\n",
        "\n",
        "        # Flow Matching Loss (MSE Loss)\n",
        "        time_expanded = time[:, None, None]\n",
        "        x_t = time_expanded * noise + (1 - time_expanded) * actions\n",
        "        u_t = noise - actions\n",
        "\n",
        "        suffix_embs, suffix_pad_masks, suffix_att_masks, adarms_cond = (\n",
        "            self.embed_suffix(x_t, time)\n",
        "        )\n",
        "\n",
        "        q_proj_dtype_suffix = (\n",
        "            self.paligemma_with_expert.paligemma.language_model.layers[0]\n",
        "            .self_attn.q_proj.weight.dtype\n",
        "        )\n",
        "        if q_proj_dtype_suffix == torch.bfloat16:\n",
        "            suffix_embs = suffix_embs.to(dtype=torch.bfloat16)\n",
        "\n",
        "        # Combine prefix and suffix for attention\n",
        "        pad_masks = torch.cat([prefix_pad_masks, suffix_pad_masks], dim=1)\n",
        "        att_masks = torch.cat([prefix_att_masks, suffix_att_masks], dim=1)\n",
        "\n",
        "        att_2d_masks = make_att_2d_masks(pad_masks, att_masks)\n",
        "        position_ids = torch.cumsum(pad_masks, dim=1) - 1\n",
        "\n",
        "        # For suffix, we only need attention mask for\n",
        "        # suffix tokens attending to full sequence\n",
        "        suffix_len = suffix_pad_masks.shape[1]\n",
        "        suffix_att_2d_masks = att_2d_masks[:, -suffix_len:, :]\n",
        "        # (B, suffix_len, full_len)\n",
        "        suffix_position_ids = position_ids[:, -suffix_len:]\n",
        "        # (B, suffix_len)\n",
        "\n",
        "        suffix_att_2d_masks_4d = self._prepare_attention_masks_4d(\n",
        "            suffix_att_2d_masks\n",
        "        )\n",
        "\n",
        "        # Forward pass for flow matching loss\n",
        "        def suffix_forward_func(\n",
        "            suffix_embs,\n",
        "            suffix_att_2d_masks_4d,\n",
        "            suffix_position_ids,\n",
        "            kv_cache,\n",
        "            adarms_cond\n",
        "        ):\n",
        "            (_, suffix_out), _ = self.paligemma_with_expert.forward(\n",
        "                attention_mask=suffix_att_2d_masks_4d,\n",
        "                position_ids=suffix_position_ids,\n",
        "                past_key_values=kv_cache,\n",
        "                inputs_embeds=[None, suffix_embs],\n",
        "                use_cache=False,\n",
        "                adarms_cond=[None, adarms_cond],\n",
        "            )\n",
        "            return suffix_out\n",
        "\n",
        "        suffix_out = self._apply_checkpoint(\n",
        "            suffix_forward_func,\n",
        "            suffix_embs,\n",
        "            suffix_att_2d_masks_4d,\n",
        "            suffix_position_ids,\n",
        "            kv_cache,\n",
        "            adarms_cond\n",
        "        )\n",
        "\n",
        "        suffix_out = suffix_out[:, -self.config.chunk_size:]\n",
        "        suffix_out = suffix_out.to(dtype=torch.float32)\n",
        "\n",
        "        def action_out_proj_func(suffix_out):\n",
        "            return self.action_out_proj(suffix_out)\n",
        "\n",
        "        v_t = self._apply_checkpoint(action_out_proj_func, suffix_out)\n",
        "\n",
        "        # Compute flow matching loss\n",
        "        if real_action_dim is not None:\n",
        "            # Truncate to real action dimension\n",
        "            flow_loss = F.mse_loss(\n",
        "                v_t[:, :, :real_action_dim],\n",
        "                u_t[:, :, :real_action_dim],\n",
        "                reduction=\"none\"\n",
        "            )\n",
        "        else:\n",
        "            flow_loss = F.mse_loss(u_t, v_t, reduction=\"none\")\n",
        "\n",
        "        # flow_loss shape: (B, action_horizon, action_dim)\n",
        "        # Mean over action dimensions\n",
        "        flow_loss = flow_loss.mean(dim=-1)  # (B, action_horizon)\n",
        "        flow_loss = flow_loss.mean(dim=-1)  # (B,)\n",
        "\n",
        "        # Combine losses\n",
        "        if subtask_generation_loss is not None:\n",
        "            total_loss = subtask_generation_loss + flow_loss\n",
        "        else:\n",
        "            total_loss = flow_loss\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample_low_level_task(\n",
        "                self,\n",
        "                images,\n",
        "                img_masks,\n",
        "                tokens,\n",
        "                masks,\n",
        "                max_decoding_steps: int = 20,\n",
        "                eos_token_id: int = 1,\n",
        "                temperature: float = 0.1):\n",
        "        \"\"\"\n",
        "        Sample tokens autoregressively from the language model.\n",
        "        Each N second the model should generate the task.\n",
        "\n",
        "        Args:\n",
        "            images: List of image tensors, each of shape\n",
        "                (B, C, H, W) or (B, H, W, C)\n",
        "            img_masks: List of boolean masks, to each image in the\n",
        "             list it corresponds to a mask each of shape (B,)\n",
        "                indicating valid images.\n",
        "            tokens: Language tokens of shape (B, seq_len_lang)\n",
        "            masks: Language attention masks of shape (B, seq_len_lang)\n",
        "            max_decoding_steps: Maximum number of tokens to generate\n",
        "            eos_token_id: End-of-sequence token ID\n",
        "            temperature: Sampling temperature\n",
        "\n",
        "        Returns:\n",
        "            output_tokens: (B, max_decoding_steps) - generated token IDs\n",
        "            past_key_values: Updated KV cache\n",
        "        \"\"\"\n",
        "        prefix_embs, prefix_pad_masks, prefix_att_masks = self.embed_prefix(\n",
        "            images, img_masks, tokens, masks\n",
        "        )\n",
        "\n",
        "        prefix_att_2d_masks = make_att_2d_masks(\n",
        "            prefix_pad_masks, prefix_att_masks\n",
        "        )\n",
        "\n",
        "        # Compute position IDs for attention masks minus 1\n",
        "        prefix_position_ids = torch.cumsum(prefix_pad_masks, dim=1) - 1\n",
        "\n",
        "        # Convert 2D attention masks to 4D format expected by the model\n",
        "        prefix_att_2d_masks_4d = self._prepare_attention_masks_4d(\n",
        "            prefix_att_2d_masks\n",
        "        )\n",
        "\n",
        "        lang_model = self.paligemma_with_expert.paligemma.language_model\n",
        "        lang_model.config._attn_implementation = \"eager\"  # noqa: SLF001\n",
        "\n",
        "        embeddings, past_key_values = self.paligemma_with_expert.forward(\n",
        "            attention_mask=prefix_att_2d_masks_4d,\n",
        "            position_ids=prefix_position_ids,\n",
        "            past_key_values=None,\n",
        "            inputs_embeds=[prefix_embs, None],\n",
        "            use_cache=True,\n",
        "        )\n",
        "\n",
        "        # embeddings[0] shape: (B, total_seq_len, embd_dim)\n",
        "        # Extract last token: (B, embd_dim)\n",
        "        last_token_embed = embeddings[0][:, -1, :]\n",
        "\n",
        "        # Convert to logits: (B, vocab_size)\n",
        "        last_logits = (\n",
        "            self.paligemma_with_expert.paligemma.lm_head(last_token_embed)\n",
        "        )\n",
        "\n",
        "        batch_size = last_logits.shape[0]\n",
        "        device = last_logits.device\n",
        "\n",
        "        # prefix_valid_length is the number of valid (non-padded) tokens\n",
        "        prefix_valid_length = torch.sum(prefix_pad_masks, dim=1)  # (B,)\n",
        "        output_tokens = torch.zeros((batch_size, max_decoding_steps),\n",
        "                                    dtype=torch.long, device=device)\n",
        "        all_eos = torch.zeros(batch_size, dtype=torch.bool, device=device)\n",
        "\n",
        "        # Prefix attention mask plus the new token attention mask\n",
        "        running_attention_mask = prefix_pad_masks.clone()\n",
        "\n",
        "        # Autoregressive Loop\n",
        "        for step in range(max_decoding_steps):\n",
        "            # Sample next token\n",
        "            if temperature > 0.0:\n",
        "                probs = F.softmax(last_logits / temperature, dim=-1)\n",
        "                # token shape: (B, 1)\n",
        "                token = torch.multinomial(probs, num_samples=1)\n",
        "            else:\n",
        "                token = torch.argmax(last_logits, dim=-1, keepdim=True)\n",
        "\n",
        "            output_tokens[:, step] = token.squeeze(-1)\n",
        "\n",
        "            # Check for EOS\n",
        "            all_eos |= (token.squeeze(-1) == eos_token_id)\n",
        "            if all_eos.all():\n",
        "                break\n",
        "\n",
        "            # Feed the new token back in the model\n",
        "            # token shape: (B, 1) -> embed_tokens returns (B, 1, embd_dim)\n",
        "            next_token_embeds = lang_model.embed_tokens(token)\n",
        "\n",
        "            # Create position_ids for the new token\n",
        "            position_ids = prefix_valid_length[:, None] + step\n",
        "\n",
        "            # Create attention mask for the new token\n",
        "            new_mask = torch.ones(\n",
        "                (batch_size, 1),\n",
        "                dtype=running_attention_mask.dtype,\n",
        "                device=device\n",
        "            )\n",
        "\n",
        "            running_attention_mask = torch.cat(\n",
        "                [running_attention_mask, new_mask], dim=1)\n",
        "\n",
        "            # The attention mask can be 2d or 4d.\n",
        "            embeds_list, past_key_values = self.paligemma_with_expert.forward(\n",
        "                inputs_embeds=[next_token_embeds, None],\n",
        "                attention_mask=running_attention_mask,\n",
        "                position_ids=position_ids,\n",
        "                past_key_values=past_key_values,\n",
        "                use_cache=True,\n",
        "            )\n",
        "\n",
        "            prefix_output = embeds_list[0]\n",
        "            last_token_embed = prefix_output[:, -1, :]\n",
        "            last_logits = (\n",
        "                self.paligemma_with_expert.paligemma.lm_head(last_token_embed)\n",
        "            )\n",
        "\n",
        "        # We return the tokens and the KV cache for the actions.\n",
        "        return output_tokens, past_key_values\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample_actions(self,\n",
        "                       images=None,\n",
        "                       img_masks=None,\n",
        "                       tokens=None,\n",
        "                       masks=None,\n",
        "                       past_key_values=None,\n",
        "                       prefix_pad_masks=None,\n",
        "                       noise=None,\n",
        "                       num_steps=None,\n",
        "                       **kwargs: Unpack[ActionSelectKwargs]):\n",
        "        \"\"\"\n",
        "        Sample actions using flow matching.\n",
        "        Each N second the model should generate the action.\n",
        "        Args:\n",
        "            images: List of image tensors, each of shape\n",
        "                (B, C, H, W) or (B, H, W, C)\n",
        "            img_masks: List of boolean masks, to each image in the\n",
        "             list it corresponds to a mask each of shape (B,)\n",
        "                indicating valid images.\n",
        "            tokens: Language tokens of shape (B, seq_len_lang)\n",
        "            masks: Language attention masks of shape (B, seq_len_lang)\n",
        "            noise: Noise tensor for flow matching\n",
        "            num_steps: Number of steps to sample\n",
        "            kwargs: Additional arguments\n",
        "\n",
        "        Returns:\n",
        "            actions: (B, action_horizon, action_dim)\n",
        "        \"\"\"\n",
        "        # If images are provided, we need to embed the prefix.\n",
        "        # Otherwise, we use the prefix_pad_masks and past_key_values that\n",
        "        # were already computed on the sample_low_level_task.\n",
        "        if images is not None:\n",
        "            prefix_embs, prefix_pad_masks, prefix_att_mask = self.embed_prefix(\n",
        "                images, img_masks, tokens, masks\n",
        "            )\n",
        "\n",
        "            prefix_att_2d_masks = make_att_2d_masks(prefix_pad_masks,\n",
        "                                                    prefix_att_mask)\n",
        "            prefix_position_ids = torch.cumsum(prefix_pad_masks, dim=1) - 1\n",
        "\n",
        "            prefix_att_2d_masks_4d = self._prepare_attention_masks_4d(\n",
        "                prefix_att_2d_masks\n",
        "            )\n",
        "            self.paligemma_with_expert.paligemma.language_model.config._attn_implementation = \"eager\"  # noqa: SLF001, E501\n",
        "\n",
        "            _, past_key_values = self.paligemma_with_expert.forward(\n",
        "                attention_mask=prefix_att_2d_masks_4d,\n",
        "                position_ids=prefix_position_ids,\n",
        "                past_key_values=None,\n",
        "                inputs_embeds=[prefix_embs, None],\n",
        "                use_cache=True,\n",
        "            )\n",
        "\n",
        "        # IMPORTANT:\n",
        "        # Here we need to be careful with the prefix_pad_masks and past_key_values.\n",
        "        # THe new tokens generated by low level task are not part of the prefix_pad_masks and past_key_values.\n",
        "\n",
        "\n",
        "        bsize = tokens.shape[0]\n",
        "        device = tokens.device\n",
        "\n",
        "        if num_steps is None:\n",
        "            num_steps = self.config.num_inference_steps\n",
        "\n",
        "        if noise is None:\n",
        "            # Sample noise with padded dimension as expected by action_in_proj\n",
        "            actions_shape = (\n",
        "                bsize,\n",
        "                self.config.chunk_size,\n",
        "                self.config.max_action_dim,\n",
        "            )  # Use config max_action_dim for internal processing\n",
        "            noise = self.sample_noise(actions_shape, device)\n",
        "\n",
        "        dt = -1.0 / num_steps\n",
        "        dt = torch.tensor(dt, dtype=torch.float32, device=device)\n",
        "        x_t = noise\n",
        "        time = torch.tensor(1.0, dtype=torch.float32, device=device)\n",
        "\n",
        "        while time >= -dt / 2:\n",
        "            expanded_time = time.expand(bsize)\n",
        "            # Define a closure function to properly capture expanded_time\n",
        "            # This avoids the lambda expression (E731)\n",
        "            # and loop variable binding (B023) issues\n",
        "            def denoise_step_partial_call(input_x_t,\n",
        "                                          current_timestep=expanded_time):\n",
        "                return self.denoise_step(\n",
        "                    prefix_pad_masks=prefix_pad_masks,\n",
        "                    past_key_values=past_key_values,\n",
        "                    x_t=input_x_t,\n",
        "                    timestep=current_timestep,\n",
        "                )\n",
        "\n",
        "            if self._rtc_enabled():\n",
        "                inference_delay = kwargs.get(\"inference_delay\")\n",
        "                prev_chunk_left_over = kwargs.get(\"prev_chunk_left_over\")\n",
        "                execution_horizon = kwargs.get(\"execution_horizon\")\n",
        "\n",
        "                v_t = self.rtc_processor.denoise_step(\n",
        "                    x_t=x_t,\n",
        "                    prev_chunk_left_over=prev_chunk_left_over,\n",
        "                    inference_delay=inference_delay,\n",
        "                    time=time,\n",
        "                    original_denoise_step_partial=denoise_step_partial_call,\n",
        "                    execution_horizon=execution_horizon,\n",
        "                )\n",
        "            else:\n",
        "                v_t = denoise_step_partial_call(x_t)\n",
        "\n",
        "            # Euler step\n",
        "            x_t += dt * v_t\n",
        "\n",
        "            # Record x_t and v_t after Euler step\n",
        "            if self.rtc_processor is not None and self.rtc_processor.is_debug_enabled():  # noqa: E501\n",
        "                self.rtc_processor.track(time=time, x_t=x_t, v_t=v_t)\n",
        "\n",
        "            time += dt\n",
        "\n",
        "        return x_t\n",
        "\n",
        "    def denoise_step(\n",
        "        self,\n",
        "        prefix_pad_masks,\n",
        "        past_key_values,\n",
        "        x_t,\n",
        "        timestep,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Apply one denoising step of the noise `x_t` at a given timestep.\n",
        "\n",
        "        Args:\n",
        "            prefix_pad_masks: (B, seq_len_images + seq_len_lang)\n",
        "            past_key_values: KV cache for efficient autoregressive generation\n",
        "            x_t: (B, action_horizon, action_dim) -noise tensor for flowmatching\n",
        "            timestep: (B,) - current timestep\n",
        "        \"\"\"\n",
        "        suffix_embs, suffix_pad_masks, suffix_att_masks, adarms_cond = (\n",
        "            self.embed_suffix(x_t, timestep)\n",
        "        )\n",
        "\n",
        "        suffix_len = suffix_pad_masks.shape[1]\n",
        "        batch_size = prefix_pad_masks.shape[0]\n",
        "        prefix_len = prefix_pad_masks.shape[1]\n",
        "\n",
        "        prefix_pad_2d_masks = prefix_pad_masks[:, None, :].expand(batch_size,\n",
        "                                                                  suffix_len,\n",
        "                                                                  prefix_len)\n",
        "        suffix_att_2d_masks = make_att_2d_masks(suffix_pad_masks,\n",
        "                                                suffix_att_masks)\n",
        "        full_att_2d_masks = torch.cat([prefix_pad_2d_masks,\n",
        "                                       suffix_att_2d_masks], dim=2)\n",
        "\n",
        "        prefix_offsets = torch.sum(prefix_pad_masks, dim=-1)[:, None]\n",
        "        position_ids = prefix_offsets + torch.cumsum(suffix_pad_masks, dim=1)\n",
        "        position_ids -= 1\n",
        "\n",
        "        full_att_2d_masks_4d = self._prepare_attention_masks_4d(\n",
        "            full_att_2d_masks)\n",
        "        self.paligemma_with_expert.gemma_expert.model.config._attn_implementation = \"eager\"  # noqa: SLF001, E501\n",
        "\n",
        "        # Important, here is the vector field core\n",
        "        outputs_embeds, _ = self.paligemma_with_expert.forward(\n",
        "            attention_mask=full_att_2d_masks_4d,\n",
        "            position_ids=position_ids,\n",
        "            past_key_values=past_key_values,\n",
        "            inputs_embeds=[None, suffix_embs],\n",
        "            use_cache=False,\n",
        "            adarms_cond=[None, adarms_cond],\n",
        "        )\n",
        "\n",
        "        suffix_out = outputs_embeds[1]\n",
        "        suffix_out = suffix_out[:, -self.config.chunk_size:]\n",
        "        suffix_out = suffix_out.to(dtype=torch.float32)\n",
        "        return self.action_out_proj(suffix_out)\n",
        "\n",
        "\n",
        "class PI05Policy(PreTrainedPolicy):\n",
        "    \"\"\"PI05 Policy for LeRobot.\"\"\"\n",
        "\n",
        "    config_class = PI05Config\n",
        "    name = \"pi05\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        config: PI05Config,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            config: Policy configuration class instance.\n",
        "        \"\"\"\n",
        "        super().__init__(config)\n",
        "        config.validate_features()\n",
        "        self.config = config\n",
        "\n",
        "        # Initialize the core PI05 model\n",
        "        self.init_rtc_processor()\n",
        "        self.model = PI05Model(config, rtc_processor=self.rtc_processor)\n",
        "\n",
        "        # Enable gradient checkpointing if requested\n",
        "        if config.gradient_checkpointing:\n",
        "            self.model.gradient_checkpointing_enable()\n",
        "\n",
        "        self.model.to(config.device)\n",
        "\n",
        "        self.reset()\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(\n",
        "        cls: builtins.type[T],\n",
        "        pretrained_name_or_path: str | Path,\n",
        "        *,\n",
        "        config: PreTrainedConfig | None = None,\n",
        "        force_download: bool = False,\n",
        "        resume_download: bool | None = None,\n",
        "        proxies: dict | None = None,\n",
        "        token: str | bool | None = None,\n",
        "        cache_dir: str | Path | None = None,\n",
        "        local_files_only: bool = False,\n",
        "        revision: str | None = None,\n",
        "        strict: bool = True,\n",
        "        **kwargs,\n",
        "    ) -> T:\n",
        "        \"\"\"\n",
        "        Override the from_pretrained method to handle\n",
        "        key remapping and display important disclaimer.\"\"\"\n",
        "        if pretrained_name_or_path is None:\n",
        "            raise ValueError(\"pretrained_name_or_path is required\")\n",
        "\n",
        "        # Use provided config if available, otherwise create default config\n",
        "        if config is None:\n",
        "            config = PreTrainedConfig.from_pretrained(\n",
        "                pretrained_name_or_path=pretrained_name_or_path,\n",
        "                force_download=force_download,\n",
        "                resume_download=resume_download,\n",
        "                proxies=proxies,\n",
        "                token=token,\n",
        "                cache_dir=cache_dir,\n",
        "                local_files_only=local_files_only,\n",
        "                revision=revision,\n",
        "                **kwargs,\n",
        "            )\n",
        "\n",
        "        # Initialize model without loading weights\n",
        "        # Check if dataset_stats were provided in kwargs\n",
        "        model = cls(config, **kwargs)\n",
        "\n",
        "        # Now manually load and remap the state dict\n",
        "        try:\n",
        "            # Try to load the pytorch_model.bin or model.safetensors file\n",
        "            print(f\"Loading model from: {pretrained_name_or_path}\")\n",
        "            try:\n",
        "                from transformers.utils import cached_file\n",
        "\n",
        "                # Try safetensors first\n",
        "                resolved_file = cached_file(\n",
        "                    pretrained_name_or_path,\n",
        "                    \"model.safetensors\",\n",
        "                    cache_dir=kwargs.get(\"cache_dir\"),\n",
        "                    force_download=kwargs.get(\"force_download\", False),\n",
        "                    resume_download=kwargs.get(\"resume_download\"),\n",
        "                    proxies=kwargs.get(\"proxies\"),\n",
        "                    use_auth_token=kwargs.get(\"use_auth_token\"),\n",
        "                    revision=kwargs.get(\"revision\"),\n",
        "                    local_files_only=kwargs.get(\"local_files_only\", False),\n",
        "                )\n",
        "                from safetensors.torch import load_file\n",
        "\n",
        "                original_state_dict = load_file(resolved_file)\n",
        "                print(\"âœ“ Loaded state dict from model.safetensors\")\n",
        "            except Exception as e:\n",
        "                print(f\"Could not load state dict from remote files: {e}\")\n",
        "                print(\"Returning model without loading pretrained weights\")\n",
        "                return model\n",
        "\n",
        "            # First, fix any key differences\n",
        "            fixed_state_dict = model._fix_pytorch_state_dict_keys(\n",
        "                original_state_dict, model.config)\n",
        "\n",
        "            # Then add \"model.\" prefix for all keys that don't already have it\n",
        "            remapped_state_dict = {}\n",
        "            remap_count = 0\n",
        "\n",
        "            for key, value in fixed_state_dict.items():\n",
        "                if not key.startswith(\"model.\"):\n",
        "                    new_key = f\"model.{key}\"\n",
        "                    remapped_state_dict[new_key] = value\n",
        "                    remap_count += 1\n",
        "                    if remap_count <= 10:  # Only print first 10 to avoid spam\n",
        "                        print(f\"Remapped: {key} -> {new_key}\")\n",
        "                else:\n",
        "                    remapped_state_dict[key] = value\n",
        "\n",
        "            if remap_count > 0:\n",
        "                print(f\"Remapped {remap_count} state dict keys\")\n",
        "\n",
        "            # Load the remapped state dict into the model\n",
        "            missing_keys, unexpected_keys = model.load_state_dict(\n",
        "                remapped_state_dict, strict=strict)\n",
        "\n",
        "            if missing_keys:\n",
        "                print(\n",
        "                    f\"Missing keys when loading state dict:\"\n",
        "                    f\"{len(missing_keys)} keys\"\n",
        "                )\n",
        "                if len(missing_keys) <= 5:\n",
        "                    for key in missing_keys:\n",
        "                        print(f\"  - {key}\")\n",
        "                else:\n",
        "                    for key in missing_keys[:5]:\n",
        "                        print(f\"  - {key}\")\n",
        "                    print(f\"  ... and {len(missing_keys) - 5} more\")\n",
        "\n",
        "            if unexpected_keys:\n",
        "                print(\n",
        "                    f\"Unexpected keys when loading state dict: \"\n",
        "                    f\"{len(unexpected_keys)} keys\"\n",
        "                )\n",
        "                if len(unexpected_keys) <= 5:\n",
        "                    for key in unexpected_keys:\n",
        "                        print(f\"  - {key}\")\n",
        "                else:\n",
        "                    for key in unexpected_keys[:5]:\n",
        "                        print(f\"  - {key}\")\n",
        "                    print(f\"  ... and {len(unexpected_keys) - 5} more\")\n",
        "\n",
        "            if not missing_keys and not unexpected_keys:\n",
        "                print(\"All keys loaded successfully!\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not remap state dict keys: {e}\")\n",
        "\n",
        "        return model\n",
        "\n",
        "    def _fix_pytorch_state_dict_keys(\n",
        "        self, state_dict, model_config\n",
        "    ):  # see openpi `BaseModelConfig, _fix_pytorch_state_dict_keys`\n",
        "        \"\"\"Fix state dict keys to match current model architecture.\"\"\"\n",
        "        import re\n",
        "\n",
        "        fixed_state_dict = {}\n",
        "\n",
        "        for key, value in state_dict.items():\n",
        "            new_key = key\n",
        "\n",
        "            # Handle layer norm structure changes:\n",
        "            # .weight -> .dense.weight + .dense.bias\n",
        "            # For gemma expert layers\n",
        "            layer_norm_pattern = (\n",
        "                r\"paligemma_with_expert\\.gemma_expert\\.model\\.layers\\.\\d+\\.\"\n",
        "                r\"(input_layernorm|post_attention_layernorm)\\.weight\"\n",
        "            )\n",
        "            if re.match(layer_norm_pattern, key):\n",
        "                # Check if the model actually has adaRMS enabled\n",
        "                # for the expert\n",
        "                expert_config = (\n",
        "                    self.model.paligemma_with_expert.gemma_expert.config\n",
        "                )\n",
        "                expert_uses_adarms = getattr(\n",
        "                    expert_config, \"use_adarms\", False\n",
        "                )\n",
        "                if expert_uses_adarms:\n",
        "                    logging.warning(\n",
        "                        f\"Skipping layer norm key (adaRMS mismatch): {key}\"\n",
        "                    )\n",
        "                    continue\n",
        "\n",
        "            norm_pattern = (\n",
        "                r\"paligemma_with_expert\\.gemma_expert\\.model\\.norm\\.weight\"\n",
        "            )\n",
        "            if re.match(norm_pattern, key):\n",
        "                # Check if the model actually has adaRMS enabled\n",
        "                # for the expert\n",
        "                expert_config = (\n",
        "                    self.model.paligemma_with_expert.gemma_expert.config\n",
        "                )\n",
        "                expert_uses_adarms = getattr(\n",
        "                    expert_config, \"use_adarms\", False\n",
        "                )\n",
        "                if expert_uses_adarms:\n",
        "                    logging.warning(\n",
        "                        f\"Skipping norm key (adaRMS mismatch): {key}\"\n",
        "                    )\n",
        "                    continue\n",
        "\n",
        "            # Handle MLP naming changes for pi05\n",
        "            # pi05 model expects time_mlp_*\n",
        "            # but checkpoint might have action_time_mlp_*\n",
        "            if key.startswith(\"action_time_mlp_in.\"):\n",
        "                new_key = key.replace(\"action_time_mlp_in.\", \"time_mlp_in.\")\n",
        "            elif key.startswith(\"action_time_mlp_out.\"):\n",
        "                new_key = key.replace(\"action_time_mlp_out.\", \"time_mlp_out.\")\n",
        "            # Also handle state_proj which shouldn't exist in pi05\n",
        "            if key.startswith(\"state_proj.\"):\n",
        "                logging.warning(\n",
        "                    f\"Skipping state_proj key in pi05 mode: {key}\"\n",
        "                )\n",
        "                continue\n",
        "\n",
        "            # Handle vision tower embedding layer potential differences\n",
        "            if \"patch_embedding\" in key:\n",
        "                # Some checkpoints might have this,\n",
        "                # but current model expects different structure\n",
        "                logging.warning(\n",
        "                    f\"Vision embedding key might need handling: {key}\"\n",
        "                )\n",
        "\n",
        "            fixed_state_dict[new_key] = value\n",
        "\n",
        "        return fixed_state_dict\n",
        "\n",
        "    def get_optim_params(self) -> dict:\n",
        "        return self.parameters()\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset internal state - called when environment resets.\"\"\"\n",
        "        self._action_queue = deque(maxlen=self.config.n_action_steps)\n",
        "        self._queues = {\n",
        "            ACTION: deque(maxlen=self.config.n_action_steps),\n",
        "        }\n",
        "\n",
        "    def init_rtc_processor(self):\n",
        "        \"\"\"Initialize RTC processor if RTC is enabled in config.\"\"\"\n",
        "        self.rtc_processor = None\n",
        "\n",
        "        # Create processor if config provided\n",
        "        # If RTC is not enabled - we can still track the denoising data\n",
        "        if self.config.rtc_config is not None:\n",
        "            self.rtc_processor = RTCProcessor(self.config.rtc_config)\n",
        "\n",
        "            model_value = getattr(self, \"model\", None)\n",
        "            if model_value is not None:\n",
        "                model_value.rtc_processor = self.rtc_processor\n",
        "\n",
        "    def _rtc_enabled(self) -> bool:\n",
        "        return (\n",
        "            self.config.rtc_config is not None\n",
        "            and self.config.rtc_config.enabled\n",
        "        )\n",
        "\n",
        "    def _preprocess_images(\n",
        "        self, batch: dict[str, Tensor]\n",
        "    ) -> tuple[list[Tensor], list[Tensor]]:\n",
        "        \"\"\"Preprocess images for the model.\n",
        "        Images from LeRobot are typically in [B, C, H, W]\n",
        "        format and normalized to [0, 1].\n",
        "        PaliGemma expects images in [B, C, H, W]\n",
        "        format and normalized to [-1, 1].\n",
        "        \"\"\"\n",
        "        images = []\n",
        "        img_masks = []\n",
        "\n",
        "        # Get device from model parameters\n",
        "        device = next(self.parameters()).device\n",
        "\n",
        "        present_img_keys = [\n",
        "            key for key in self.config.image_features if key in batch\n",
        "        ]\n",
        "        missing_img_keys = [\n",
        "            key for key in self.config.image_features if key not in batch\n",
        "        ]\n",
        "\n",
        "        if len(present_img_keys) == 0:\n",
        "            raise ValueError(\n",
        "                f\"All image features are missing from the batch. \"\n",
        "                f\"At least one expected. \"\n",
        "                f\"(batch: {batch.keys()}) \"\n",
        "                f\"(image_features: {self.config.image_features})\"\n",
        "            )\n",
        "\n",
        "        # Preprocess image features present in the batch\n",
        "        # present_img_keys: ['top', 'left', 'right']\n",
        "        for key in present_img_keys:\n",
        "            # img shape: (B, H, W, C)\n",
        "            img = batch[key]\n",
        "\n",
        "            # Ensure tensor is on the same device as the model\n",
        "            if img.device != device:\n",
        "                img = img.to(device)\n",
        "\n",
        "            # Ensure float32 dtype for consistency\n",
        "            if img.dtype != torch.float32:\n",
        "                img = img.to(torch.float32)\n",
        "\n",
        "            # from openpi preprocess_observation_pytorch:\n",
        "            # Handle both [B, C, H, W] and [B, H, W, C] formats\n",
        "            is_channels_first = img.shape[1] == 3\n",
        "            # Check if channels are in dimension 1\n",
        "\n",
        "            if is_channels_first:\n",
        "                # Convert [B, C, H, W] to [B, H, W, C] for processing\n",
        "                img = img.permute(0, 2, 3, 1)\n",
        "\n",
        "            # from openpi preprocess_observation_pytorch:\n",
        "            # Resize with padding if needed\n",
        "            if img.shape[1:3] != self.config.image_resolution:\n",
        "                img = resize_with_pad_torch(img, *self.config.image_resolution)\n",
        "\n",
        "            # Normalize from [0,1] to [-1,1] as expected by siglip\n",
        "            img = img * 2.0 - 1.0\n",
        "\n",
        "            # Convert back to [B, C, H, W] format if it was originally C-first\n",
        "            if is_channels_first:\n",
        "                img = img.permute(0, 3, 1, 2)  # [B, H, W, C] -> [B, C, H, W]\n",
        "\n",
        "            images.append(img)\n",
        "            # Create mask (all ones for real images)\n",
        "            bsize = img.shape[0]\n",
        "            mask = torch.ones(bsize, dtype=torch.bool, device=device)\n",
        "            img_masks.append(mask)\n",
        "\n",
        "        # Create image features not present in\n",
        "        # the batch as fully 0 padded images\n",
        "        for _num_empty_cameras in range(len(missing_img_keys)):\n",
        "            img = torch.ones_like(img) * -1  # Padded with -1 for SigLIP\n",
        "            mask = torch.zeros_like(mask)  # Mask is zero for empty cameras\n",
        "            images.append(img)\n",
        "            img_masks.append(mask)\n",
        "\n",
        "        return images, img_masks\n",
        "\n",
        "    def prepare_action(self, batch):\n",
        "        \"\"\"Pad action\"\"\"\n",
        "        actions = pad_vector(batch[ACTION], self.config.max_action_dim)\n",
        "        return actions\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def select_action(self, batch: dict[str, Tensor]) -> Tensor:\n",
        "        \"\"\"\n",
        "        Select a single action given environment observations.\n",
        "        Args:\n",
        "            batch: dict[str, Tensor] - batch of environment observations\n",
        "\n",
        "        Returns:\n",
        "            action: (action_dim) - selected action\n",
        "        \"\"\"\n",
        "        assert not self._rtc_enabled(), (\n",
        "            \"RTC is not supported for select_action,\"\n",
        "            \" use it with predict_action_chunk\"\n",
        "        )\n",
        "\n",
        "        self.eval()\n",
        "\n",
        "        # Action queue logic for n_action_steps > 1\n",
        "        if len(self._action_queue) == 0:\n",
        "            actions = self.predict_action_chunk(batch)[\n",
        "                :, : self.config.n_action_steps\n",
        "            ]\n",
        "            # Transpose to get shape (n_action_steps, batch_size, action_dim)\n",
        "            self._action_queue.extend(actions.transpose(0, 1))\n",
        "\n",
        "        return self._action_queue.popleft()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict_action_chunk(self, batch: dict[str, Tensor],\n",
        "                             **kwargs: Unpack[ActionSelectKwargs]) -> Tensor:\n",
        "        \"\"\"Predict a chunk of actions given environment observations.\n",
        "        Args:\n",
        "            batch: dict[str, Tensor] - batch of environment observations\n",
        "            kwargs: Additional arguments\n",
        "\n",
        "        Returns:\n",
        "            actions: (B, action_horizon, action_dim)\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "\n",
        "        # Prepare inputs\n",
        "        images, img_masks = self._preprocess_images(batch)\n",
        "        tokens = batch[f\"{OBS_LANGUAGE_TOKENS}\"]\n",
        "        masks = batch[f\"{OBS_LANGUAGE_ATTENTION_MASK}\"]\n",
        "\n",
        "        subtask_condition = batch.get(\"subtask_condition\", None)\n",
        "\n",
        "        if subtask_condition:\n",
        "            subtask_tokens, subtask_past_key_values = self.model.sample_low_level_task(\n",
        "                images, img_masks, tokens, masks\n",
        "            )\n",
        "\n",
        "            actions = self.model.sample_actions(\n",
        "                past_key_values=subtask_past_key_values,\n",
        "            )\n",
        "\n",
        "        actions = self.model.sample_actions(\n",
        "            images, img_masks, tokens, masks, **kwargs\n",
        "        )\n",
        "\n",
        "        # Unpad actions to actual action dimension\n",
        "        original_action_dim = self.config.output_features[ACTION].shape[0]\n",
        "        actions = actions[:, :, :original_action_dim]\n",
        "\n",
        "        return actions\n",
        "\n",
        "    def forward(self, batch: dict[str, Tensor]) -> tuple[Tensor, dict]:\n",
        "        \"\"\"\n",
        "        Run the batch through the model\n",
        "        and compute the loss for training.\n",
        "\n",
        "        Args:\n",
        "          batch: dict[str, Tensor]\n",
        "            - images: [B, C, H, W] image tensors\n",
        "            - img_masks: [B] image mask tensors\n",
        "            - tokens: [B, N] tokenized prompt tokens (includes prefix + suffix)\n",
        "            - masks: [B, N] token padding masks\n",
        "            - actions: [B, N, action_dim] actions\n",
        "            - token_loss_mask: [B, N] loss mask (True where we compute loss)\n",
        "\n",
        "        Returns:\n",
        "          tuple[Tensor, dict]\n",
        "            - loss: [B] per-sample loss\n",
        "            - loss_dict: dict\n",
        "              - loss: float\n",
        "              - loss_per_sample: [B] per-sample loss\n",
        "        \"\"\"\n",
        "        # Prepare inputs\n",
        "        images, img_masks = self._preprocess_images(batch)\n",
        "        tokens, masks = batch[\n",
        "            f\"{OBS_LANGUAGE_TOKENS}\"], batch[f\"{OBS_LANGUAGE_ATTENTION_MASK}\"]\n",
        "\n",
        "        actions = self.prepare_action(batch)\n",
        "\n",
        "        # Get token loss mask if available (for cross-entropy loss on tokens)\n",
        "        token_loss_mask = batch.get(\"token_loss_mask\", None)\n",
        "\n",
        "        # Get real action dimension (for handling padded actions)\n",
        "        original_action_dim = self.config.output_features[ACTION].shape[0]\n",
        "\n",
        "        # Compute loss (includes both CE loss for tokens and\n",
        "        # flow matching loss for actions)\n",
        "        losses = self.model.forward(\n",
        "            images, img_masks, tokens, masks, actions,\n",
        "            token_loss_mask=token_loss_mask,\n",
        "            real_action_dim=original_action_dim\n",
        "        )\n",
        "\n",
        "        # losses shape: (B,) - per-sample total loss (CE + flow matching)\n",
        "        loss = losses.mean()\n",
        "\n",
        "        loss_dict = {\n",
        "            \"loss\": loss.item(),\n",
        "            \"loss_per_sample\": losses.detach().cpu().numpy().tolist(),\n",
        "        }\n",
        "\n",
        "        return loss, loss_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378,
          "referenced_widgets": [
            "fc658bdf7b124f718ec19a3a3973a4c9",
            "0d6204dfffb44402b4d23a777f6fe735",
            "34b425b87c39459fa1a618eac9cbe098",
            "f514c38b6277467faf7c232f1de6c5ba",
            "15c134fb254449ae99d93ad87fc3615e",
            "cab5ac418e674705b890326b177b5a5d",
            "ce52fe31786842fa8cd9b39016e53d1b",
            "dacc8ceae3f94a828247a5db584c80b4",
            "2c1739a5a15e4bf98e81d49a40fb6448",
            "2b2479e713f648cf927bb7a57c58fe99",
            "5c8639cea21a41feafa33d5b737a6b4d",
            "5269d5e9ae4742d28dffb5a7a2d2ad3a",
            "6e731d21d556420b97ef078b21ccbaff",
            "dfff16bb373c4f0898a405362f9b433f",
            "5dea5d8829414ce99f4c00b6fa2a7542",
            "5ab2e30001fb4d56877b948668d53854",
            "baf1a2523e494fb6b4e73b58e23fa018",
            "f7eb6178deec4ba8b4512d581479fdff",
            "3af1a767a5be49a0bd01074f27d07918",
            "64e8f7cde9aa48c199a1b7f892099d14",
            "c1387265401646a1ba74a76c0ec5dade",
            "a26ba53b2f144a56b8a546831a80e7dd",
            "8035e16955b14ed78d49466f0e33b14a",
            "84524bd868e74d209b591c04a6b7e4ec",
            "1c8bd00a5b22444c84c5f722365bed69",
            "26faf003c0004a8ea19ef734a3c120b2",
            "aad6ff78d1dc44e388dbe0fedbe783c9",
            "a46934adfdec41ef9e426cd88a1bd965",
            "464446bbc10549f68ea07d43174c1376",
            "1c20a9fc381e4cfab0eb425821c59e90",
            "4fe63aa057d44b3abf86a6f294b5d7af",
            "09d63640e916408cacf3ab1489433476",
            "f36df089958c491ca4883845526d59a1",
            "d01aa132b2464d1e87373a021b144053",
            "d8ba975f46da4762a444fd5b91e4d83c",
            "d9e5c9001f854b6cacae7287d5807200",
            "0fcb2f9fe84d4b959a5f91c0c62c3c7a",
            "482db021888648ed8954ceb8f9b049c5",
            "a2657c70c94c4a408f3724901bbcbd59",
            "babdfbde1a1146a8b8414dca8110648d",
            "78609182e21b4fa683c11ec0d637ec0a",
            "cd20d6f9eb774b6190f1af949de68efe",
            "c1a3c3fa3ad54faa80f1d48e33e39ded",
            "fe7bd857555b462792d515817020c806",
            "9db2de0fda644ea6877a9c48b8343fda",
            "5ffb8a5dda7141bb9d5c57022c001f92",
            "1499ade83860452187b973a7d913ee80",
            "1c27978b487a4399be988721fab6a672",
            "a7cc6d3ecbe841d48bdbf06cca332bcf",
            "7dac0c8bf86e4d67a65aea356497e38a",
            "ee41465d42ba4fa9b7de1f7baf106f8e",
            "75bb2630ed0f48f6aa4c0ec7673618bd",
            "867d696c086e438db873b68d65b09e1b",
            "82a407228de9499c94e4dba9e18b9965",
            "104e5de9e3824c279d8dede026b46e89"
          ]
        },
        "id": "gp93wBnKO8jp",
        "outputId": "eac2a300-ac12-4918-998f-913966e2105b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading tokenizer...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fc658bdf7b124f718ec19a3a3973a4c9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/40.0k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5269d5e9ae4742d28dffb5a7a2d2ad3a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.26M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8035e16955b14ed78d49466f0e33b14a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/24.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d01aa132b2464d1e87373a021b144053",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/607 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9db2de0fda644ea6877a9c48b8343fda",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "# Load tokenizer\n",
        "print(\"Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/paligemma-3b-pt-224\",\n",
        "                                          use_fast=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "df88f78d1cca4239984b4cdfe357a63c",
            "d364090f18cd4fb6b22d33ef7d904a3c",
            "ff111c65db4d4d99b1d4ffb817b001ef",
            "66f1142e575c43c2950a1d4706f9dfeb",
            "2410335e9d1d42e4ba7acf2d6faa3856",
            "9a12a1c0010d4ae7a36cabc0f7ec42e1",
            "e3b7f9f144d8418fa6af2486e43b9fc7",
            "87bae00937484569b22b274780d068d0",
            "fde77f9987ac4f309e62dbbbbff07f05",
            "d13a8525e7314f2292758724004d40b7",
            "25d200f164794cd69baad2412e85f657",
            "832bab4e846e44feb1f108828c1698c6",
            "ff066dd95fa448a586c4363bf1b91ae1",
            "ba37fbea0c9d46cfb91ef291ebcbcbf5",
            "3a5b9b098662440da16f606dd8d6418e",
            "9f22fad86bd74b089462f4b334933953",
            "893e44e9ca4b4a178c67a9aff17547ac",
            "136ce2d3e4b24283808c9b8006c185f8",
            "c2e7261ae3be4a3cb3f3b991e4f2cf1c",
            "43ba99f3f53c4b0a99b1efad636e774a",
            "0c833d4fdd2344fa95758d2b9074b3cf",
            "cedce10b573543d8a7daa8ac3eb309c3"
          ]
        },
        "id": "y3WH1evWO89q",
        "outputId": "eb38e274-73fe-4546-a086-fe192f57b63b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading policy...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "df88f78d1cca4239984b4cdfe357a63c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:lerobot.configs.policies:Device 'mps' is not available. Switching to 'cuda'.\n",
            "WARNING:lerobot.configs.policies:Device 'mps' is not available. Switching to 'cuda'.\n",
            "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model from: lerobot/pi05_base\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "832bab4e846e44feb1f108828c1698c6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/14.5G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Vision embedding key might need handling: paligemma_with_expert.paligemma.model.vision_tower.vision_model.embeddings.patch_embedding.bias\n",
            "WARNING:root:Vision embedding key might need handling: paligemma_with_expert.paligemma.model.vision_tower.vision_model.embeddings.patch_embedding.weight\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Loaded state dict from model.safetensors\n",
            "Remapped: action_in_proj.bias -> model.action_in_proj.bias\n",
            "Remapped: action_in_proj.weight -> model.action_in_proj.weight\n",
            "Remapped: action_out_proj.bias -> model.action_out_proj.bias\n",
            "Remapped: action_out_proj.weight -> model.action_out_proj.weight\n",
            "Remapped: paligemma_with_expert.gemma_expert.lm_head.weight -> model.paligemma_with_expert.gemma_expert.lm_head.weight\n",
            "Remapped: paligemma_with_expert.gemma_expert.model.layers.0.input_layernorm.dense.bias -> model.paligemma_with_expert.gemma_expert.model.layers.0.input_layernorm.dense.bias\n",
            "Remapped: paligemma_with_expert.gemma_expert.model.layers.0.input_layernorm.dense.weight -> model.paligemma_with_expert.gemma_expert.model.layers.0.input_layernorm.dense.weight\n",
            "Remapped: paligemma_with_expert.gemma_expert.model.layers.0.mlp.down_proj.weight -> model.paligemma_with_expert.gemma_expert.model.layers.0.mlp.down_proj.weight\n",
            "Remapped: paligemma_with_expert.gemma_expert.model.layers.0.mlp.gate_proj.weight -> model.paligemma_with_expert.gemma_expert.model.layers.0.mlp.gate_proj.weight\n",
            "Remapped: paligemma_with_expert.gemma_expert.model.layers.0.mlp.up_proj.weight -> model.paligemma_with_expert.gemma_expert.model.layers.0.mlp.up_proj.weight\n",
            "Remapped 812 state dict keys\n",
            "Warning: Could not remap state dict keys: Error(s) in loading state_dict for PI05Policy:\n",
            "\tMissing key(s) in state_dict: \"model.paligemma_with_expert.paligemma.model.language_model.embed_tokens.weight\". \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "PI05Model(\n",
              "  (paligemma_with_expert): PaliGemmaWithExpertModel(\n",
              "    (paligemma): PaliGemmaForConditionalGeneration(\n",
              "      (model): PaliGemmaModel(\n",
              "        (vision_tower): SiglipVisionModel(\n",
              "          (vision_model): SiglipVisionTransformer(\n",
              "            (embeddings): SiglipVisionEmbeddings(\n",
              "              (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
              "              (position_embedding): Embedding(256, 1152)\n",
              "            )\n",
              "            (encoder): SiglipEncoder(\n",
              "              (layers): ModuleList(\n",
              "                (0-26): 27 x SiglipEncoderLayer(\n",
              "                  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "                  (self_attn): SiglipAttention(\n",
              "                    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
              "                    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
              "                    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
              "                    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
              "                  )\n",
              "                  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "                  (mlp): SiglipMLP(\n",
              "                    (activation_fn): PytorchGELUTanh()\n",
              "                    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
              "                    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "        (multi_modal_projector): PaliGemmaMultiModalProjector(\n",
              "          (linear): Linear(in_features=1152, out_features=2048, bias=True)\n",
              "        )\n",
              "        (language_model): GemmaModel(\n",
              "          (embed_tokens): Embedding(257152, 2048, padding_idx=0)\n",
              "          (layers): ModuleList(\n",
              "            (0-17): 18 x GemmaDecoderLayer(\n",
              "              (self_attn): GemmaAttention(\n",
              "                (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "                (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
              "                (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
              "                (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "              )\n",
              "              (mlp): GemmaMLP(\n",
              "                (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
              "                (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
              "                (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
              "                (act_fn): PytorchGELUTanh()\n",
              "              )\n",
              "              (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
              "              (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
              "            )\n",
              "          )\n",
              "          (norm): GemmaRMSNorm((2048,), eps=1e-06)\n",
              "          (rotary_emb): GemmaRotaryEmbedding()\n",
              "        )\n",
              "      )\n",
              "      (lm_head): Linear(in_features=2048, out_features=257152, bias=False)\n",
              "    )\n",
              "    (gemma_expert): GemmaForCausalLM(\n",
              "      (model): GemmaModel(\n",
              "        (embed_tokens): None\n",
              "        (layers): ModuleList(\n",
              "          (0-17): 18 x GemmaDecoderLayer(\n",
              "            (self_attn): GemmaAttention(\n",
              "              (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
              "              (k_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
              "              (v_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
              "              (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
              "            )\n",
              "            (mlp): GemmaMLP(\n",
              "              (gate_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "              (up_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "              (down_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (act_fn): PytorchGELUTanh()\n",
              "            )\n",
              "            (input_layernorm): GemmaRMSNorm(\n",
              "              eps=1e-06, adaptive=True, cond_dim=1024\n",
              "              (dense): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            )\n",
              "            (post_attention_layernorm): GemmaRMSNorm(\n",
              "              eps=1e-06, adaptive=True, cond_dim=1024\n",
              "              (dense): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (norm): GemmaRMSNorm(\n",
              "          eps=1e-06, adaptive=True, cond_dim=1024\n",
              "          (dense): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "        )\n",
              "        (rotary_emb): GemmaRotaryEmbedding()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=1024, out_features=257152, bias=False)\n",
              "    )\n",
              "  )\n",
              "  (action_in_proj): Linear(in_features=32, out_features=1024, bias=True)\n",
              "  (action_out_proj): Linear(in_features=1024, out_features=32, bias=True)\n",
              "  (time_mlp_in): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "  (time_mlp_out): Linear(in_features=1024, out_features=1024, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "MODEL_ID = \"lerobot/pi05_base\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "PALIGEMMA_EOS_TOKEN = 1\n",
        "max_decoding_steps = 40\n",
        "temperature = 0.1\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# FIX for AttributeError: 'GemmaRMSNorm' object has no attribute 'weight'\n",
        "# The custom transformers branch uses adaRMS which removes the weight parameter,\n",
        "# but the __repr__ method expects it. We monkey-patch it here.\n",
        "from transformers.models.gemma import modeling_gemma\n",
        "\n",
        "def fixed_extra_repr(self):\n",
        "    if hasattr(self, \"weight\"):\n",
        "        repr_str = f\"{tuple(self.weight.shape)}, eps={self.eps}\"\n",
        "    else:\n",
        "        repr_str = f\"eps={self.eps}\"\n",
        "    if getattr(self, \"dense\", None) is not None:\n",
        "        repr_str += f\", adaptive=True, cond_dim={self.cond_dim}\"\n",
        "    return repr_str\n",
        "\n",
        "modeling_gemma.GemmaRMSNorm.extra_repr = fixed_extra_repr\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "print(\"Loading policy...\")\n",
        "policy = PI05Policy.from_pretrained(MODEL_ID)\n",
        "policy.model.eval()\n",
        "policy.model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "y-rRAwC_PHNP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "img_share_path = '/content'\n",
        "img_name_list = ['top.png']\n",
        "img_list = []\n",
        "\n",
        "if img_share_path and os.path.exists(img_share_path):\n",
        "    # Load images from path\n",
        "    for img_name in img_name_list:\n",
        "        img_path = os.path.join(img_share_path, img_name)\n",
        "        if os.path.exists(img_path):\n",
        "            img = cv2.imread(img_path)\n",
        "            if img is not None:\n",
        "                # Convert BGR to RGB\n",
        "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "                img_list.append(img)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsWSWCu8PkXF",
        "outputId": "59e89446-7831-445e-cc54-2acb53075442"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizing prompt: 'Put the fruits in the basket\n",
            "Sub task:'\n"
          ]
        }
      ],
      "source": [
        "from lerobot.utils.constants import OBS_LANGUAGE_TOKENS, OBS_LANGUAGE_ATTENTION_MASK\n",
        "\n",
        "batch = {}\n",
        "if len(img_list) > 0:\n",
        "    # Prepare image tensor: [H, W, C] -> [1, H, W, C]\n",
        "    # Normalize to [0, 1] as expected by _preprocess_images (which then scales to [-1, 1])\n",
        "    img_tensor = torch.from_numpy(img_list[0]).float() / 255.0\n",
        "    img_tensor = img_tensor.unsqueeze(0) # Add batch dimension -> [1, H, W, C]\n",
        "\n",
        "    batch[\"observation.images.base_0_rgb\"] = img_tensor\n",
        "\n",
        "# Process Language Prompt\n",
        "# Note: PI05 automatically prepends image embeddings, so we DO NOT need an <image> token in the text.\n",
        "# However, we ensure the prompt ends with a newline for better model performance.\n",
        "prompt = \"Put the fruits in the basket\\nSub task: \"\n",
        "if not prompt.endswith(\"\\n\"):\n",
        "    prompt += \"\\n\"\n",
        "\n",
        "print(f\"Tokenizing prompt: '{prompt.strip()}'\")\n",
        "tokenized = tokenizer(\n",
        "    prompt,\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=512\n",
        ")\n",
        "\n",
        "# Add to batch (converting mask to bool for compatibility)\n",
        "batch[OBS_LANGUAGE_TOKENS] = tokenized[\"input_ids\"]\n",
        "batch[OBS_LANGUAGE_ATTENTION_MASK] = tokenized[\"attention_mask\"].bool()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "tj0a6aXwPp2m"
      },
      "outputs": [],
      "source": [
        "processed_images, img_masks = policy._preprocess_images(batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpG-FpqqQMSe",
        "outputId": "b0425fff-2d10-4c05-fd63-fb56b4ce9d8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generation Start...\n",
            "\n",
            "Step 0:\n",
            "  Batch 0 Top 5:\n",
            "    1: 'pick' (p=0.6136)\n",
            "    2: 'move' (p=0.0564)\n",
            "    3: 'No' (p=0.0170)\n",
            "    4: 'Yes' (p=0.0147)\n",
            "    5: 'no' (p=0.0110)\n",
            "    6: '<eos>' (p=0.0085)\n",
            "    7: ' to' (p=0.0074)\n",
            "    8: 'adjust' (p=0.0064)\n",
            "    9: '  ' (p=0.0060)\n",
            "    10: 'place' (p=0.0058)\n",
            "    11: 'put' (p=0.0045)\n",
            "    12: ' in' (p=0.0039)\n",
            "    13: ' and' (p=0.0029)\n",
            "    14: 'yes' (p=0.0028)\n",
            "    15: 'return' (p=0.0024)\n",
            "    16: ' up' (p=0.0024)\n",
            "    17: ' pick' (p=0.0022)\n",
            "    18: ' on' (p=0.0018)\n",
            "    19: 'The' (p=0.0018)\n",
            "    20: ' the' (p=0.0016)\n",
            "\n",
            "Step 1:\n",
            "  Batch 0 Top 5:\n",
            "    1: '<eos>' (p=0.1080)\n",
            "    2: ' fruit' (p=0.0573)\n",
            "    3: '\n",
            "' (p=0.0290)\n",
            "    4: '>' (p=0.0257)\n",
            "    5: ' fruits' (p=0.0241)\n",
            "    6: ' all' (p=0.0231)\n",
            "    7: ' up' (p=0.0167)\n",
            "    8: ' and' (p=0.0165)\n",
            "    9: ' position' (p=0.0161)\n",
            "    10: ' metal' (p=0.0126)\n",
            "    11: ' three' (p=0.0117)\n",
            "    12: ' two' (p=0.0113)\n",
            "    13: ' to' (p=0.0103)\n",
            "    14: ' placed' (p=0.0101)\n",
            "    15: ' left' (p=0.0098)\n",
            "    16: ' the' (p=0.0088)\n",
            "    17: ' items' (p=0.0088)\n",
            "    18: ' not' (p=0.0077)\n",
            "    19: 'Task' (p=0.0076)\n",
            "    20: ' objects' (p=0.0068)\n",
            "\n",
            "Step 2:\n",
            "  Batch 0 Top 5:\n",
            "    1: '<eos>' (p=0.1701)\n",
            "    2: '  ' (p=0.0358)\n",
            "    3: ' and' (p=0.0297)\n",
            "    4: ' up' (p=0.0221)\n",
            "    5: ' pick' (p=0.0193)\n",
            "    6: ' to' (p=0.0173)\n",
            "    7: ' in' (p=0.0113)\n",
            "    8: ' basket' (p=0.0080)\n",
            "    9: ' on' (p=0.0077)\n",
            "    10: ' -' (p=0.0076)\n",
            "    11: ' put' (p=0.0075)\n",
            "    12: ' the' (p=0.0070)\n",
            "    13: '\n",
            "' (p=0.0067)\n",
            "    14: ',' (p=0.0061)\n",
            "    15: 'â‹' (p=0.0059)\n",
            "    16: ' a' (p=0.0056)\n",
            "    17: ' drawer' (p=0.0052)\n",
            "    18: ' home' (p=0.0042)\n",
            "    19: ' into' (p=0.0040)\n",
            "    20: ' ' (p=0.0040)\n",
            "\n",
            "Step 3:\n",
            "  Batch 0 Top 5:\n",
            "    1: ' aid' (p=0.0498)\n",
            "    2: ' knot' (p=0.0465)\n",
            "    3: ' hook' (p=0.0460)\n",
            "    4: '\n",
            "' (p=0.0426)\n",
            "    5: '<eos>' (p=0.0363)\n",
            "    6: 'Is' (p=0.0252)\n",
            "    7: ' pack' (p=0.0199)\n",
            "    8: ' p' (p=0.0178)\n",
            "    9: ' fold' (p=0.0169)\n",
            "    10: ' frame' (p=0.0134)\n",
            "    11: ' can' (p=0.0124)\n",
            "    12: '  ' (p=0.0118)\n",
            "    13: 'task' (p=0.0116)\n",
            "    14: ' ' (p=0.0112)\n",
            "    15: ' C' (p=0.0111)\n",
            "    16: ' tools' (p=0.0110)\n",
            "    17: ' pull' (p=0.0108)\n",
            "    18: ' region' (p=0.0104)\n",
            "    19: ' throw' (p=0.0089)\n",
            "    20: ' table' (p=0.0082)\n",
            "\n",
            "Step 4:\n",
            "  Batch 0 Top 5:\n",
            "    1: '<eos>' (p=0.1831)\n",
            "    2: '  ' (p=0.0622)\n",
            "    3: '.' (p=0.0601)\n",
            "    4: ' ' (p=0.0274)\n",
            "    5: ' in' (p=0.0197)\n",
            "    6: ' can' (p=0.0187)\n",
            "    7: ' <' (p=0.0147)\n",
            "    8: ' on' (p=0.0138)\n",
            "    9: ' the' (p=0.0130)\n",
            "    10: ' holder' (p=0.0125)\n",
            "    11: ' with' (p=0.0110)\n",
            "    12: ' blue' (p=0.0097)\n",
            "    13: ' '' (p=0.0093)\n",
            "    14: ' move' (p=0.0088)\n",
            "    15: ' body' (p=0.0083)\n",
            "    16: ' The' (p=0.0079)\n",
            "    17: ' rotate' (p=0.0072)\n",
            "    18: '7' (p=0.0071)\n",
            "    19: '\n",
            "' (p=0.0066)\n",
            "    20: ';' (p=0.0057)\n",
            "\n",
            "Step 5:\n",
            "  Batch 0 Top 5:\n",
            "    1: 'open' (p=0.1841)\n",
            "    2: ' q' (p=0.0343)\n",
            "    3: ' cam' (p=0.0331)\n",
            "    4: 'Sub' (p=0.0287)\n",
            "    5: 'a' (p=0.0164)\n",
            "    6: ' basket' (p=0.0142)\n",
            "    7: ' chip' (p=0.0131)\n",
            "    8: 'pull' (p=0.0091)\n",
            "    9: 'un' (p=0.0088)\n",
            "    10: ' <' (p=0.0088)\n",
            "    11: ' energy' (p=0.0087)\n",
            "    12: ' '' (p=0.0087)\n",
            "    13: 'â“š' (p=0.0080)\n",
            "    14: ' shoes' (p=0.0078)\n",
            "    15: ' surfaces' (p=0.0075)\n",
            "    16: ' cotton' (p=0.0073)\n",
            "    17: ' a' (p=0.0066)\n",
            "    18: 'Yes' (p=0.0061)\n",
            "    19: 'flatten' (p=0.0057)\n",
            "    20: '  ' (p=0.0055)\n",
            "\n",
            "Step 6:\n",
            "  Batch 0 Top 5:\n",
            "    1: 'à»‘' (p=0.2600)\n",
            "    2: '<eos>' (p=0.0463)\n",
            "    3: ' and' (p=0.0265)\n",
            "    4: '  ' (p=0.0105)\n",
            "    5: ' red' (p=0.0105)\n",
            "    6: '1' (p=0.0090)\n",
            "    7: ' ' (p=0.0075)\n",
            "    8: ' blue' (p=0.0067)\n",
            "    9: ' top' (p=0.0066)\n",
            "    10: ' put' (p=0.0063)\n",
            "    11: '.' (p=0.0061)\n",
            "    12: ' the' (p=0.0055)\n",
            "    13: ' move' (p=0.0047)\n",
            "    14: ' orange' (p=0.0040)\n",
            "    15: ' green' (p=0.0035)\n",
            "    16: ' with' (p=0.0031)\n",
            "    17: '0' (p=0.0030)\n",
            "    18: ' pack' (p=0.0028)\n",
            "    19: ' book' (p=0.0028)\n",
            "    20: ' into' (p=0.0027)\n",
            "\n",
            "Step 7:\n",
            "  Batch 0 Top 5:\n",
            "    1: '<eos>' (p=0.6036)\n",
            "    2: ',' (p=0.0726)\n",
            "    3: '.' (p=0.0170)\n",
            "    4: ' see' (p=0.0107)\n",
            "    5: ':' (p=0.0098)\n",
            "    6: ';' (p=0.0083)\n",
            "    7: '\n",
            "' (p=0.0077)\n",
            "    8: ' the' (p=0.0055)\n",
            "    9: 'â‹' (p=0.0051)\n",
            "    10: ' color' (p=0.0049)\n",
            "    11: ' whole' (p=0.0045)\n",
            "    12: ' pick' (p=0.0043)\n",
            "    13: ' a' (p=0.0039)\n",
            "    14: ' hand' (p=0.0039)\n",
            "    15: ' take' (p=0.0034)\n",
            "    16: ' coaster' (p=0.0030)\n",
            "    17: ' hitch' (p=0.0029)\n",
            "    18: ' peel' (p=0.0025)\n",
            "    19: '>' (p=0.0024)\n",
            "    20: ' put' (p=0.0024)\n",
            "\n",
            "Output tokens shape: torch.Size([1, 40])\n",
            "Output tokens:\n",
            "tensor([[  2931,    108, 235265,  11288,  13092, 255667,    578,      1,      0,\n",
            "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "              0,      0,      0,      0]], device='cuda:0')\n",
            "\n",
            "================================================================================\n",
            "High Level Prompt\n",
            " Put the fruits in the basket\n",
            "Sub task:\n",
            "\n",
            "Decoded output:\n",
            "================================================================================\n",
            "Batch 0: move\n",
            ". drawn vehiclesÓ” and<eos>\n",
            "\n",
            "================================================================================\n",
            "Test completed successfully!\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    # FIX: Ensure images are in (B, C, H, W) format\n",
        "    # The error \"expected input... to have 3 channels, but got 224\" occurs because\n",
        "    # the input was (B, H, W, C) but the model expects (B, C, H, W).\n",
        "    fixed_processed_images = []\n",
        "    for img in processed_images:\n",
        "        # Check if shape is (B, H, W, C=3) instead of (B, C=3, H, W)\n",
        "        if img.ndim == 4 and img.shape[-1] == 3 and img.shape[1] != 3:\n",
        "            img = img.permute(0, 3, 1, 2)\n",
        "        fixed_processed_images.append(img)\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Manual Sampling Loop to inspect Top 5 Tokens\n",
        "    # -------------------------------------------------------------------------\n",
        "    import torch.nn.functional as F\n",
        "\n",
        "    # 1. Embed prefix\n",
        "    prefix_embs, prefix_pad_masks, prefix_att_masks = policy.model.embed_prefix(\n",
        "        fixed_processed_images, img_masks, tokens, masks.bool()\n",
        "    )\n",
        "\n",
        "    # 2. Prepare masks\n",
        "    # make_att_2d_masks is in global scope from previous cell execution\n",
        "    prefix_att_2d_masks = make_att_2d_masks(prefix_pad_masks, prefix_att_masks)\n",
        "    prefix_position_ids = torch.cumsum(prefix_pad_masks, dim=1) - 1\n",
        "    prefix_att_2d_masks_4d = policy.model._prepare_attention_masks_4d(prefix_att_2d_masks)\n",
        "\n",
        "    # 3. Initial Forward Pass\n",
        "    lang_model = policy.model.paligemma_with_expert.paligemma.language_model\n",
        "    lang_model.config._attn_implementation = \"eager\"\n",
        "\n",
        "    embeddings, past_key_values = policy.model.paligemma_with_expert.forward(\n",
        "        attention_mask=prefix_att_2d_masks_4d,\n",
        "        position_ids=prefix_position_ids,\n",
        "        past_key_values=None,\n",
        "        inputs_embeds=[prefix_embs, None],\n",
        "        use_cache=True,\n",
        "    )\n",
        "\n",
        "    last_token_embed = embeddings[0][:, -1, :]\n",
        "    last_logits = policy.model.paligemma_with_expert.paligemma.lm_head(last_token_embed)\n",
        "\n",
        "    batch_size = last_logits.shape[0]\n",
        "    device = last_logits.device\n",
        "\n",
        "    prefix_valid_length = torch.sum(prefix_pad_masks, dim=1)\n",
        "    output_tokens = torch.zeros((batch_size, max_decoding_steps), dtype=torch.long, device=device)\n",
        "    all_eos = torch.zeros(batch_size, dtype=torch.bool, device=device)\n",
        "\n",
        "    running_attention_mask = prefix_pad_masks.clone()\n",
        "\n",
        "    print(\"Generation Start...\")\n",
        "\n",
        "    # Autoregressive Loop\n",
        "    for step in range(max_decoding_steps):\n",
        "        # --- Inspect Probabilities ---\n",
        "        probs_all = F.softmax(last_logits, dim=-1)\n",
        "        top_probs, top_indices = torch.topk(probs_all, 20, dim=-1)\n",
        "\n",
        "        print(f\"\\nStep {step}:\")\n",
        "        for b in range(batch_size):\n",
        "            print(f\"  Batch {b} Top 5:\")\n",
        "            for k in range(20):\n",
        "                token_idx = top_indices[b, k].item()\n",
        "                prob = top_probs[b, k].item()\n",
        "                token_str = tokenizer.decode([token_idx])\n",
        "                print(f\"    {k+1}: '{token_str}' (p={prob:.4f})\")\n",
        "        # -----------------------------\n",
        "\n",
        "        # Sample next token\n",
        "        current_temp = 0.9 # Using 0.7 as in the original cell call\n",
        "        if current_temp > 0.0:\n",
        "            probs = F.softmax(last_logits / current_temp, dim=-1)\n",
        "            token = torch.multinomial(probs, num_samples=1)\n",
        "        else:\n",
        "            token = torch.argmax(last_logits, dim=-1, keepdim=True)\n",
        "\n",
        "        output_tokens[:, step] = token.squeeze(-1)\n",
        "\n",
        "        # Check for EOS\n",
        "        all_eos |= (token.squeeze(-1) == PALIGEMMA_EOS_TOKEN)\n",
        "        if all_eos.all():\n",
        "            break\n",
        "\n",
        "        # Feed the new token back in the model\n",
        "        next_token_embeds = lang_model.embed_tokens(token)\n",
        "        position_ids = prefix_valid_length[:, None] + step\n",
        "\n",
        "        new_mask = torch.ones((batch_size, 1), dtype=running_attention_mask.dtype, device=device)\n",
        "        running_attention_mask = torch.cat([running_attention_mask, new_mask], dim=1)\n",
        "\n",
        "        embeds_list, past_key_values = policy.model.paligemma_with_expert.forward(\n",
        "            inputs_embeds=[next_token_embeds, None],\n",
        "            attention_mask=running_attention_mask,\n",
        "            position_ids=position_ids,\n",
        "            past_key_values=past_key_values,\n",
        "            use_cache=True,\n",
        "        )\n",
        "\n",
        "        prefix_output = embeds_list[0]\n",
        "        last_token_embed = prefix_output[:, -1, :]\n",
        "        last_logits = policy.model.paligemma_with_expert.paligemma.lm_head(last_token_embed)\n",
        "\n",
        "    print(f\"\\nOutput tokens shape: {output_tokens.shape}\")\n",
        "    print(f\"Output tokens:\\n{output_tokens}\")\n",
        "\n",
        "    # Decode the generated tokens\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"High Level Prompt\\n\",high_level_prompt)\n",
        "    print(\"Decoded output:\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    for batch_idx in range(output_tokens.shape[0]):\n",
        "        batch_tokens = output_tokens[batch_idx]\n",
        "        # Remove padding (zeros) and decode\n",
        "        non_zero_tokens = batch_tokens[batch_tokens != 0]\n",
        "        if len(non_zero_tokens) > 0:\n",
        "            decoded_text = tokenizer.decode(non_zero_tokens, skip_special_tokens=False)\n",
        "            print(f\"Batch {batch_idx}: {decoded_text}\")\n",
        "        else:\n",
        "            print(f\"Batch {batch_idx}: (empty)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Test completed successfully!\")\n",
        "print(\"=\"*80)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "03ced8628d9c422ba31cd3deefbffa3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "CheckboxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_f4d781486ec743fd9a1eb0e26d89014a",
            "style": "IPY_MODEL_4d3c71094f00441bbf7f937df6982a4b",
            "value": true
          }
        },
        "09d24dddf93b41bb9769916ea031e4b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "09d63640e916408cacf3ab1489433476": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c7a973c53ac41cb89eb5873fb5139a5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c833d4fdd2344fa95758d2b9074b3cf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d6204dfffb44402b4d23a777f6fe735": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cab5ac418e674705b890326b177b5a5d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ce52fe31786842fa8cd9b39016e53d1b",
            "value": "tokenizer_config.json:â€‡100%"
          }
        },
        "0fcb2f9fe84d4b959a5f91c0c62c3c7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1a3c3fa3ad54faa80f1d48e33e39ded",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_fe7bd857555b462792d515817020c806",
            "value": "â€‡607/607â€‡[00:00&lt;00:00,â€‡86.9kB/s]"
          }
        },
        "104e5de9e3824c279d8dede026b46e89": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "136ce2d3e4b24283808c9b8006c185f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1499ade83860452187b973a7d913ee80": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75bb2630ed0f48f6aa4c0ec7673618bd",
            "max": 17549604,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_867d696c086e438db873b68d65b09e1b",
            "value": 17549604
          }
        },
        "15c134fb254449ae99d93ad87fc3615e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c20a9fc381e4cfab0eb425821c59e90": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c27978b487a4399be988721fab6a672": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82a407228de9499c94e4dba9e18b9965",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_104e5de9e3824c279d8dede026b46e89",
            "value": "â€‡17.5M/17.5Mâ€‡[00:00&lt;00:00,â€‡320kB/s]"
          }
        },
        "1c8bd00a5b22444c84c5f722365bed69": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c20a9fc381e4cfab0eb425821c59e90",
            "max": 24,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4fe63aa057d44b3abf86a6f294b5d7af",
            "value": 24
          }
        },
        "2410335e9d1d42e4ba7acf2d6faa3856": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25d200f164794cd69baad2412e85f657": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "26faf003c0004a8ea19ef734a3c120b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09d63640e916408cacf3ab1489433476",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f36df089958c491ca4883845526d59a1",
            "value": "â€‡24.0/24.0â€‡[00:00&lt;00:00,â€‡3.23kB/s]"
          }
        },
        "2b2479e713f648cf927bb7a57c58fe99": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c1739a5a15e4bf98e81d49a40fb6448": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "330a744c61864587b4f25a96fcdc9e46": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_c8770dacca394e7fb6597aaa58ecdd41",
            "style": "IPY_MODEL_5aaeab307dd247aaac2e36781eef85dc",
            "tooltip": ""
          }
        },
        "34b425b87c39459fa1a618eac9cbe098": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dacc8ceae3f94a828247a5db584c80b4",
            "max": 39968,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2c1739a5a15e4bf98e81d49a40fb6448",
            "value": 39968
          }
        },
        "3a5b9b098662440da16f606dd8d6418e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c833d4fdd2344fa95758d2b9074b3cf",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_cedce10b573543d8a7daa8ac3eb309c3",
            "value": "â€‡14.5G/14.5Gâ€‡[00:38&lt;00:00,â€‡341MB/s]"
          }
        },
        "3af1a767a5be49a0bd01074f27d07918": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43ba99f3f53c4b0a99b1efad636e774a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "464446bbc10549f68ea07d43174c1376": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "482db021888648ed8954ceb8f9b049c5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d3c71094f00441bbf7f937df6982a4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4fe63aa057d44b3abf86a6f294b5d7af": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5269d5e9ae4742d28dffb5a7a2d2ad3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6e731d21d556420b97ef078b21ccbaff",
              "IPY_MODEL_dfff16bb373c4f0898a405362f9b433f",
              "IPY_MODEL_5dea5d8829414ce99f4c00b6fa2a7542"
            ],
            "layout": "IPY_MODEL_5ab2e30001fb4d56877b948668d53854"
          }
        },
        "5aaeab307dd247aaac2e36781eef85dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "5ab2e30001fb4d56877b948668d53854": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c8639cea21a41feafa33d5b737a6b4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5dea5d8829414ce99f4c00b6fa2a7542": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1387265401646a1ba74a76c0ec5dade",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a26ba53b2f144a56b8a546831a80e7dd",
            "value": "â€‡4.26M/4.26Mâ€‡[00:01&lt;00:00,â€‡3.17MB/s]"
          }
        },
        "5ffb8a5dda7141bb9d5c57022c001f92": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7dac0c8bf86e4d67a65aea356497e38a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ee41465d42ba4fa9b7de1f7baf106f8e",
            "value": "tokenizer.json:â€‡100%"
          }
        },
        "64e8f7cde9aa48c199a1b7f892099d14": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "66f1142e575c43c2950a1d4706f9dfeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d13a8525e7314f2292758724004d40b7",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_25d200f164794cd69baad2412e85f657",
            "value": "â€‡1.90k/?â€‡[00:00&lt;00:00,â€‡189kB/s]"
          }
        },
        "671cbd380f8f47abbd52c56bcf3f3cec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c08646bdd1f446ffa06e62187f413fa3",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_09d24dddf93b41bb9769916ea031e4b4",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "6b3cb969da214b55a601bb2632065d5b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "6e731d21d556420b97ef078b21ccbaff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_baf1a2523e494fb6b4e73b58e23fa018",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f7eb6178deec4ba8b4512d581479fdff",
            "value": "tokenizer.model:â€‡100%"
          }
        },
        "7288cec4470a4b3396716326fe9c16e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "PasswordModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_dcf030c1e28e4d939eea04ec182d66c3",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_86d67298962a437886c82fec2e9095b8",
            "value": ""
          }
        },
        "75bb2630ed0f48f6aa4c0ec7673618bd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78609182e21b4fa683c11ec0d637ec0a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7dac0c8bf86e4d67a65aea356497e38a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8035e16955b14ed78d49466f0e33b14a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_84524bd868e74d209b591c04a6b7e4ec",
              "IPY_MODEL_1c8bd00a5b22444c84c5f722365bed69",
              "IPY_MODEL_26faf003c0004a8ea19ef734a3c120b2"
            ],
            "layout": "IPY_MODEL_aad6ff78d1dc44e388dbe0fedbe783c9"
          }
        },
        "82a407228de9499c94e4dba9e18b9965": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "832bab4e846e44feb1f108828c1698c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff066dd95fa448a586c4363bf1b91ae1",
              "IPY_MODEL_ba37fbea0c9d46cfb91ef291ebcbcbf5",
              "IPY_MODEL_3a5b9b098662440da16f606dd8d6418e"
            ],
            "layout": "IPY_MODEL_9f22fad86bd74b089462f4b334933953"
          }
        },
        "84524bd868e74d209b591c04a6b7e4ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a46934adfdec41ef9e426cd88a1bd965",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_464446bbc10549f68ea07d43174c1376",
            "value": "added_tokens.json:â€‡100%"
          }
        },
        "867d696c086e438db873b68d65b09e1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "86d67298962a437886c82fec2e9095b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "87bae00937484569b22b274780d068d0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "893e44e9ca4b4a178c67a9aff17547ac": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a12a1c0010d4ae7a36cabc0f7ec42e1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9afbca9880cb4969bc13764f5b6d715e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_6b3cb969da214b55a601bb2632065d5b"
          }
        },
        "9db2de0fda644ea6877a9c48b8343fda": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5ffb8a5dda7141bb9d5c57022c001f92",
              "IPY_MODEL_1499ade83860452187b973a7d913ee80",
              "IPY_MODEL_1c27978b487a4399be988721fab6a672"
            ],
            "layout": "IPY_MODEL_a7cc6d3ecbe841d48bdbf06cca332bcf"
          }
        },
        "9f22fad86bd74b089462f4b334933953": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2657c70c94c4a408f3724901bbcbd59": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a26ba53b2f144a56b8a546831a80e7dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a46934adfdec41ef9e426cd88a1bd965": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7cc6d3ecbe841d48bdbf06cca332bcf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aad6ff78d1dc44e388dbe0fedbe783c9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba37fbea0c9d46cfb91ef291ebcbcbf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2e7261ae3be4a3cb3f3b991e4f2cf1c",
            "max": 14467165872,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_43ba99f3f53c4b0a99b1efad636e774a",
            "value": 14467165872
          }
        },
        "babdfbde1a1146a8b8414dca8110648d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "baf1a2523e494fb6b4e73b58e23fa018": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c08646bdd1f446ffa06e62187f413fa3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1387265401646a1ba74a76c0ec5dade": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1a3c3fa3ad54faa80f1d48e33e39ded": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2e7261ae3be4a3cb3f3b991e4f2cf1c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8770dacca394e7fb6597aaa58ecdd41": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cab5ac418e674705b890326b177b5a5d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd20d6f9eb774b6190f1af949de68efe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ce52fe31786842fa8cd9b39016e53d1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cedce10b573543d8a7daa8ac3eb309c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d01aa132b2464d1e87373a021b144053": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d8ba975f46da4762a444fd5b91e4d83c",
              "IPY_MODEL_d9e5c9001f854b6cacae7287d5807200",
              "IPY_MODEL_0fcb2f9fe84d4b959a5f91c0c62c3c7a"
            ],
            "layout": "IPY_MODEL_482db021888648ed8954ceb8f9b049c5"
          }
        },
        "d13a8525e7314f2292758724004d40b7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2634243dfee496da2e433ff99f1ba08": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c7a973c53ac41cb89eb5873fb5139a5",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e1db3569f909456ca1ed63f2f2c9d32e",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "d364090f18cd4fb6b22d33ef7d904a3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a12a1c0010d4ae7a36cabc0f7ec42e1",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e3b7f9f144d8418fa6af2486e43b9fc7",
            "value": "config.json:â€‡"
          }
        },
        "d8ba975f46da4762a444fd5b91e4d83c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2657c70c94c4a408f3724901bbcbd59",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_babdfbde1a1146a8b8414dca8110648d",
            "value": "special_tokens_map.json:â€‡100%"
          }
        },
        "d9e5c9001f854b6cacae7287d5807200": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78609182e21b4fa683c11ec0d637ec0a",
            "max": 607,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cd20d6f9eb774b6190f1af949de68efe",
            "value": 607
          }
        },
        "dacc8ceae3f94a828247a5db584c80b4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dcf030c1e28e4d939eea04ec182d66c3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df88f78d1cca4239984b4cdfe357a63c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d364090f18cd4fb6b22d33ef7d904a3c",
              "IPY_MODEL_ff111c65db4d4d99b1d4ffb817b001ef",
              "IPY_MODEL_66f1142e575c43c2950a1d4706f9dfeb"
            ],
            "layout": "IPY_MODEL_2410335e9d1d42e4ba7acf2d6faa3856"
          }
        },
        "dfff16bb373c4f0898a405362f9b433f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3af1a767a5be49a0bd01074f27d07918",
            "max": 4264023,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_64e8f7cde9aa48c199a1b7f892099d14",
            "value": 4264023
          }
        },
        "e1db3569f909456ca1ed63f2f2c9d32e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e201b77d53c740e8aac3549970715737": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e3b7f9f144d8418fa6af2486e43b9fc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e95b677548c542f4adcee867dbc68d3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f23561568c824c9fb4874ebe19a972bd",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e201b77d53c740e8aac3549970715737",
            "value": "Connecting..."
          }
        },
        "ee41465d42ba4fa9b7de1f7baf106f8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f23561568c824c9fb4874ebe19a972bd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f36df089958c491ca4883845526d59a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f4d781486ec743fd9a1eb0e26d89014a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f514c38b6277467faf7c232f1de6c5ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b2479e713f648cf927bb7a57c58fe99",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5c8639cea21a41feafa33d5b737a6b4d",
            "value": "â€‡40.0k/40.0kâ€‡[00:00&lt;00:00,â€‡5.09MB/s]"
          }
        },
        "f7eb6178deec4ba8b4512d581479fdff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fc658bdf7b124f718ec19a3a3973a4c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0d6204dfffb44402b4d23a777f6fe735",
              "IPY_MODEL_34b425b87c39459fa1a618eac9cbe098",
              "IPY_MODEL_f514c38b6277467faf7c232f1de6c5ba"
            ],
            "layout": "IPY_MODEL_15c134fb254449ae99d93ad87fc3615e"
          }
        },
        "fde77f9987ac4f309e62dbbbbff07f05": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fe7bd857555b462792d515817020c806": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff066dd95fa448a586c4363bf1b91ae1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_893e44e9ca4b4a178c67a9aff17547ac",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_136ce2d3e4b24283808c9b8006c185f8",
            "value": "model.safetensors:â€‡100%"
          }
        },
        "ff111c65db4d4d99b1d4ffb817b001ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87bae00937484569b22b274780d068d0",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fde77f9987ac4f309e62dbbbbff07f05",
            "value": 1
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
