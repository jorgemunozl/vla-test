{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jorgemunozl/vla-test/blob/main/pi05-test/fourth_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZJvT5chXhV1"
      },
      "source": [
        "# Fourth Test\n",
        "\n",
        "After test our sub task implementation with a fixed temperature, this time we visualize the distribution of more probable tokens, tweaking the temperature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "b84c30d947024ef6b177a5a74662baba",
            "e176c54725f64b699c7f560ef7c46554",
            "afd1802ae3d44acca45b1a5a2e6110c8",
            "606fedc33f4f413ca824ad9c7ba84a34",
            "89f9756b355341f884e7b2d6d18b6e0f",
            "9805e7b102f44479a60dfe578a1b84cf",
            "b7e098f36bfc40db8d65a63550ecec51",
            "fa4460f624334d889394e50e7ec3a941",
            "39bc728c95ff43989132b27fee86ddb8",
            "03da519921654a5bab87c85543184936",
            "7dafafe80eda470c9aaa659d857d33ac",
            "a7a19f850262426cac457d755ce3a4a8",
            "7a250fedde8447d2a6f8f44c7f21b092",
            "f5d2f49c0de64ed68e8f0cbd1a5ee7e9",
            "c89e5230bb374acf9e1d4de38b49f353",
            "726b44720258460c9a8f25fdcfd6b744",
            "b566a841e6954c89acf6d0a0889fece0",
            "7e3a72644a3b4658b91dbe5aa0b10b24",
            "739ad6f705fb4596ac3466f3533eba68",
            "7041584c531849adb3d526e1bb1ef679"
          ]
        },
        "id": "0aM4ig9gOJ3D",
        "outputId": "952eb03f-23b5-4f62-d1fa-69d1bfad44a1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b84c30d947024ef6b177a5a74662baba"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcqFen2xOUZh",
        "outputId": "ac4b49bf-1332-4668-a0e2-3a3d6ab61f9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m109 packages\u001b[0m \u001b[2min 17.58s\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m29 packages\u001b[0m \u001b[2min 42.66s\u001b[0m\u001b[0m\n",
            "\u001b[2mUninstalled \u001b[1m11 packages\u001b[0m \u001b[2min 927ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m29 packages\u001b[0m \u001b[2min 213ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mav\u001b[0m\u001b[2m==15.1.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mdeepdiff\u001b[0m\u001b[2m==8.6.1\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mdiffusers\u001b[0m\u001b[2m==0.36.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mdiffusers\u001b[0m\u001b[2m==0.35.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mdraccus\u001b[0m\u001b[2m==0.10.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mevdev\u001b[0m\u001b[2m==1.9.2\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==0.36.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==0.35.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1minquirerpy\u001b[0m\u001b[2m==0.3.4\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjsonlines\u001b[0m\u001b[2m==4.0.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlerobot\u001b[0m\u001b[2m==0.4.3 (from git+https://github.com/huggingface/lerobot.git@46e19ae579f80ce66211afafd1c3c649c569131f)\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmergedeep\u001b[0m\u001b[2m==1.3.4\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmypy-extensions\u001b[0m\u001b[2m==1.1.0\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.10.2.21\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.5.1.17\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnvidia-cusparselt-cu12\u001b[0m\u001b[2m==0.7.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparselt-cu12\u001b[0m\u001b[2m==0.6.3\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnvidia-nccl-cu12\u001b[0m\u001b[2m==2.27.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-nccl-cu12\u001b[0m\u001b[2m==2.26.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1morderly-set\u001b[0m\u001b[2m==5.5.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpfzy\u001b[0m\u001b[2m==0.3.4\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpynput\u001b[0m\u001b[2m==1.8.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpyserial\u001b[0m\u001b[2m==3.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpython-xlib\u001b[0m\u001b[2m==0.33\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpyyaml-include\u001b[0m\u001b[2m==1.4.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mrerun-sdk\u001b[0m\u001b[2m==0.26.2\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.22.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.21.4\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.9.0+cu126\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.7.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtorchcodec\u001b[0m\u001b[2m==0.5\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.24.0+cu126\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.22.1\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.57.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.53.3 (from git+https://github.com/huggingface/transformers.git@dcddb970176382c0fcf4521b0c0e6fc15894dfe0)\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==3.5.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==3.3.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtyping-inspect\u001b[0m\u001b[2m==0.9.0\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mwandb\u001b[0m\u001b[2m==0.23.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mwandb\u001b[0m\u001b[2m==0.24.0\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!uv pip install \"git+https://github.com/huggingface/transformers.git@fix/lerobot_openpi\" \"lerobot @ git+https://github.com/huggingface/lerobot.git\" opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_OY5jHpoOY9S"
      },
      "outputs": [],
      "source": [
        "import builtins\n",
        "import logging\n",
        "import math\n",
        "from collections import deque\n",
        "from pathlib import Path\n",
        "from typing import TYPE_CHECKING, Literal, TypedDict\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F  # noqa: N812\n",
        "from torch import Tensor, nn\n",
        "from typing_extensions import Unpack\n",
        "\n",
        "from lerobot.utils.import_utils import _transformers_available\n",
        "\n",
        "# Conditional import for type checking and lazy loading\n",
        "if TYPE_CHECKING or _transformers_available:\n",
        "    from transformers.models.auto import CONFIG_MAPPING\n",
        "    from transformers.models.gemma import modeling_gemma\n",
        "    from transformers.models.gemma.modeling_gemma import GemmaForCausalLM\n",
        "    from transformers.models.paligemma.modeling_paligemma import (\n",
        "        PaliGemmaForConditionalGeneration,\n",
        "    )\n",
        "else:\n",
        "    CONFIG_MAPPING = None\n",
        "    modeling_gemma = None\n",
        "    GemmaForCausalLM = None\n",
        "    PaliGemmaForConditionalGeneration = None\n",
        "\n",
        "from lerobot.configs.policies import PreTrainedConfig\n",
        "from lerobot.policies.pi05.configuration_pi05 import PI05Config\n",
        "from lerobot.policies.pretrained import PreTrainedPolicy, T\n",
        "from lerobot.policies.rtc.modeling_rtc import RTCProcessor\n",
        "from lerobot.utils.constants import (\n",
        "    ACTION,\n",
        "    OBS_LANGUAGE_ATTENTION_MASK,\n",
        "    OBS_LANGUAGE_TOKENS,\n",
        "    OPENPI_ATTENTION_MASK_VALUE,\n",
        ")\n",
        "\n",
        "\n",
        "class ActionSelectKwargs(TypedDict, total=False):\n",
        "    inference_delay: int | None\n",
        "    prev_chunk_left_over: Tensor | None\n",
        "    execution_horizon: int | None\n",
        "\n",
        "\n",
        "def get_safe_dtype(target_dtype, device_type):\n",
        "    \"\"\"Get a safe dtype for the given device type.\"\"\"\n",
        "    if device_type == \"mps\" and target_dtype == torch.float64:\n",
        "        return torch.float32\n",
        "    if device_type == \"cpu\":\n",
        "        # CPU doesn't support bfloat16, use float32 instead\n",
        "        if target_dtype == torch.bfloat16:\n",
        "            return torch.float32\n",
        "        if target_dtype == torch.float64:\n",
        "            return torch.float64\n",
        "    return target_dtype\n",
        "\n",
        "\n",
        "# Positional Embedding for time action embedding\n",
        "def create_sinusoidal_pos_embedding(\n",
        "    time: torch.Tensor, dimension: int,\n",
        "    min_period: float, max_period: float, device=\"cpu\"\n",
        ") -> Tensor:\n",
        "    \"\"\"Computes sine-cosine positional embedding\n",
        "    vectors for scalar positions.\"\"\"\n",
        "    if dimension % 2 != 0:\n",
        "        raise ValueError(f\"dimension ({dimension}) must be divisible by 2\")\n",
        "\n",
        "    if time.ndim != 1:\n",
        "        raise ValueError(\"The time tensor is expected \" +\n",
        "                         \"to be of shape `(batch_size, )`.\")\n",
        "\n",
        "    dtype = get_safe_dtype(torch.float64, device.type)\n",
        "    fraction = torch.linspace(0.0, 1.0, dimension // 2,\n",
        "                              dtype=dtype, device=device)\n",
        "    period = min_period * (max_period / min_period) ** fraction\n",
        "\n",
        "    # Compute the outer product\n",
        "    scaling_factor = 1.0 / period * 2 * math.pi\n",
        "    sin_input = scaling_factor[None, :] * time[:, None]\n",
        "    return torch.cat([torch.sin(sin_input), torch.cos(sin_input)], dim=1)\n",
        "\n",
        "\n",
        "# For training\n",
        "def sample_beta(alpha, beta, bsize, device):\n",
        "    alpha_t = torch.as_tensor(alpha, dtype=torch.float32, device=device)\n",
        "    beta_t = torch.as_tensor(beta, dtype=torch.float32, device=device)\n",
        "    dist = torch.distributions.Beta(alpha_t, beta_t)\n",
        "    return dist.sample((bsize,))\n",
        "\n",
        "\n",
        "def make_att_2d_masks(pad_masks, att_masks):\n",
        "    \"\"\"\n",
        "    Tokens can attend to valid inputs tokens which have a cumulative mask_ar\n",
        "    smaller or equal to theirs. This way `mask_ar` int[B, N] can be used to\n",
        "    setup several types of attention, for example:\n",
        "\n",
        "      [[1 1 1 1 1 1]]: pure causal attention.\n",
        "\n",
        "      [[0 0 0 1 1 1]]: prefix-lm attention. The first 3 tokens can attend\n",
        "        between\n",
        "          themselves and the last 3 tokens have a causal attention. The first\n",
        "          entry could also be a 1 without changing behaviour.\n",
        "\n",
        "      [[1 0 1 0 1 0 0 1 0 0]]: causal attention between 4 blocks. Tokens of a\n",
        "          block can attend all previous blocks and all\n",
        "          tokens on the same block.\n",
        "\n",
        "    Args:\n",
        "        N: int - number of tokens in the sequence\n",
        "        pad_masks: bool[B, N] true if its part of the input,\n",
        "            false if padding.\n",
        "        att_masks: int[B, N] mask that's 1 where previous tokens\n",
        "            cannot depend on it and 0 where it shares the same\n",
        "            attention mask as the previous token.\n",
        "\n",
        "    Returns:\n",
        "        att_2d_masks: bool[B, N, N] 2D attention mask\n",
        "    \"\"\"\n",
        "    if att_masks.ndim != 2:\n",
        "        raise ValueError(att_masks.ndim)\n",
        "    if pad_masks.ndim != 2:\n",
        "        raise ValueError(pad_masks.ndim)\n",
        "\n",
        "    # cumsum shape: (B, N)\n",
        "    cumsum = torch.cumsum(att_masks, dim=1)\n",
        "    # att_2d_masks shape: (B, N, N)\n",
        "    att_2d_masks = cumsum[:, None, :] <= cumsum[:, :, None]\n",
        "    # pad_2d_masks shape: (B, N, N)\n",
        "    pad_2d_masks = pad_masks[:, None, :] * pad_masks[:, :, None]\n",
        "    # att_2d_masks & pad_2d_masks shape: (B, N, N)\n",
        "    return att_2d_masks & pad_2d_masks\n",
        "\n",
        "\n",
        "def pad_vector(vector, new_dim):\n",
        "    \"\"\"Pad the last dimension of a vector to new_dim with zeros.\n",
        "\n",
        "    Can be (batch_size x sequence_length x features_dimension)\n",
        "    or (batch_size x features_dimension)\n",
        "    \"\"\"\n",
        "    if vector.shape[-1] >= new_dim:\n",
        "        return vector\n",
        "    return F.pad(vector, (0, new_dim - vector.shape[-1]))\n",
        "\n",
        "\n",
        "def resize_with_pad_torch(\n",
        "    images: torch.Tensor,\n",
        "    height: int,\n",
        "    width: int,\n",
        "    mode: str = \"bilinear\",\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"PyTorch version of resize_with_pad. Resizes an image\n",
        "      to a target height and width without distortion\n",
        "    by padding with black. If the image is float32,\n",
        "      it must be in the range [-1, 1].\n",
        "\n",
        "    Args:\n",
        "        images: Tensor of shape [*b, h, w, c] or [*b, c, h, w]\n",
        "        height: Target height\n",
        "        width: Target width\n",
        "        mode: Interpolation mode ('bilinear', 'nearest', etc.)\n",
        "\n",
        "    Returns:\n",
        "        Resized and padded tensor with same shape format as input\n",
        "    \"\"\"\n",
        "    # Check if input is in channels-last format\n",
        "    # [*b, h, w, c] or channels-first [*b, c, h, w]\n",
        "    if images.shape[-1] <= 4:  # Assume channels-last format\n",
        "        channels_last = True\n",
        "        if images.dim() == 3:\n",
        "            images = images.unsqueeze(0)  # Add batch dimension\n",
        "        images = images.permute(0, 3, 1, 2)  # [b, h, w, c] -> [b, c, h, w]\n",
        "    else:\n",
        "        channels_last = False\n",
        "        if images.dim() == 3:\n",
        "            images = images.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "    batch_size, channels, cur_height, cur_width = images.shape\n",
        "\n",
        "    # Calculate resize ratio\n",
        "    ratio = max(cur_width / width, cur_height / height)\n",
        "    resized_height = int(cur_height / ratio)\n",
        "    resized_width = int(cur_width / ratio)\n",
        "\n",
        "    # Resize\n",
        "    resized_images = F.interpolate(\n",
        "        images,\n",
        "        size=(resized_height, resized_width),\n",
        "        mode=mode,\n",
        "        align_corners=False if mode == \"bilinear\" else None,\n",
        "    )\n",
        "\n",
        "    # Handle dtype-specific clipping\n",
        "    if images.dtype == torch.uint8:\n",
        "        resized_images = torch.round(resized_images)\n",
        "        resized_images = resized_images.clamp(0, 255).to(torch.uint8)\n",
        "    elif images.dtype == torch.float32:\n",
        "        resized_images = resized_images.clamp(-1.0, 1.0)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported image dtype: {images.dtype}\")\n",
        "\n",
        "    # Calculate padding\n",
        "    pad_h0, remainder_h = divmod(height - resized_height, 2)\n",
        "    pad_h1 = pad_h0 + remainder_h\n",
        "    pad_w0, remainder_w = divmod(width - resized_width, 2)\n",
        "    pad_w1 = pad_w0 + remainder_w\n",
        "\n",
        "    # Pad\n",
        "    constant_value = 0 if images.dtype == torch.uint8 else -1.0\n",
        "    padded_images = F.pad(\n",
        "        resized_images,\n",
        "        (pad_w0, pad_w1, pad_h0, pad_h1),  # left, right, top, bottom\n",
        "        mode=\"constant\",\n",
        "        value=constant_value,\n",
        "    )\n",
        "\n",
        "    # Convert back to original format if needed\n",
        "    if channels_last:\n",
        "        # [b, c, h, w] -> [b, h, w, c]\n",
        "        padded_images = padded_images.permute(0, 2, 3, 1)\n",
        "\n",
        "    return padded_images\n",
        "\n",
        "\n",
        "# Define the complete layer computation function for gradient checkpointing\n",
        "def compute_layer_complete(\n",
        "    layer_idx, inputs_embeds, attention_mask,\n",
        "    position_ids, adarms_cond, paligemma, gemma_expert\n",
        "):\n",
        "    models = [paligemma.language_model, gemma_expert.model]\n",
        "    query_states = []\n",
        "    key_states = []\n",
        "    value_states = []\n",
        "    gates = []\n",
        "    for i, hidden_states in enumerate(inputs_embeds):\n",
        "        layer = models[i].layers[layer_idx]\n",
        "        hidden_states, gate = layer.input_layernorm(hidden_states,\n",
        "                                                    cond=adarms_cond[i])\n",
        "        gates.append(gate)\n",
        "        input_shape = hidden_states.shape[:-1]\n",
        "        hidden_shape = (*input_shape, -1, layer.self_attn.head_dim)\n",
        "        query_state = layer.self_attn.q_proj(hidden_states)\n",
        "        query_state = query_state.view(hidden_shape).transpose(1, 2)\n",
        "        key_state = layer.self_attn.k_proj(hidden_states)\n",
        "        key_state = key_state.view(hidden_shape).transpose(1, 2)\n",
        "        value_state = layer.self_attn.v_proj(hidden_states)\n",
        "        value_state = value_state.view(hidden_shape).transpose(1, 2)\n",
        "        query_states.append(query_state)\n",
        "        key_states.append(key_state)\n",
        "        value_states.append(value_state)\n",
        "    # Concatenate and process attention\n",
        "    query_states = torch.cat(query_states, dim=2)\n",
        "    key_states = torch.cat(key_states, dim=2)\n",
        "    value_states = torch.cat(value_states, dim=2)\n",
        "    dummy_tensor = torch.zeros(\n",
        "        query_states.shape[0],\n",
        "        query_states.shape[2],\n",
        "        query_states.shape[-1],\n",
        "        device=query_states.device,\n",
        "        dtype=query_states.dtype,\n",
        "    )\n",
        "    cos, sin = paligemma.model.language_model.rotary_emb(dummy_tensor,\n",
        "                                                         position_ids)\n",
        "    query_states, key_states = modeling_gemma.apply_rotary_pos_emb(\n",
        "        query_states, key_states, cos, sin, unsqueeze_dim=1\n",
        "    )\n",
        "    batch_size = query_states.shape[0]\n",
        "    scaling = paligemma.language_model.layers[layer_idx].self_attn.scaling\n",
        "    # Attention computation\n",
        "    att_output, _ = modeling_gemma.eager_attention_forward(\n",
        "        paligemma.language_model.layers[layer_idx].self_attn,\n",
        "        query_states,\n",
        "        key_states,\n",
        "        value_states,\n",
        "        attention_mask,\n",
        "        scaling,\n",
        "    )\n",
        "    # Get head_dim from the current layer, not from the model\n",
        "    head_dim = paligemma.language_model.layers[layer_idx].self_attn.head_dim\n",
        "    att_output = att_output.reshape(batch_size, -1, 1 * 8 * head_dim)\n",
        "    # Process layer outputs\n",
        "    outputs_embeds = []\n",
        "    start_pos = 0\n",
        "    for i, hidden_states in enumerate(inputs_embeds):\n",
        "        layer = models[i].layers[layer_idx]\n",
        "        end_pos = start_pos + hidden_states.shape[1]\n",
        "        if att_output.dtype != layer.self_attn.o_proj.weight.dtype:\n",
        "            att_output = att_output.to(layer.self_attn.o_proj.weight.dtype)\n",
        "        out_emb = layer.self_attn.o_proj(att_output[:, start_pos:end_pos])\n",
        "        # first residual\n",
        "        out_emb = modeling_gemma._gated_residual(hidden_states,\n",
        "                                                 out_emb, gates[i])\n",
        "        after_first_residual = out_emb.clone()\n",
        "        out_emb, gate = layer.post_attention_layernorm(out_emb,\n",
        "                                                       cond=adarms_cond[i])\n",
        "        # Convert to bfloat16 if the next layer (mlp) uses bfloat16\n",
        "        if layer.mlp.up_proj.weight.dtype == torch.bfloat16:\n",
        "            out_emb = out_emb.to(dtype=torch.bfloat16)\n",
        "        out_emb = layer.mlp(out_emb)\n",
        "        # second residual\n",
        "        out_emb = modeling_gemma._gated_residual(after_first_residual,\n",
        "                                                 out_emb, gate)\n",
        "        outputs_embeds.append(out_emb)\n",
        "        start_pos = end_pos\n",
        "    return outputs_embeds\n",
        "\n",
        "\n",
        "class GemmaConfig:  # see openpi `gemma.py: Config`\n",
        "    \"\"\"Configuration for Gemma model variants.\"\"\"\n",
        "\n",
        "    def __init__(self, width, depth, mlp_dim,\n",
        "                 num_heads, num_kv_heads, head_dim):\n",
        "        self.width = width\n",
        "        self.depth = depth\n",
        "        self.mlp_dim = mlp_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.num_kv_heads = num_kv_heads\n",
        "        self.head_dim = head_dim\n",
        "\n",
        "\n",
        "def get_gemma_config(variant: str) -> GemmaConfig:\n",
        "    \"\"\"Returns config for specified gemma variant.\"\"\"\n",
        "    # This is the config for the action expert\n",
        "    if variant == \"gemma_300m\":\n",
        "        return GemmaConfig(\n",
        "            width=1024,\n",
        "            depth=18,\n",
        "            mlp_dim=4096,\n",
        "            num_heads=8,\n",
        "            num_kv_heads=1,\n",
        "            head_dim=256,\n",
        "        )\n",
        "    # Vison Language Config\n",
        "    elif variant == \"gemma_2b\":\n",
        "        return GemmaConfig(\n",
        "            width=2048,\n",
        "            depth=18,\n",
        "            mlp_dim=16_384,\n",
        "            num_heads=8,\n",
        "            num_kv_heads=1,\n",
        "            head_dim=256,\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown variant: {variant}\")\n",
        "\n",
        "\n",
        "class PaliGemmaWithExpertModel(nn.Module):\n",
        "    \"\"\"PaliGemma model with action expert for PI05.\"\"\"\n",
        "    def __init__(\n",
        "        self, vlm_config, action_expert_config, use_adarms=None,\n",
        "        precision: Literal[\"bfloat16\", \"float32\"] = \"bfloat16\",\n",
        "    ):\n",
        "        if use_adarms is None:\n",
        "            use_adarms = [False, False]\n",
        "        super().__init__()\n",
        "\n",
        "        # Configuration from the VLM PALIGEMMA\n",
        "        vlm_config_hf = CONFIG_MAPPING[\"paligemma\"]()\n",
        "        vlm_config_hf._vocab_size = 257152  # noqa: SLF001\n",
        "        vlm_config_hf.image_token_index = 257152\n",
        "        vlm_config_hf.text_config.hidden_size = vlm_config.width\n",
        "        vlm_config_hf.text_config.intermediate_size = vlm_config.mlp_dim\n",
        "        vlm_config_hf.text_config.num_attention_heads = vlm_config.num_heads\n",
        "        vlm_config_hf.text_config.head_dim = vlm_config.head_dim\n",
        "        vlm_config_hf.text_config.num_hidden_layers = vlm_config.depth\n",
        "        vlm_config_hf.text_config.num_key_value_heads = vlm_config.num_kv_heads\n",
        "        vlm_config_hf.text_config.hidden_activation = \"gelu_pytorch_tanh\"\n",
        "        vlm_config_hf.text_config.torch_dtype = \"float32\"\n",
        "        vlm_config_hf.text_config.vocab_size = 257152\n",
        "        vlm_config_hf.text_config.use_adarms = use_adarms[0]\n",
        "        vlm_config_hf.text_config.adarms_cond_dim = (\n",
        "            vlm_config.width if use_adarms[0] else None\n",
        "        )\n",
        "        vlm_config_hf.vision_config.intermediate_size = 4304\n",
        "        vlm_config_hf.vision_config.projection_dim = 2048\n",
        "        vlm_config_hf.vision_config.projector_hidden_act = \"gelu_fast\"\n",
        "        vlm_config_hf.vision_config.torch_dtype = \"float32\"\n",
        "\n",
        "        # CONFIGURATION FOR THE ACTION EXPERT\n",
        "        action_expert_config_hf = CONFIG_MAPPING[\"gemma\"](\n",
        "            head_dim=action_expert_config.head_dim,\n",
        "            hidden_size=action_expert_config.width,\n",
        "            intermediate_size=action_expert_config.mlp_dim,\n",
        "            num_attention_heads=action_expert_config.num_heads,\n",
        "            num_hidden_layers=action_expert_config.depth,\n",
        "            num_key_value_heads=action_expert_config.num_kv_heads,\n",
        "            vocab_size=257152,\n",
        "            hidden_activation=\"gelu_pytorch_tanh\",\n",
        "            torch_dtype=\"float32\",\n",
        "            use_adarms=use_adarms[1],\n",
        "            adarms_cond_dim=(\n",
        "                action_expert_config.width if use_adarms[1] else None\n",
        "            ),\n",
        "        )\n",
        "        # VLM\n",
        "        self.paligemma = PaliGemmaForConditionalGeneration(\n",
        "            config=vlm_config_hf)\n",
        "\n",
        "        # Expert Architecture Initialized from a small Gemma Version\n",
        "        # From pretrained only loads the paligemma model\n",
        "        self.gemma_expert = GemmaForCausalLM(config=action_expert_config_hf)\n",
        "        self.gemma_expert.model.embed_tokens = None\n",
        "\n",
        "        self.to_bfloat16_for_selected_params(precision)\n",
        "\n",
        "    def to_bfloat16_for_selected_params(\n",
        "            self, precision: Literal[\"bfloat16\", \"float32\"] = \"bfloat16\"):\n",
        "        if precision == \"bfloat16\":\n",
        "            self.to(dtype=torch.bfloat16)\n",
        "        elif precision == \"float32\":\n",
        "            self.to(dtype=torch.float32)\n",
        "            return\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid precision: {precision}\")\n",
        "\n",
        "        params_to_keep_float32 = [\n",
        "            \"vision_tower.vision_model.embeddings.patch_embedding.weight\",\n",
        "            \"vision_tower.vision_model.embeddings.patch_embedding.bias\",\n",
        "            \"vision_tower.vision_model.embeddings.position_embedding.weight\",\n",
        "            \"input_layernorm\",\n",
        "            \"post_attention_layernorm\",\n",
        "            \"model.norm\",\n",
        "        ]\n",
        "\n",
        "        for name, param in self.named_parameters():\n",
        "            if any(selector in name for selector in params_to_keep_float32):\n",
        "                param.data = param.data.to(dtype=torch.float32)\n",
        "\n",
        "    def embed_image(self, image: torch.Tensor):\n",
        "        return self.paligemma.model.get_image_features(image)\n",
        "\n",
        "    def embed_language_tokens(self, tokens: torch.Tensor):\n",
        "        return self.paligemma.language_model.embed_tokens(tokens)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        attention_mask: torch.Tensor | None = None,\n",
        "        position_ids: torch.LongTensor | None = None,\n",
        "        past_key_values: list[torch.FloatTensor] | None = None,\n",
        "        inputs_embeds: list[torch.FloatTensor] | None = None,\n",
        "        use_cache: bool | None = None,\n",
        "        adarms_cond: list[torch.Tensor] | None = None,\n",
        "    ):\n",
        "        if adarms_cond is None:\n",
        "            adarms_cond = [None, None]\n",
        "\n",
        "        # The first step is give [preffix, None]\n",
        "        # then just the VLM return a suffix\n",
        "        if inputs_embeds[1] is None:\n",
        "            prefix_output = self.paligemma.language_model.forward(\n",
        "                inputs_embeds=inputs_embeds[0],\n",
        "                attention_mask=attention_mask,\n",
        "                position_ids=position_ids,\n",
        "                past_key_values=past_key_values,\n",
        "                use_cache=use_cache,\n",
        "                adarms_cond=(\n",
        "                    adarms_cond[0] if adarms_cond is not None else None\n",
        "                )\n",
        "            )\n",
        "            prefix_past_key_values = prefix_output.past_key_values\n",
        "            prefix_output = prefix_output.last_hidden_state\n",
        "            suffix_output = None\n",
        "\n",
        "        # When inputs_embeds=[None, suffix_embs] the Gemma_Expert Activates\n",
        "        elif inputs_embeds[0] is None:\n",
        "            suffix_output = self.gemma_expert.model.forward(\n",
        "                inputs_embeds=inputs_embeds[1],\n",
        "                attention_mask=attention_mask,\n",
        "                position_ids=position_ids,\n",
        "                past_key_values=past_key_values,\n",
        "                use_cache=use_cache,\n",
        "                adarms_cond=(\n",
        "                    adarms_cond[1] if adarms_cond is not None else None\n",
        "                )\n",
        "            )\n",
        "            suffix_output = suffix_output.last_hidden_state\n",
        "            prefix_output = None\n",
        "            prefix_past_key_values = None\n",
        "\n",
        "        # A complete forward for VLM and action expert.\n",
        "        else:\n",
        "            models = [self.paligemma.language_model, self.gemma_expert.model]\n",
        "            num_layers = self.paligemma.config.text_config.num_hidden_layers\n",
        "\n",
        "            # Check if gradient checkpointing is enabled for any of the models\n",
        "            use_gradient_checkpointing = (\n",
        "                hasattr(self.gemma_expert.model, \"gradient_checkpointing\")\n",
        "                and self.gemma_expert.model.gradient_checkpointing\n",
        "                and self.training\n",
        "            ) or (\n",
        "                hasattr(self, \"gradient_checkpointing\")\n",
        "                and self.gradient_checkpointing\n",
        "                and self.training\n",
        "            )\n",
        "\n",
        "            # Process all layers with gradient checkpointing if enabled\n",
        "            for layer_idx in range(num_layers):\n",
        "                if use_gradient_checkpointing:\n",
        "                    inputs_embeds = torch.utils.checkpoint.checkpoint(\n",
        "                        compute_layer_complete,\n",
        "                        layer_idx,\n",
        "                        inputs_embeds,\n",
        "                        attention_mask,\n",
        "                        position_ids,\n",
        "                        adarms_cond,\n",
        "                        use_reentrant=False,\n",
        "                        preserve_rng_state=False,\n",
        "                        paligemma=self.paligemma,\n",
        "                        gemma_expert=self.gemma_expert,\n",
        "                    )\n",
        "                else:\n",
        "                    inputs_embeds = compute_layer_complete(\n",
        "                        layer_idx,\n",
        "                        inputs_embeds,\n",
        "                        attention_mask,\n",
        "                        position_ids,\n",
        "                        adarms_cond,\n",
        "                        paligemma=self.paligemma,\n",
        "                        gemma_expert=self.gemma_expert,\n",
        "                    )\n",
        "\n",
        "            # final norm\n",
        "            def compute_final_norms(inputs_embeds, adarms_cond):\n",
        "                outputs_embeds = []\n",
        "                for i, hidden_states in enumerate(inputs_embeds):\n",
        "                    out_emb, _ = models[i].norm(hidden_states,\n",
        "                                                cond=adarms_cond[i])\n",
        "                    outputs_embeds.append(out_emb)\n",
        "                return outputs_embeds\n",
        "\n",
        "            # Apply gradient checkpointing to final norm if enabled\n",
        "            if use_gradient_checkpointing:\n",
        "                outputs_embeds = torch.utils.checkpoint.checkpoint(\n",
        "                    compute_final_norms,\n",
        "                    inputs_embeds,\n",
        "                    adarms_cond,\n",
        "                    use_reentrant=False,\n",
        "                    preserve_rng_state=False,\n",
        "                )\n",
        "            else:\n",
        "                outputs_embeds = compute_final_norms(inputs_embeds,\n",
        "                                                     adarms_cond)\n",
        "\n",
        "            prefix_output = outputs_embeds[0]\n",
        "            suffix_output = outputs_embeds[1]\n",
        "            prefix_past_key_values = None\n",
        "\n",
        "        # You only care about the suffix_output to denoise actions.\n",
        "        return [prefix_output, suffix_output], prefix_past_key_values\n",
        "\n",
        "\n",
        "class PI05Model(nn.Module):\n",
        "    \"\"\"Core PI05 Model.\"\"\"\n",
        "    def __init__(self, config: PI05Config,\n",
        "                 rtc_processor: RTCProcessor | None = None):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.rtc_processor = rtc_processor\n",
        "\n",
        "        paligemma_config = get_gemma_config(config.paligemma_variant)\n",
        "        action_expert_config = get_gemma_config(config.action_expert_variant)\n",
        "\n",
        "        self.paligemma_with_expert = PaliGemmaWithExpertModel(\n",
        "            paligemma_config,\n",
        "            action_expert_config,\n",
        "            use_adarms=[False, True],\n",
        "            precision=config.dtype,\n",
        "        )\n",
        "\n",
        "        # Embeds the noisy action\n",
        "        self.action_in_proj = nn.Linear(config.max_action_dim,\n",
        "                                        action_expert_config.width)\n",
        "\n",
        "        # Unmbed the denoisy action into the space action\n",
        "        self.action_out_proj = nn.Linear(action_expert_config.width,\n",
        "                                         config.max_action_dim)\n",
        "\n",
        "        # Time Embedding\n",
        "        self.time_mlp_in = nn.Linear(action_expert_config.width,\n",
        "                                     action_expert_config.width)\n",
        "        # Time Unmbedding\n",
        "        self.time_mlp_out = nn.Linear(action_expert_config.width,\n",
        "                                      action_expert_config.width)\n",
        "\n",
        "        # Initialize gradient checkpointing flag\n",
        "        self.gradient_checkpointing_enabled = False\n",
        "\n",
        "        # Compile model if requested\n",
        "        if config.compile_model:\n",
        "            torch.set_float32_matmul_precision(\"high\")\n",
        "            self.sample_actions = torch.compile(self.sample_actions,\n",
        "                                                mode=config.compile_mode)\n",
        "\n",
        "        msg = \"\"\"An incorrect transformer version is used, please create\n",
        "          an issue on https://github.com/huggingface/lerobot/issues\"\"\"\n",
        "\n",
        "        try:\n",
        "            from transformers.models.siglip import check\n",
        "\n",
        "            if not check.check_whether_transformers_replace_is_installed_correctly():  # noqa: E501\n",
        "                raise ValueError(msg)\n",
        "        except ImportError:\n",
        "            raise ValueError(msg) from None\n",
        "\n",
        "    def gradient_checkpointing_enable(self):\n",
        "        \"\"\"Enable gradient checkpointing for memory optimization.\"\"\"\n",
        "        self.gradient_checkpointing_enabled = True\n",
        "        self.paligemma_with_expert.paligemma.language_model.gradient_checkpointing = True  # noqa: E501\n",
        "        self.paligemma_with_expert.paligemma.vision_tower.gradient_checkpointing = True  # noqa: E501\n",
        "        self.paligemma_with_expert.gemma_expert.model.gradient_checkpointing = True  # noqa: E501\n",
        "        logging.info(\"Enabled gradient checkpointing for PI05Pytorch model\")\n",
        "\n",
        "    def gradient_checkpointing_disable(self):\n",
        "        \"\"\"Disable gradient checkpointing.\"\"\"\n",
        "        self.gradient_checkpointing_enabled = False\n",
        "        self.paligemma_with_expert.paligemma.language_model.gradient_checkpointing = False  # noqa: E501\n",
        "        self.paligemma_with_expert.paligemma.vision_tower.gradient_checkpointing = False  # noqa: E501\n",
        "        self.paligemma_with_expert.gemma_expert.model.gradient_checkpointing = False  # noqa: E501\n",
        "        logging.info(\"Disabled gradient checkpointing for PI05Pytorch model\")\n",
        "\n",
        "    def _rtc_enabled(self):\n",
        "        return self.config.rtc_config is not None and self.config.rtc_config.enabled  # noqa: E501\n",
        "\n",
        "    def _apply_checkpoint(self, func, *args, **kwargs):\n",
        "        \"\"\"Helper method to apply gradient checkpointing if enabled.\"\"\"\n",
        "        if self.gradient_checkpointing_enabled and self.training:\n",
        "            return torch.utils.checkpoint.checkpoint(\n",
        "                func, *args, use_reentrant=False,\n",
        "                preserve_rng_state=False, **kwargs\n",
        "            )\n",
        "        return func(*args, **kwargs)\n",
        "\n",
        "    def _prepare_attention_masks_4d(self, att_2d_masks):\n",
        "        \"\"\"\n",
        "        Helper method to prepare 4D attention masks for transformer.\n",
        "        \"\"\"\n",
        "        att_2d_masks_4d = att_2d_masks[:, None, :, :]\n",
        "        return torch.where(att_2d_masks_4d, 0.0, OPENPI_ATTENTION_MASK_VALUE)\n",
        "\n",
        "    def sample_noise(self, shape, device):\n",
        "        return torch.normal(\n",
        "            mean=0.0,\n",
        "            std=1.0,\n",
        "            size=shape,\n",
        "            dtype=torch.float32,\n",
        "            device=device,\n",
        "        )\n",
        "\n",
        "    def sample_time(self, bsize, device):\n",
        "        time_beta = sample_beta(\n",
        "            self.config.time_sampling_beta_alpha,\n",
        "            self.config.time_sampling_beta_beta, bsize, device\n",
        "        )\n",
        "        time = time_beta * self.config.time_sampling_scale + self.config.time_sampling_offset  # noqa:E501\n",
        "        return time.to(dtype=torch.float32, device=device)\n",
        "\n",
        "    def embed_prefix(self, images, img_masks, tokens, masks\n",
        "                     ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Embed images with SigLIP and language\n",
        "        tokens with embedding layer.\n",
        "\n",
        "        Args:\n",
        "            images: List of image tensors\n",
        "            img_masks: List of image masks\n",
        "            tokens: Tokenized prompt tokens (B, seq_len)\n",
        "            masks: Attention masks for tokens (B, seq_len)\n",
        "\n",
        "        Returns:\n",
        "            A tuple of tensors containing the embedded images,\n",
        "            pad masks, and attention masks.\n",
        "            embs: (B, seq_len, embd_dim)\n",
        "            pad_masks: (B, seq_len)\n",
        "            att_masks: (B, seq_len)\n",
        "        \"\"\"\n",
        "        embs = []\n",
        "        pad_masks = []\n",
        "        att_masks = []\n",
        "\n",
        "        # Process images\n",
        "        for img, img_mask in zip(images, img_masks, strict=True):\n",
        "            def image_embed_func(img):\n",
        "                return self.paligemma_with_expert.embed_image(img)\n",
        "\n",
        "            # img shape: (B, C, H, W)\n",
        "            # img_emb shape: (B, num_img_embs, embd_dim)\n",
        "            img_emb = self._apply_checkpoint(image_embed_func, img)\n",
        "            bsize, num_img_embs = img_emb.shape[:2]\n",
        "\n",
        "            embs.append(img_emb)\n",
        "\n",
        "            # img_pad_mask shape: (B, num_img_embs)\n",
        "            img_pad_mask = img_mask[:, None].expand(bsize, num_img_embs)\n",
        "            pad_masks.append(img_pad_mask)\n",
        "            att_masks += [0] * num_img_embs\n",
        "\n",
        "        # Process language tokens\n",
        "        def lang_embed_func(tokens):\n",
        "            lang_emb = self.paligemma_with_expert.embed_language_tokens(tokens)\n",
        "            lang_emb_dim = lang_emb.shape[-1]\n",
        "            return lang_emb * math.sqrt(lang_emb_dim)\n",
        "\n",
        "        lang_emb = self._apply_checkpoint(lang_embed_func, tokens)\n",
        "\n",
        "        # Image + Language\n",
        "        embs.append(lang_emb)\n",
        "\n",
        "        pad_masks.append(masks)\n",
        "\n",
        "        num_lang_embs = lang_emb.shape[1]\n",
        "        # Prefix attention masks are all 0 for language and image tokens\n",
        "        # att_masks shape: (num_images* num_img_embs + num_lang_embs)\n",
        "        att_masks += [0] * num_lang_embs\n",
        "\n",
        "        # Convert from list to tensor\n",
        "        embs = torch.cat(embs, dim=1)\n",
        "\n",
        "        # pad_masks shape: (B, num_images* num_img_embs + num_lang_embs)\n",
        "        pad_masks = torch.cat(pad_masks, dim=1)\n",
        "        # att_masks shape: (num_images* num_img_embs + num_lang_embs)\n",
        "        att_masks = torch.tensor(att_masks,\n",
        "                                 dtype=torch.bool, device=pad_masks.device)\n",
        "        bsize = pad_masks.shape[0]\n",
        "        # att_masks shape: (B, num_images* num_img_embs + num_lang_embs)\n",
        "        att_masks = att_masks[None, :].expand(bsize, len(att_masks))\n",
        "\n",
        "        return embs, pad_masks, att_masks\n",
        "\n",
        "    def embed_suffix(self, noisy_actions, timestep):\n",
        "        \"\"\"\n",
        "        Embeds noisy_actions,\n",
        "        timestep to prepare for Expert Gemma processing.\n",
        "        Args:\n",
        "            noisy_actions: The noisy actions to embed.\n",
        "                shape: (B, action_horizon, action_dim)\n",
        "            timestep: The timestep to embed.\n",
        "            Begin with in -1 and end with in 0.\n",
        "                shape: (B,)\n",
        "\n",
        "        Returns:\n",
        "            A tuple of tensors containing the embedded noisy\n",
        "            actions, pad masks, and attention masks.\n",
        "        \"\"\"\n",
        "        embs = []\n",
        "        pad_masks = []\n",
        "        att_masks = []\n",
        "\n",
        "        # Embed timestep using sine-cosine positional encoding\n",
        "        time_emb = create_sinusoidal_pos_embedding(\n",
        "            timestep,\n",
        "            self.action_in_proj.out_features,\n",
        "            min_period=self.config.min_period,\n",
        "            max_period=self.config.max_period,\n",
        "            device=timestep.device,\n",
        "        )\n",
        "        time_emb = time_emb.type(dtype=timestep.dtype)\n",
        "\n",
        "        # Fuse timestep + action information using an MLP\n",
        "        def action_proj_func(noisy_actions):\n",
        "            return self.action_in_proj(noisy_actions)\n",
        "\n",
        "        # (B, chunk_size, action_dim)\n",
        "        action_emb = self._apply_checkpoint(action_proj_func, noisy_actions)\n",
        "\n",
        "        def time_mlp_func(time_emb):\n",
        "            x = self.time_mlp_in(time_emb)\n",
        "            x = F.silu(x)\n",
        "            x = self.time_mlp_out(x)\n",
        "            return F.silu(x)\n",
        "\n",
        "        time_emb = self._apply_checkpoint(time_mlp_func, time_emb)\n",
        "        action_time_emb = action_emb\n",
        "        adarms_cond = time_emb\n",
        "\n",
        "        embs.append(action_time_emb)\n",
        "        bsize, action_time_dim = action_time_emb.shape[:2]\n",
        "        action_time_mask = torch.ones(bsize, action_time_dim, dtype=torch.bool,\n",
        "                                      device=timestep.device)\n",
        "        pad_masks.append(action_time_mask)\n",
        "\n",
        "        # Set attention masks so that image,\n",
        "        # language and state inputs do not attend to action tokens\n",
        "        att_masks += [1] + ([0] * (self.config.chunk_size - 1))\n",
        "\n",
        "        embs = torch.cat(embs, dim=1)\n",
        "        pad_masks = torch.cat(pad_masks, dim=1)\n",
        "        att_masks = torch.tensor(att_masks,\n",
        "                                 dtype=embs.dtype, device=embs.device)\n",
        "        att_masks = att_masks[None, :].expand(bsize, len(att_masks))\n",
        "\n",
        "        return embs, pad_masks, att_masks, adarms_cond\n",
        "\n",
        "    def forward(self, images, img_masks,\n",
        "                tokens, masks, actions, noise=None, time=None,\n",
        "                token_loss_mask=None, real_action_dim=None) -> Tensor:\n",
        "        \"\"\"\n",
        "        Do a full training forward pass and compute the loss.\n",
        "        Actions are data that comes from teleop or from the human.\n",
        "\n",
        "        Args:\n",
        "            images: List of image tensors\n",
        "            img_masks: List of image masks\n",
        "            tokens: Tokenized prompt tokens (B, seq_len)\n",
        "            masks: Attention masks for tokens (B, seq_len)\n",
        "            actions: Action tensors (B, action_horizon, action_dim)\n",
        "            noise: Optional noise tensor for flow matching\n",
        "            time: Optional time tensor for flow matching\n",
        "            token_loss_mask: Optional mask for which tokens to compute\n",
        "                CE loss (B, seq_len)\n",
        "            real_action_dim: Optional real action dimension\n",
        "                (for padding handling)\n",
        "        \"\"\"\n",
        "        if noise is None:\n",
        "            noise = self.sample_noise(actions.shape, actions.device)\n",
        "\n",
        "        if time is None:\n",
        "            time = self.sample_time(actions.shape[0], actions.device)\n",
        "\n",
        "        # Embed prefix (images + language tokens)\n",
        "        prefix_embs, prefix_pad_masks, prefix_att_masks = (\n",
        "            self.embed_prefix(images, img_masks, tokens, masks)\n",
        "        )\n",
        "\n",
        "        # Prepare attention masks for prefix\n",
        "        prefix_att_2d_masks = make_att_2d_masks(\n",
        "            prefix_pad_masks, prefix_att_masks\n",
        "        )\n",
        "        prefix_position_ids = torch.cumsum(prefix_pad_masks, dim=1) - 1\n",
        "        prefix_att_2d_masks_4d = self._prepare_attention_masks_4d(\n",
        "            prefix_att_2d_masks\n",
        "        )\n",
        "        # Convert prefix_embs to bfloat16 if needed\n",
        "        q_proj_dtype = (\n",
        "            self.paligemma_with_expert.paligemma.language_model.layers[0]\n",
        "            .self_attn.q_proj.weight.dtype\n",
        "        )\n",
        "        if q_proj_dtype == torch.bfloat16:\n",
        "            prefix_embs = prefix_embs.to(dtype=torch.bfloat16)\n",
        "\n",
        "        # Forward pass through prefix to get prefix output and KV cache\n",
        "        def prefix_forward_func(\n",
        "            prefix_embs, prefix_att_2d_masks_4d, prefix_position_ids\n",
        "        ):\n",
        "            (prefix_out, _), kv_cache = self.paligemma_with_expert.forward(\n",
        "                attention_mask=prefix_att_2d_masks_4d,\n",
        "                position_ids=prefix_position_ids,\n",
        "                past_key_values=None,\n",
        "                inputs_embeds=[prefix_embs, None],\n",
        "                use_cache=True,\n",
        "                adarms_cond=[None, None],\n",
        "            )\n",
        "            return prefix_out, kv_cache\n",
        "\n",
        "        prefix_out, kv_cache = self._apply_checkpoint(\n",
        "            prefix_forward_func,\n",
        "            prefix_embs,\n",
        "            prefix_att_2d_masks_4d,\n",
        "            prefix_position_ids\n",
        "        )\n",
        "\n",
        "        # Compute Cross-Entropy Loss for Subtask Generation\n",
        "        subtask_generation_loss = None\n",
        "        if token_loss_mask is not None and tokens is not None:\n",
        "            # Shift tokens by 1 for next-token prediction\n",
        "            # tokens shape: (B, seq_len)\n",
        "            targets = tokens[:, 1:]  # (B, seq_len - 1)\n",
        "            loss_mask = token_loss_mask[:, 1:]  # (B, seq_len - 1)\n",
        "\n",
        "            # prefix_out shape: (B, prefix_seq_len, embd_dim)\n",
        "            # The prefix contains: images + language tokens\n",
        "            # prefix_out[:, :-1] removes the last token\n",
        "            # (to align with shifted targets)\n",
        "            # We need to extract the language token part from the end\n",
        "            # Since language tokens are at the end of prefix,\n",
        "            # we take the last tokens.shape[1] - 1 tokens\n",
        "            prefix_out_shifted = prefix_out[:, :-1]\n",
        "            # (B, prefix_seq_len - 1, embd_dim)\n",
        "            token_seq_len = tokens.shape[1]\n",
        "\n",
        "            # Extract the language token embeddings\n",
        "            # (last token_seq_len - 1 tokens)\n",
        "            if prefix_out_shifted.shape[1] >= token_seq_len - 1:\n",
        "                lang_prefix_out = prefix_out_shifted[\n",
        "                    :, -(token_seq_len - 1):\n",
        "                ]  # (B, token_seq_len - 1, embd_dim)\n",
        "                # Ensure we have the right length\n",
        "                min_len = min(lang_prefix_out.shape[1], targets.shape[1])\n",
        "                lang_prefix_out = lang_prefix_out[:, :min_len]\n",
        "                targets = targets[:, :min_len]\n",
        "                loss_mask = loss_mask[:, :min_len]\n",
        "\n",
        "                # Convert embeddings to logits\n",
        "                def deembed_func(lang_prefix_out):\n",
        "                    return (\n",
        "                        self.paligemma_with_expert.paligemma.lm_head(\n",
        "                            lang_prefix_out\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "                logits = self._apply_checkpoint(deembed_func, lang_prefix_out)\n",
        "                # logits shape: (B, min_len, vocab_size)\n",
        "\n",
        "                # Compute log probabilities\n",
        "                log_probs = F.log_softmax(logits, dim=-1)\n",
        "\n",
        "                # Get the log probability of the target token\n",
        "                target_log_probs = torch.gather(\n",
        "                    log_probs, dim=-1, index=targets.unsqueeze(-1)\n",
        "                ).squeeze(-1)  # (B, min_len)\n",
        "\n",
        "                # Apply loss mask and compute mean\n",
        "                masked_loss = -target_log_probs * loss_mask.float()\n",
        "                # Sum over sequence dimension and divide by\n",
        "                # number of valid tokens\n",
        "                num_valid_tokens = loss_mask.float().sum(dim=-1).clamp(min=1.0)\n",
        "                subtask_generation_loss = (\n",
        "                    masked_loss.sum(dim=-1) / num_valid_tokens\n",
        "                )  # (B,)\n",
        "\n",
        "        # Flow Matching Loss (MSE Loss)\n",
        "        time_expanded = time[:, None, None]\n",
        "        x_t = time_expanded * noise + (1 - time_expanded) * actions\n",
        "        u_t = noise - actions\n",
        "\n",
        "        suffix_embs, suffix_pad_masks, suffix_att_masks, adarms_cond = (\n",
        "            self.embed_suffix(x_t, time)\n",
        "        )\n",
        "\n",
        "        q_proj_dtype_suffix = (\n",
        "            self.paligemma_with_expert.paligemma.language_model.layers[0]\n",
        "            .self_attn.q_proj.weight.dtype\n",
        "        )\n",
        "        if q_proj_dtype_suffix == torch.bfloat16:\n",
        "            suffix_embs = suffix_embs.to(dtype=torch.bfloat16)\n",
        "\n",
        "        # Combine prefix and suffix for attention\n",
        "        pad_masks = torch.cat([prefix_pad_masks, suffix_pad_masks], dim=1)\n",
        "        att_masks = torch.cat([prefix_att_masks, suffix_att_masks], dim=1)\n",
        "\n",
        "        att_2d_masks = make_att_2d_masks(pad_masks, att_masks)\n",
        "        position_ids = torch.cumsum(pad_masks, dim=1) - 1\n",
        "\n",
        "        # For suffix, we only need attention mask for\n",
        "        # suffix tokens attending to full sequence\n",
        "        suffix_len = suffix_pad_masks.shape[1]\n",
        "        suffix_att_2d_masks = att_2d_masks[:, -suffix_len:, :]\n",
        "        # (B, suffix_len, full_len)\n",
        "        suffix_position_ids = position_ids[:, -suffix_len:]\n",
        "        # (B, suffix_len)\n",
        "\n",
        "        suffix_att_2d_masks_4d = self._prepare_attention_masks_4d(\n",
        "            suffix_att_2d_masks\n",
        "        )\n",
        "\n",
        "        # Forward pass for flow matching loss\n",
        "        def suffix_forward_func(\n",
        "            suffix_embs,\n",
        "            suffix_att_2d_masks_4d,\n",
        "            suffix_position_ids,\n",
        "            kv_cache,\n",
        "            adarms_cond\n",
        "        ):\n",
        "            (_, suffix_out), _ = self.paligemma_with_expert.forward(\n",
        "                attention_mask=suffix_att_2d_masks_4d,\n",
        "                position_ids=suffix_position_ids,\n",
        "                past_key_values=kv_cache,\n",
        "                inputs_embeds=[None, suffix_embs],\n",
        "                use_cache=False,\n",
        "                adarms_cond=[None, adarms_cond],\n",
        "            )\n",
        "            return suffix_out\n",
        "\n",
        "        suffix_out = self._apply_checkpoint(\n",
        "            suffix_forward_func,\n",
        "            suffix_embs,\n",
        "            suffix_att_2d_masks_4d,\n",
        "            suffix_position_ids,\n",
        "            kv_cache,\n",
        "            adarms_cond\n",
        "        )\n",
        "\n",
        "        suffix_out = suffix_out[:, -self.config.chunk_size:]\n",
        "        suffix_out = suffix_out.to(dtype=torch.float32)\n",
        "\n",
        "        def action_out_proj_func(suffix_out):\n",
        "            return self.action_out_proj(suffix_out)\n",
        "\n",
        "        v_t = self._apply_checkpoint(action_out_proj_func, suffix_out)\n",
        "\n",
        "        # Compute flow matching loss\n",
        "        if real_action_dim is not None:\n",
        "            # Truncate to real action dimension\n",
        "            flow_loss = F.mse_loss(\n",
        "                v_t[:, :, :real_action_dim],\n",
        "                u_t[:, :, :real_action_dim],\n",
        "                reduction=\"none\"\n",
        "            )\n",
        "        else:\n",
        "            flow_loss = F.mse_loss(u_t, v_t, reduction=\"none\")\n",
        "\n",
        "        # flow_loss shape: (B, action_horizon, action_dim)\n",
        "        # Mean over action dimensions\n",
        "        flow_loss = flow_loss.mean(dim=-1)  # (B, action_horizon)\n",
        "        flow_loss = flow_loss.mean(dim=-1)  # (B,)\n",
        "\n",
        "        # Combine losses\n",
        "        if subtask_generation_loss is not None:\n",
        "            total_loss = subtask_generation_loss + flow_loss\n",
        "        else:\n",
        "            total_loss = flow_loss\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample_low_level_task(\n",
        "                self,\n",
        "                images,\n",
        "                img_masks,\n",
        "                tokens,\n",
        "                masks,\n",
        "                max_decoding_steps: int = 20,\n",
        "                eos_token_id: int = 1,\n",
        "                temperature: float = 0.1):\n",
        "        \"\"\"\n",
        "        Sample tokens autoregressively from the language model.\n",
        "        Each N second the model should generate the task.\n",
        "\n",
        "        Args:\n",
        "            images: List of image tensors, each of shape\n",
        "                (B, C, H, W) or (B, H, W, C)\n",
        "            img_masks: List of boolean masks, to each image in the\n",
        "             list it corresponds to a mask each of shape (B,)\n",
        "                indicating valid images.\n",
        "            tokens: Language tokens of shape (B, seq_len_lang)\n",
        "            masks: Language attention masks of shape (B, seq_len_lang)\n",
        "            max_decoding_steps: Maximum number of tokens to generate\n",
        "            eos_token_id: End-of-sequence token ID\n",
        "            temperature: Sampling temperature\n",
        "\n",
        "        Returns:\n",
        "            output_tokens: (B, max_decoding_steps) - generated token IDs\n",
        "            past_key_values: Updated KV cache\n",
        "        \"\"\"\n",
        "        prefix_embs, prefix_pad_masks, prefix_att_masks = self.embed_prefix(\n",
        "            images, img_masks, tokens, masks\n",
        "        )\n",
        "\n",
        "        prefix_att_2d_masks = make_att_2d_masks(\n",
        "            prefix_pad_masks, prefix_att_masks\n",
        "        )\n",
        "\n",
        "        # Compute position IDs for attention masks minus 1\n",
        "        prefix_position_ids = torch.cumsum(prefix_pad_masks, dim=1) - 1\n",
        "\n",
        "        # Convert 2D attention masks to 4D format expected by the model\n",
        "        prefix_att_2d_masks_4d = self._prepare_attention_masks_4d(\n",
        "            prefix_att_2d_masks\n",
        "        )\n",
        "\n",
        "        lang_model = self.paligemma_with_expert.paligemma.language_model\n",
        "        lang_model.config._attn_implementation = \"eager\"  # noqa: SLF001\n",
        "\n",
        "        embeddings, past_key_values = self.paligemma_with_expert.forward(\n",
        "            attention_mask=prefix_att_2d_masks_4d,\n",
        "            position_ids=prefix_position_ids,\n",
        "            past_key_values=None,\n",
        "            inputs_embeds=[prefix_embs, None],\n",
        "            use_cache=True,\n",
        "        )\n",
        "\n",
        "        # embeddings[0] shape: (B, total_seq_len, embd_dim)\n",
        "        # Extract last token: (B, embd_dim)\n",
        "        last_token_embed = embeddings[0][:, -1, :]\n",
        "\n",
        "        # Convert to logits: (B, vocab_size)\n",
        "        last_logits = (\n",
        "            self.paligemma_with_expert.paligemma.lm_head(last_token_embed)\n",
        "        )\n",
        "\n",
        "        batch_size = last_logits.shape[0]\n",
        "        device = last_logits.device\n",
        "\n",
        "        # prefix_valid_length is the number of valid (non-padded) tokens\n",
        "        prefix_valid_length = torch.sum(prefix_pad_masks, dim=1)  # (B,)\n",
        "        output_tokens = torch.zeros((batch_size, max_decoding_steps),\n",
        "                                    dtype=torch.long, device=device)\n",
        "        all_eos = torch.zeros(batch_size, dtype=torch.bool, device=device)\n",
        "\n",
        "        # Prefix attention mask plus the new token attention mask\n",
        "        running_attention_mask = prefix_pad_masks.clone()\n",
        "\n",
        "        # Autoregressive Loop\n",
        "        for step in range(max_decoding_steps):\n",
        "            # Sample next token\n",
        "            if temperature > 0.0:\n",
        "                probs = F.softmax(last_logits / temperature, dim=-1)\n",
        "                # token shape: (B, 1)\n",
        "                token = torch.multinomial(probs, num_samples=1)\n",
        "            else:\n",
        "                token = torch.argmax(last_logits, dim=-1, keepdim=True)\n",
        "\n",
        "            output_tokens[:, step] = token.squeeze(-1)\n",
        "\n",
        "            # Check for EOS\n",
        "            all_eos |= (token.squeeze(-1) == eos_token_id)\n",
        "            if all_eos.all():\n",
        "                break\n",
        "\n",
        "            # Feed the new token back in the model\n",
        "            # token shape: (B, 1) -> embed_tokens returns (B, 1, embd_dim)\n",
        "            next_token_embeds = lang_model.embed_tokens(token)\n",
        "\n",
        "            # Create position_ids for the new token\n",
        "            position_ids = prefix_valid_length[:, None] + step\n",
        "\n",
        "            # Create attention mask for the new token\n",
        "            new_mask = torch.ones(\n",
        "                (batch_size, 1),\n",
        "                dtype=running_attention_mask.dtype,\n",
        "                device=device\n",
        "            )\n",
        "\n",
        "            running_attention_mask = torch.cat(\n",
        "                [running_attention_mask, new_mask], dim=1)\n",
        "\n",
        "            # The attention mask can be 2d or 4d.\n",
        "            embeds_list, past_key_values = self.paligemma_with_expert.forward(\n",
        "                inputs_embeds=[next_token_embeds, None],\n",
        "                attention_mask=running_attention_mask,\n",
        "                position_ids=position_ids,\n",
        "                past_key_values=past_key_values,\n",
        "                use_cache=True,\n",
        "            )\n",
        "\n",
        "            prefix_output = embeds_list[0]\n",
        "            last_token_embed = prefix_output[:, -1, :]\n",
        "            last_logits = (\n",
        "                self.paligemma_with_expert.paligemma.lm_head(last_token_embed)\n",
        "            )\n",
        "\n",
        "        # We return the tokens and the KV cache for the actions.\n",
        "        return output_tokens, past_key_values\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample_actions(self,\n",
        "                       images=None,\n",
        "                       img_masks=None,\n",
        "                       tokens=None,\n",
        "                       masks=None,\n",
        "                       past_key_values=None,\n",
        "                       prefix_pad_masks=None,\n",
        "                       noise=None,\n",
        "                       num_steps=None,\n",
        "                       **kwargs: Unpack[ActionSelectKwargs]):\n",
        "        \"\"\"\n",
        "        Sample actions using flow matching.\n",
        "        Each N second the model should generate the action.\n",
        "        Args:\n",
        "            images: List of image tensors, each of shape\n",
        "                (B, C, H, W) or (B, H, W, C)\n",
        "            img_masks: List of boolean masks, to each image in the\n",
        "             list it corresponds to a mask each of shape (B,)\n",
        "                indicating valid images.\n",
        "            tokens: Language tokens of shape (B, seq_len_lang)\n",
        "            masks: Language attention masks of shape (B, seq_len_lang)\n",
        "            noise: Noise tensor for flow matching\n",
        "            num_steps: Number of steps to sample\n",
        "            kwargs: Additional arguments\n",
        "\n",
        "        Returns:\n",
        "            actions: (B, action_horizon, action_dim)\n",
        "        \"\"\"\n",
        "        # If images are provided, we need to embed the prefix.\n",
        "        # Otherwise, we use the prefix_pad_masks and past_key_values that\n",
        "        # were already computed on the sample_low_level_task.\n",
        "        if images is not None:\n",
        "            prefix_embs, prefix_pad_masks, prefix_att_mask = self.embed_prefix(\n",
        "                images, img_masks, tokens, masks\n",
        "            )\n",
        "\n",
        "            prefix_att_2d_masks = make_att_2d_masks(prefix_pad_masks,\n",
        "                                                    prefix_att_mask)\n",
        "            prefix_position_ids = torch.cumsum(prefix_pad_masks, dim=1) - 1\n",
        "\n",
        "            prefix_att_2d_masks_4d = self._prepare_attention_masks_4d(\n",
        "                prefix_att_2d_masks\n",
        "            )\n",
        "            self.paligemma_with_expert.paligemma.language_model.config._attn_implementation = \"eager\"  # noqa: SLF001, E501\n",
        "\n",
        "            _, past_key_values = self.paligemma_with_expert.forward(\n",
        "                attention_mask=prefix_att_2d_masks_4d,\n",
        "                position_ids=prefix_position_ids,\n",
        "                past_key_values=None,\n",
        "                inputs_embeds=[prefix_embs, None],\n",
        "                use_cache=True,\n",
        "            )\n",
        "\n",
        "        # IMPORTANT:\n",
        "        # Here we need to be careful with the prefix_pad_masks and past_key_values.\n",
        "        # THe new tokens generated by low level task are not part of the prefix_pad_masks and past_key_values.\n",
        "\n",
        "\n",
        "        bsize = tokens.shape[0]\n",
        "        device = tokens.device\n",
        "\n",
        "        if num_steps is None:\n",
        "            num_steps = self.config.num_inference_steps\n",
        "\n",
        "        if noise is None:\n",
        "            # Sample noise with padded dimension as expected by action_in_proj\n",
        "            actions_shape = (\n",
        "                bsize,\n",
        "                self.config.chunk_size,\n",
        "                self.config.max_action_dim,\n",
        "            )  # Use config max_action_dim for internal processing\n",
        "            noise = self.sample_noise(actions_shape, device)\n",
        "\n",
        "        dt = -1.0 / num_steps\n",
        "        dt = torch.tensor(dt, dtype=torch.float32, device=device)\n",
        "        x_t = noise\n",
        "        time = torch.tensor(1.0, dtype=torch.float32, device=device)\n",
        "\n",
        "        while time >= -dt / 2:\n",
        "            expanded_time = time.expand(bsize)\n",
        "            # Define a closure function to properly capture expanded_time\n",
        "            # This avoids the lambda expression (E731)\n",
        "            # and loop variable binding (B023) issues\n",
        "            def denoise_step_partial_call(input_x_t,\n",
        "                                          current_timestep=expanded_time):\n",
        "                return self.denoise_step(\n",
        "                    prefix_pad_masks=prefix_pad_masks,\n",
        "                    past_key_values=past_key_values,\n",
        "                    x_t=input_x_t,\n",
        "                    timestep=current_timestep,\n",
        "                )\n",
        "\n",
        "            if self._rtc_enabled():\n",
        "                inference_delay = kwargs.get(\"inference_delay\")\n",
        "                prev_chunk_left_over = kwargs.get(\"prev_chunk_left_over\")\n",
        "                execution_horizon = kwargs.get(\"execution_horizon\")\n",
        "\n",
        "                v_t = self.rtc_processor.denoise_step(\n",
        "                    x_t=x_t,\n",
        "                    prev_chunk_left_over=prev_chunk_left_over,\n",
        "                    inference_delay=inference_delay,\n",
        "                    time=time,\n",
        "                    original_denoise_step_partial=denoise_step_partial_call,\n",
        "                    execution_horizon=execution_horizon,\n",
        "                )\n",
        "            else:\n",
        "                v_t = denoise_step_partial_call(x_t)\n",
        "\n",
        "            # Euler step\n",
        "            x_t += dt * v_t\n",
        "\n",
        "            # Record x_t and v_t after Euler step\n",
        "            if self.rtc_processor is not None and self.rtc_processor.is_debug_enabled():  # noqa: E501\n",
        "                self.rtc_processor.track(time=time, x_t=x_t, v_t=v_t)\n",
        "\n",
        "            time += dt\n",
        "\n",
        "        return x_t\n",
        "\n",
        "    def denoise_step(\n",
        "        self,\n",
        "        prefix_pad_masks,\n",
        "        past_key_values,\n",
        "        x_t,\n",
        "        timestep,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Apply one denoising step of the noise `x_t` at a given timestep.\n",
        "\n",
        "        Args:\n",
        "            prefix_pad_masks: (B, seq_len_images + seq_len_lang)\n",
        "            past_key_values: KV cache for efficient autoregressive generation\n",
        "            x_t: (B, action_horizon, action_dim) -noise tensor for flowmatching\n",
        "            timestep: (B,) - current timestep\n",
        "        \"\"\"\n",
        "        suffix_embs, suffix_pad_masks, suffix_att_masks, adarms_cond = (\n",
        "            self.embed_suffix(x_t, timestep)\n",
        "        )\n",
        "\n",
        "        suffix_len = suffix_pad_masks.shape[1]\n",
        "        batch_size = prefix_pad_masks.shape[0]\n",
        "        prefix_len = prefix_pad_masks.shape[1]\n",
        "\n",
        "        prefix_pad_2d_masks = prefix_pad_masks[:, None, :].expand(batch_size,\n",
        "                                                                  suffix_len,\n",
        "                                                                  prefix_len)\n",
        "        suffix_att_2d_masks = make_att_2d_masks(suffix_pad_masks,\n",
        "                                                suffix_att_masks)\n",
        "        full_att_2d_masks = torch.cat([prefix_pad_2d_masks,\n",
        "                                       suffix_att_2d_masks], dim=2)\n",
        "\n",
        "        prefix_offsets = torch.sum(prefix_pad_masks, dim=-1)[:, None]\n",
        "        position_ids = prefix_offsets + torch.cumsum(suffix_pad_masks, dim=1)\n",
        "        position_ids -= 1\n",
        "\n",
        "        full_att_2d_masks_4d = self._prepare_attention_masks_4d(\n",
        "            full_att_2d_masks)\n",
        "        self.paligemma_with_expert.gemma_expert.model.config._attn_implementation = \"eager\"  # noqa: SLF001, E501\n",
        "\n",
        "        # Important, here is the vector field core\n",
        "        outputs_embeds, _ = self.paligemma_with_expert.forward(\n",
        "            attention_mask=full_att_2d_masks_4d,\n",
        "            position_ids=position_ids,\n",
        "            past_key_values=past_key_values,\n",
        "            inputs_embeds=[None, suffix_embs],\n",
        "            use_cache=False,\n",
        "            adarms_cond=[None, adarms_cond],\n",
        "        )\n",
        "\n",
        "        suffix_out = outputs_embeds[1]\n",
        "        suffix_out = suffix_out[:, -self.config.chunk_size:]\n",
        "        suffix_out = suffix_out.to(dtype=torch.float32)\n",
        "        return self.action_out_proj(suffix_out)\n",
        "\n",
        "\n",
        "class PI05Policy(PreTrainedPolicy):\n",
        "    \"\"\"PI05 Policy for LeRobot.\"\"\"\n",
        "\n",
        "    config_class = PI05Config\n",
        "    name = \"pi05\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        config: PI05Config,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            config: Policy configuration class instance.\n",
        "        \"\"\"\n",
        "        super().__init__(config)\n",
        "        config.validate_features()\n",
        "        self.config = config\n",
        "\n",
        "        # Initialize the core PI05 model\n",
        "        self.init_rtc_processor()\n",
        "        self.model = PI05Model(config, rtc_processor=self.rtc_processor)\n",
        "\n",
        "        # Enable gradient checkpointing if requested\n",
        "        if config.gradient_checkpointing:\n",
        "            self.model.gradient_checkpointing_enable()\n",
        "\n",
        "        self.model.to(config.device)\n",
        "\n",
        "        self.reset()\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(\n",
        "        cls: builtins.type[T],\n",
        "        pretrained_name_or_path: str | Path,\n",
        "        *,\n",
        "        config: PreTrainedConfig | None = None,\n",
        "        force_download: bool = False,\n",
        "        resume_download: bool | None = None,\n",
        "        proxies: dict | None = None,\n",
        "        token: str | bool | None = None,\n",
        "        cache_dir: str | Path | None = None,\n",
        "        local_files_only: bool = False,\n",
        "        revision: str | None = None,\n",
        "        strict: bool = True,\n",
        "        **kwargs,\n",
        "    ) -> T:\n",
        "        \"\"\"\n",
        "        Override the from_pretrained method to handle\n",
        "        key remapping and display important disclaimer.\"\"\"\n",
        "        if pretrained_name_or_path is None:\n",
        "            raise ValueError(\"pretrained_name_or_path is required\")\n",
        "\n",
        "        # Use provided config if available, otherwise create default config\n",
        "        if config is None:\n",
        "            config = PreTrainedConfig.from_pretrained(\n",
        "                pretrained_name_or_path=pretrained_name_or_path,\n",
        "                force_download=force_download,\n",
        "                resume_download=resume_download,\n",
        "                proxies=proxies,\n",
        "                token=token,\n",
        "                cache_dir=cache_dir,\n",
        "                local_files_only=local_files_only,\n",
        "                revision=revision,\n",
        "                **kwargs,\n",
        "            )\n",
        "\n",
        "        # Initialize model without loading weights\n",
        "        # Check if dataset_stats were provided in kwargs\n",
        "        model = cls(config, **kwargs)\n",
        "\n",
        "        # Now manually load and remap the state dict\n",
        "        try:\n",
        "            # Try to load the pytorch_model.bin or model.safetensors file\n",
        "            print(f\"Loading model from: {pretrained_name_or_path}\")\n",
        "            try:\n",
        "                from transformers.utils import cached_file\n",
        "\n",
        "                # Try safetensors first\n",
        "                resolved_file = cached_file(\n",
        "                    pretrained_name_or_path,\n",
        "                    \"model.safetensors\",\n",
        "                    cache_dir=kwargs.get(\"cache_dir\"),\n",
        "                    force_download=kwargs.get(\"force_download\", False),\n",
        "                    resume_download=kwargs.get(\"resume_download\"),\n",
        "                    proxies=kwargs.get(\"proxies\"),\n",
        "                    use_auth_token=kwargs.get(\"use_auth_token\"),\n",
        "                    revision=kwargs.get(\"revision\"),\n",
        "                    local_files_only=kwargs.get(\"local_files_only\", False),\n",
        "                )\n",
        "                from safetensors.torch import load_file\n",
        "\n",
        "                original_state_dict = load_file(resolved_file)\n",
        "                print(\"âœ“ Loaded state dict from model.safetensors\")\n",
        "            except Exception as e:\n",
        "                print(f\"Could not load state dict from remote files: {e}\")\n",
        "                print(\"Returning model without loading pretrained weights\")\n",
        "                return model\n",
        "\n",
        "            # First, fix any key differences\n",
        "            fixed_state_dict = model._fix_pytorch_state_dict_keys(\n",
        "                original_state_dict, model.config)\n",
        "\n",
        "            # Then add \"model.\" prefix for all keys that don't already have it\n",
        "            remapped_state_dict = {}\n",
        "            remap_count = 0\n",
        "\n",
        "            for key, value in fixed_state_dict.items():\n",
        "                if not key.startswith(\"model.\"):\n",
        "                    new_key = f\"model.{key}\"\n",
        "                    remapped_state_dict[new_key] = value\n",
        "                    remap_count += 1\n",
        "                    if remap_count <= 10:  # Only print first 10 to avoid spam\n",
        "                        print(f\"Remapped: {key} -> {new_key}\")\n",
        "                else:\n",
        "                    remapped_state_dict[key] = value\n",
        "\n",
        "            if remap_count > 0:\n",
        "                print(f\"Remapped {remap_count} state dict keys\")\n",
        "\n",
        "            # Load the remapped state dict into the model\n",
        "            missing_keys, unexpected_keys = model.load_state_dict(\n",
        "                remapped_state_dict, strict=strict)\n",
        "\n",
        "            if missing_keys:\n",
        "                print(\n",
        "                    f\"Missing keys when loading state dict:\"\n",
        "                    f\"{len(missing_keys)} keys\"\n",
        "                )\n",
        "                if len(missing_keys) <= 5:\n",
        "                    for key in missing_keys:\n",
        "                        print(f\"  - {key}\")\n",
        "                else:\n",
        "                    for key in missing_keys[:5]:\n",
        "                        print(f\"  - {key}\")\n",
        "                    print(f\"  ... and {len(missing_keys) - 5} more\")\n",
        "\n",
        "            if unexpected_keys:\n",
        "                print(\n",
        "                    f\"Unexpected keys when loading state dict: \"\n",
        "                    f\"{len(unexpected_keys)} keys\"\n",
        "                )\n",
        "                if len(unexpected_keys) <= 5:\n",
        "                    for key in unexpected_keys:\n",
        "                        print(f\"  - {key}\")\n",
        "                else:\n",
        "                    for key in unexpected_keys[:5]:\n",
        "                        print(f\"  - {key}\")\n",
        "                    print(f\"  ... and {len(unexpected_keys) - 5} more\")\n",
        "\n",
        "            if not missing_keys and not unexpected_keys:\n",
        "                print(\"All keys loaded successfully!\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not remap state dict keys: {e}\")\n",
        "\n",
        "        return model\n",
        "\n",
        "    def _fix_pytorch_state_dict_keys(\n",
        "        self, state_dict, model_config\n",
        "    ):  # see openpi `BaseModelConfig, _fix_pytorch_state_dict_keys`\n",
        "        \"\"\"Fix state dict keys to match current model architecture.\"\"\"\n",
        "        import re\n",
        "\n",
        "        fixed_state_dict = {}\n",
        "\n",
        "        for key, value in state_dict.items():\n",
        "            new_key = key\n",
        "\n",
        "            # Handle layer norm structure changes:\n",
        "            # .weight -> .dense.weight + .dense.bias\n",
        "            # For gemma expert layers\n",
        "            layer_norm_pattern = (\n",
        "                r\"paligemma_with_expert\\.gemma_expert\\.model\\.layers\\.\\d+\\.\"\n",
        "                r\"(input_layernorm|post_attention_layernorm)\\.weight\"\n",
        "            )\n",
        "            if re.match(layer_norm_pattern, key):\n",
        "                # Check if the model actually has adaRMS enabled\n",
        "                # for the expert\n",
        "                expert_config = (\n",
        "                    self.model.paligemma_with_expert.gemma_expert.config\n",
        "                )\n",
        "                expert_uses_adarms = getattr(\n",
        "                    expert_config, \"use_adarms\", False\n",
        "                )\n",
        "                if expert_uses_adarms:\n",
        "                    logging.warning(\n",
        "                        f\"Skipping layer norm key (adaRMS mismatch): {key}\"\n",
        "                    )\n",
        "                    continue\n",
        "\n",
        "            norm_pattern = (\n",
        "                r\"paligemma_with_expert\\.gemma_expert\\.model\\.norm\\.weight\"\n",
        "            )\n",
        "            if re.match(norm_pattern, key):\n",
        "                # Check if the model actually has adaRMS enabled\n",
        "                # for the expert\n",
        "                expert_config = (\n",
        "                    self.model.paligemma_with_expert.gemma_expert.config\n",
        "                )\n",
        "                expert_uses_adarms = getattr(\n",
        "                    expert_config, \"use_adarms\", False\n",
        "                )\n",
        "                if expert_uses_adarms:\n",
        "                    logging.warning(\n",
        "                        f\"Skipping norm key (adaRMS mismatch): {key}\"\n",
        "                    )\n",
        "                    continue\n",
        "\n",
        "            # Handle MLP naming changes for pi05\n",
        "            # pi05 model expects time_mlp_*\n",
        "            # but checkpoint might have action_time_mlp_*\n",
        "            if key.startswith(\"action_time_mlp_in.\"):\n",
        "                new_key = key.replace(\"action_time_mlp_in.\", \"time_mlp_in.\")\n",
        "            elif key.startswith(\"action_time_mlp_out.\"):\n",
        "                new_key = key.replace(\"action_time_mlp_out.\", \"time_mlp_out.\")\n",
        "            # Also handle state_proj which shouldn't exist in pi05\n",
        "            if key.startswith(\"state_proj.\"):\n",
        "                logging.warning(\n",
        "                    f\"Skipping state_proj key in pi05 mode: {key}\"\n",
        "                )\n",
        "                continue\n",
        "\n",
        "            # Handle vision tower embedding layer potential differences\n",
        "            if \"patch_embedding\" in key:\n",
        "                # Some checkpoints might have this,\n",
        "                # but current model expects different structure\n",
        "                logging.warning(\n",
        "                    f\"Vision embedding key might need handling: {key}\"\n",
        "                )\n",
        "\n",
        "            fixed_state_dict[new_key] = value\n",
        "\n",
        "        return fixed_state_dict\n",
        "\n",
        "    def get_optim_params(self) -> dict:\n",
        "        return self.parameters()\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset internal state - called when environment resets.\"\"\"\n",
        "        self._action_queue = deque(maxlen=self.config.n_action_steps)\n",
        "        self._queues = {\n",
        "            ACTION: deque(maxlen=self.config.n_action_steps),\n",
        "        }\n",
        "\n",
        "    def init_rtc_processor(self):\n",
        "        \"\"\"Initialize RTC processor if RTC is enabled in config.\"\"\"\n",
        "        self.rtc_processor = None\n",
        "\n",
        "        # Create processor if config provided\n",
        "        # If RTC is not enabled - we can still track the denoising data\n",
        "        if self.config.rtc_config is not None:\n",
        "            self.rtc_processor = RTCProcessor(self.config.rtc_config)\n",
        "\n",
        "            model_value = getattr(self, \"model\", None)\n",
        "            if model_value is not None:\n",
        "                model_value.rtc_processor = self.rtc_processor\n",
        "\n",
        "    def _rtc_enabled(self) -> bool:\n",
        "        return (\n",
        "            self.config.rtc_config is not None\n",
        "            and self.config.rtc_config.enabled\n",
        "        )\n",
        "\n",
        "    def _preprocess_images(\n",
        "        self, batch: dict[str, Tensor]\n",
        "    ) -> tuple[list[Tensor], list[Tensor]]:\n",
        "        \"\"\"Preprocess images for the model.\n",
        "        Images from LeRobot are typically in [B, C, H, W]\n",
        "        format and normalized to [0, 1].\n",
        "        PaliGemma expects images in [B, C, H, W]\n",
        "        format and normalized to [-1, 1].\n",
        "        \"\"\"\n",
        "        images = []\n",
        "        img_masks = []\n",
        "\n",
        "        # Get device from model parameters\n",
        "        device = next(self.parameters()).device\n",
        "\n",
        "        present_img_keys = [\n",
        "            key for key in self.config.image_features if key in batch\n",
        "        ]\n",
        "        missing_img_keys = [\n",
        "            key for key in self.config.image_features if key not in batch\n",
        "        ]\n",
        "\n",
        "        if len(present_img_keys) == 0:\n",
        "            raise ValueError(\n",
        "                f\"All image features are missing from the batch. \"\n",
        "                f\"At least one expected. \"\n",
        "                f\"(batch: {batch.keys()}) \"\n",
        "                f\"(image_features: {self.config.image_features})\"\n",
        "            )\n",
        "\n",
        "        # Preprocess image features present in the batch\n",
        "        # present_img_keys: ['top', 'left', 'right']\n",
        "        for key in present_img_keys:\n",
        "            # img shape: (B, H, W, C)\n",
        "            img = batch[key]\n",
        "\n",
        "            # Ensure tensor is on the same device as the model\n",
        "            if img.device != device:\n",
        "                img = img.to(device)\n",
        "\n",
        "            # Ensure float32 dtype for consistency\n",
        "            if img.dtype != torch.float32:\n",
        "                img = img.to(torch.float32)\n",
        "\n",
        "            # from openpi preprocess_observation_pytorch:\n",
        "            # Handle both [B, C, H, W] and [B, H, W, C] formats\n",
        "            is_channels_first = img.shape[1] == 3\n",
        "            # Check if channels are in dimension 1\n",
        "\n",
        "            if is_channels_first:\n",
        "                # Convert [B, C, H, W] to [B, H, W, C] for processing\n",
        "                img = img.permute(0, 2, 3, 1)\n",
        "\n",
        "            # from openpi preprocess_observation_pytorch:\n",
        "            # Resize with padding if needed\n",
        "            if img.shape[1:3] != self.config.image_resolution:\n",
        "                img = resize_with_pad_torch(img, *self.config.image_resolution)\n",
        "\n",
        "            # Normalize from [0,1] to [-1,1] as expected by siglip\n",
        "            img = img * 2.0 - 1.0\n",
        "\n",
        "            # Convert back to [B, C, H, W] format if it was originally C-first\n",
        "            if is_channels_first:\n",
        "                img = img.permute(0, 3, 1, 2)  # [B, H, W, C] -> [B, C, H, W]\n",
        "\n",
        "            images.append(img)\n",
        "            # Create mask (all ones for real images)\n",
        "            bsize = img.shape[0]\n",
        "            mask = torch.ones(bsize, dtype=torch.bool, device=device)\n",
        "            img_masks.append(mask)\n",
        "\n",
        "        # Create image features not present in\n",
        "        # the batch as fully 0 padded images\n",
        "        for _num_empty_cameras in range(len(missing_img_keys)):\n",
        "            img = torch.ones_like(img) * -1  # Padded with -1 for SigLIP\n",
        "            mask = torch.zeros_like(mask)  # Mask is zero for empty cameras\n",
        "            images.append(img)\n",
        "            img_masks.append(mask)\n",
        "\n",
        "        return images, img_masks\n",
        "\n",
        "    def prepare_action(self, batch):\n",
        "        \"\"\"Pad action\"\"\"\n",
        "        actions = pad_vector(batch[ACTION], self.config.max_action_dim)\n",
        "        return actions\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def select_action(self, batch: dict[str, Tensor]) -> Tensor:\n",
        "        \"\"\"\n",
        "        Select a single action given environment observations.\n",
        "        Args:\n",
        "            batch: dict[str, Tensor] - batch of environment observations\n",
        "\n",
        "        Returns:\n",
        "            action: (action_dim) - selected action\n",
        "        \"\"\"\n",
        "        assert not self._rtc_enabled(), (\n",
        "            \"RTC is not supported for select_action,\"\n",
        "            \" use it with predict_action_chunk\"\n",
        "        )\n",
        "\n",
        "        self.eval()\n",
        "\n",
        "        # Action queue logic for n_action_steps > 1\n",
        "        if len(self._action_queue) == 0:\n",
        "            actions = self.predict_action_chunk(batch)[\n",
        "                :, : self.config.n_action_steps\n",
        "            ]\n",
        "            # Transpose to get shape (n_action_steps, batch_size, action_dim)\n",
        "            self._action_queue.extend(actions.transpose(0, 1))\n",
        "\n",
        "        return self._action_queue.popleft()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict_action_chunk(self, batch: dict[str, Tensor],\n",
        "                             **kwargs: Unpack[ActionSelectKwargs]) -> Tensor:\n",
        "        \"\"\"Predict a chunk of actions given environment observations.\n",
        "        Args:\n",
        "            batch: dict[str, Tensor] - batch of environment observations\n",
        "            kwargs: Additional arguments\n",
        "\n",
        "        Returns:\n",
        "            actions: (B, action_horizon, action_dim)\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "\n",
        "        # Prepare inputs\n",
        "        images, img_masks = self._preprocess_images(batch)\n",
        "        tokens = batch[f\"{OBS_LANGUAGE_TOKENS}\"]\n",
        "        masks = batch[f\"{OBS_LANGUAGE_ATTENTION_MASK}\"]\n",
        "\n",
        "        subtask_condition = batch.get(\"subtask_condition\", None)\n",
        "\n",
        "        if subtask_condition:\n",
        "            subtask_tokens, subtask_past_key_values = self.model.sample_low_level_task(\n",
        "                images, img_masks, tokens, masks\n",
        "            )\n",
        "\n",
        "            actions = self.model.sample_actions(\n",
        "                past_key_values=subtask_past_key_values,\n",
        "            )\n",
        "\n",
        "        actions = self.model.sample_actions(\n",
        "            images, img_masks, tokens, masks, **kwargs\n",
        "        )\n",
        "\n",
        "        # Unpad actions to actual action dimension\n",
        "        original_action_dim = self.config.output_features[ACTION].shape[0]\n",
        "        actions = actions[:, :, :original_action_dim]\n",
        "\n",
        "        return actions\n",
        "\n",
        "    def forward(self, batch: dict[str, Tensor]) -> tuple[Tensor, dict]:\n",
        "        \"\"\"\n",
        "        Run the batch through the model\n",
        "        and compute the loss for training.\n",
        "\n",
        "        Args:\n",
        "          batch: dict[str, Tensor]\n",
        "            - images: [B, C, H, W] image tensors\n",
        "            - img_masks: [B] image mask tensors\n",
        "            - tokens: [B, N] tokenized prompt tokens (includes prefix + suffix)\n",
        "            - masks: [B, N] token padding masks\n",
        "            - actions: [B, N, action_dim] actions\n",
        "            - token_loss_mask: [B, N] loss mask (True where we compute loss)\n",
        "\n",
        "        Returns:\n",
        "          tuple[Tensor, dict]\n",
        "            - loss: [B] per-sample loss\n",
        "            - loss_dict: dict\n",
        "              - loss: float\n",
        "              - loss_per_sample: [B] per-sample loss\n",
        "        \"\"\"\n",
        "        # Prepare inputs\n",
        "        images, img_masks = self._preprocess_images(batch)\n",
        "        tokens, masks = batch[\n",
        "            f\"{OBS_LANGUAGE_TOKENS}\"], batch[f\"{OBS_LANGUAGE_ATTENTION_MASK}\"]\n",
        "\n",
        "        actions = self.prepare_action(batch)\n",
        "\n",
        "        # Get token loss mask if available (for cross-entropy loss on tokens)\n",
        "        token_loss_mask = batch.get(\"token_loss_mask\", None)\n",
        "\n",
        "        # Get real action dimension (for handling padded actions)\n",
        "        original_action_dim = self.config.output_features[ACTION].shape[0]\n",
        "\n",
        "        # Compute loss (includes both CE loss for tokens and\n",
        "        # flow matching loss for actions)\n",
        "        losses = self.model.forward(\n",
        "            images, img_masks, tokens, masks, actions,\n",
        "            token_loss_mask=token_loss_mask,\n",
        "            real_action_dim=original_action_dim\n",
        "        )\n",
        "\n",
        "        # losses shape: (B,) - per-sample total loss (CE + flow matching)\n",
        "        loss = losses.mean()\n",
        "\n",
        "        loss_dict = {\n",
        "            \"loss\": loss.item(),\n",
        "            \"loss_per_sample\": losses.detach().cpu().numpy().tolist(),\n",
        "        }\n",
        "\n",
        "        return loss, loss_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319,
          "referenced_widgets": [
            "55b76b19e6d643f2863d49f8d17770fd",
            "92fe0c4f171e4277bc92e2f76bd7b8ca",
            "354b05db6c2742569690177416deedf2",
            "21ba6cc5d39a4acf9d0c13f27ede9153",
            "41bbe56b6fee46e2aee47b486365ea10",
            "d2eda561de0b4e19a7efba6a74876f7c",
            "5aa1aa3d32b140d3bef1483cc74411d7",
            "f0b0a98f00e34039809a08ea17258840",
            "3f188e0e975144598c7ea8cdc21a743c",
            "43ee1b1856534ea1859dd6a18ce8af3f",
            "32c721722edf4ecc986d6696076698a3",
            "46b46c92f71d44b0b95c1ae8f3e966bb",
            "b3afb172f8eb444f812320ab845027b5",
            "b94200e0d7464d25b1efad9b15779066",
            "1ea5697015a148a9b9221a405ddaf349",
            "9d3d53b86c3d4064892a9762620c10e1",
            "b9fc1964f5634f82950605f286437ab6",
            "ba1bff96e27941be967a66e11080a617",
            "bee1037741574150ae70b0d9abd99472",
            "2c4f3fb3fd3e4e6a99d3f4eb68bd06bb",
            "ec0ea6ba423f434ba4bea34049ffdb6e",
            "dff9dedbe4eb4495af8c738d3aba9bf4",
            "c1234854234944debfbff33bf19cf144",
            "0dd2b95b66d34bff91f0b188da1e8162",
            "eb32da5921974aaaad0e2e61d18b3906",
            "78d30d850e264f388663d7b53020ca27",
            "cf3d25ff5dca411aaece66b24c80b542",
            "8c0e368f3b06487d960ceda435762995",
            "acb2a96b1cb04efb8191a40c5d6d1ec0",
            "34079fea82654bcc99f8b5f61961d642",
            "cf76f0e71eb742d18e3b925051ec6a9f",
            "652a3773f83b486bac473b772678b303",
            "4475ced03e674752a15b74b22675952b",
            "562fbd46ca71495bbdb31485657ae0f9",
            "71ceb5cea51440bd98c3dd2ffe396729",
            "814709f8d5934b30994720d17aeabbd2",
            "30e52ea0eaa64b31b4f097d7416b3f1d",
            "642ed517c82c41d49b08a8f03f037fe1",
            "83e32c8c349f431d8ed3e623727d5baf",
            "9c90b087c5164494983f7ddd091992e5",
            "d8942cce372d4a15bd825c61b8c6b315",
            "0f6d37333dae4daf9bd168ee9fbb0bb4",
            "4ca81ad055bc4c8d9464493de77de4fd",
            "aa6b17fcedce4ae1a8983f866fc1fa54",
            "d202b67557024a71980a5fbac84c9ec7",
            "b09458032fbd4cca9ec701dfdfa3b774",
            "5b333a7d686c4dadb48e6183e3c567cb",
            "bd8bee9872ba46ccb3fbf0f40f59fca3",
            "90b115415a78498db76198fd5a963b59",
            "8becd8f994434975a596819814b00d26",
            "8bb4561af6354bd7a79c37c9b77137e1",
            "3e62feeeced54508a8ac991104b6fd8b",
            "3f8eb292d9db4458857e18813964e61e",
            "04d2f1b0da6c488ea2cce221e84a54af",
            "7cc817872ea84a5e8c777d59aeff258b"
          ]
        },
        "id": "gp93wBnKO8jp",
        "outputId": "b1ef94d2-a860-490e-bcb2-228d4063d97d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading tokenizer...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/40.0k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "55b76b19e6d643f2863d49f8d17770fd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.26M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "46b46c92f71d44b0b95c1ae8f3e966bb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/24.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c1234854234944debfbff33bf19cf144"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/607 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "562fbd46ca71495bbdb31485657ae0f9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d202b67557024a71980a5fbac84c9ec7"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "# Load tokenizer\n",
        "print(\"Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/paligemma-3b-pt-224\",\n",
        "                                          use_fast=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b71ed7b654a848e985b834f635650aa8",
            "36bbb309d8e14bc9ab012d86420026f9",
            "955feb894c4c41269506820f6e799849",
            "a635d77b52d443d29181db64b334c0b3",
            "8366641a841044b1aef8942241d18990",
            "17a3df8269c94fe083c5973146a27ff0",
            "1cfde02998f74bc682e522a3f809ca46",
            "d2cfce8b85b34226a0afe0d96b9e5c36",
            "526e2e334c114a2ba051af1444e0580c",
            "9b2513d690e243cdadee8b48579edfef",
            "07fe7ec4b3a349c9bc9123d14ce2384b",
            "ea9840b452e74ab084d18671bb73291e",
            "f2218bbfbd844332bc77ed8aa0059039",
            "60f93d5f91624226b10abae94e4e925b",
            "21d364e25e70401a8a3d433501a9e6a0",
            "d1f460488f784f9fa7c689ea6034ee8d",
            "55dc82ad2762463096cd9171977e2362",
            "c51b7fb051cf4f4a8f39e1df2530bfb4",
            "e66f927e17f149b3b9158f7ebcd3d2f9",
            "1995915675944791b1267f940e40fd34",
            "d699649b439045229dfe5dafc2a70fdb",
            "a2f3aae9484d48c78d207b05b1f9a998"
          ]
        },
        "id": "y3WH1evWO89q",
        "outputId": "10d01e3d-681d-4575-d955-93d39f127be6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading policy...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b71ed7b654a848e985b834f635650aa8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:lerobot.configs.policies:Device 'mps' is not available. Switching to 'cuda'.\n",
            "WARNING:lerobot.configs.policies:Device 'mps' is not available. Switching to 'cuda'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from: lerobot/pi05_base\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/14.5G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ea9840b452e74ab084d18671bb73291e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Vision embedding key might need handling: paligemma_with_expert.paligemma.model.vision_tower.vision_model.embeddings.patch_embedding.bias\n",
            "WARNING:root:Vision embedding key might need handling: paligemma_with_expert.paligemma.model.vision_tower.vision_model.embeddings.patch_embedding.weight\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Loaded state dict from model.safetensors\n",
            "Remapped: action_in_proj.bias -> model.action_in_proj.bias\n",
            "Remapped: action_in_proj.weight -> model.action_in_proj.weight\n",
            "Remapped: action_out_proj.bias -> model.action_out_proj.bias\n",
            "Remapped: action_out_proj.weight -> model.action_out_proj.weight\n",
            "Remapped: paligemma_with_expert.gemma_expert.lm_head.weight -> model.paligemma_with_expert.gemma_expert.lm_head.weight\n",
            "Remapped: paligemma_with_expert.gemma_expert.model.layers.0.input_layernorm.dense.bias -> model.paligemma_with_expert.gemma_expert.model.layers.0.input_layernorm.dense.bias\n",
            "Remapped: paligemma_with_expert.gemma_expert.model.layers.0.input_layernorm.dense.weight -> model.paligemma_with_expert.gemma_expert.model.layers.0.input_layernorm.dense.weight\n",
            "Remapped: paligemma_with_expert.gemma_expert.model.layers.0.mlp.down_proj.weight -> model.paligemma_with_expert.gemma_expert.model.layers.0.mlp.down_proj.weight\n",
            "Remapped: paligemma_with_expert.gemma_expert.model.layers.0.mlp.gate_proj.weight -> model.paligemma_with_expert.gemma_expert.model.layers.0.mlp.gate_proj.weight\n",
            "Remapped: paligemma_with_expert.gemma_expert.model.layers.0.mlp.up_proj.weight -> model.paligemma_with_expert.gemma_expert.model.layers.0.mlp.up_proj.weight\n",
            "Remapped 812 state dict keys\n",
            "Warning: Could not remap state dict keys: Error(s) in loading state_dict for PI05Policy:\n",
            "\tMissing key(s) in state_dict: \"model.paligemma_with_expert.paligemma.model.language_model.embed_tokens.weight\". \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PI05Model(\n",
              "  (paligemma_with_expert): PaliGemmaWithExpertModel(\n",
              "    (paligemma): PaliGemmaForConditionalGeneration(\n",
              "      (model): PaliGemmaModel(\n",
              "        (vision_tower): SiglipVisionModel(\n",
              "          (vision_model): SiglipVisionTransformer(\n",
              "            (embeddings): SiglipVisionEmbeddings(\n",
              "              (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
              "              (position_embedding): Embedding(256, 1152)\n",
              "            )\n",
              "            (encoder): SiglipEncoder(\n",
              "              (layers): ModuleList(\n",
              "                (0-26): 27 x SiglipEncoderLayer(\n",
              "                  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "                  (self_attn): SiglipAttention(\n",
              "                    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
              "                    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
              "                    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
              "                    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
              "                  )\n",
              "                  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "                  (mlp): SiglipMLP(\n",
              "                    (activation_fn): PytorchGELUTanh()\n",
              "                    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
              "                    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "        (multi_modal_projector): PaliGemmaMultiModalProjector(\n",
              "          (linear): Linear(in_features=1152, out_features=2048, bias=True)\n",
              "        )\n",
              "        (language_model): GemmaModel(\n",
              "          (embed_tokens): Embedding(257152, 2048, padding_idx=0)\n",
              "          (layers): ModuleList(\n",
              "            (0-17): 18 x GemmaDecoderLayer(\n",
              "              (self_attn): GemmaAttention(\n",
              "                (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "                (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
              "                (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
              "                (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "              )\n",
              "              (mlp): GemmaMLP(\n",
              "                (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
              "                (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
              "                (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
              "                (act_fn): PytorchGELUTanh()\n",
              "              )\n",
              "              (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
              "              (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
              "            )\n",
              "          )\n",
              "          (norm): GemmaRMSNorm((2048,), eps=1e-06)\n",
              "          (rotary_emb): GemmaRotaryEmbedding()\n",
              "        )\n",
              "      )\n",
              "      (lm_head): Linear(in_features=2048, out_features=257152, bias=False)\n",
              "    )\n",
              "    (gemma_expert): GemmaForCausalLM(\n",
              "      (model): GemmaModel(\n",
              "        (embed_tokens): None\n",
              "        (layers): ModuleList(\n",
              "          (0-17): 18 x GemmaDecoderLayer(\n",
              "            (self_attn): GemmaAttention(\n",
              "              (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
              "              (k_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
              "              (v_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
              "              (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
              "            )\n",
              "            (mlp): GemmaMLP(\n",
              "              (gate_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "              (up_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "              (down_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (act_fn): PytorchGELUTanh()\n",
              "            )\n",
              "            (input_layernorm): GemmaRMSNorm(\n",
              "              eps=1e-06, adaptive=True, cond_dim=1024\n",
              "              (dense): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            )\n",
              "            (post_attention_layernorm): GemmaRMSNorm(\n",
              "              eps=1e-06, adaptive=True, cond_dim=1024\n",
              "              (dense): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (norm): GemmaRMSNorm(\n",
              "          eps=1e-06, adaptive=True, cond_dim=1024\n",
              "          (dense): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "        )\n",
              "        (rotary_emb): GemmaRotaryEmbedding()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=1024, out_features=257152, bias=False)\n",
              "    )\n",
              "  )\n",
              "  (action_in_proj): Linear(in_features=32, out_features=1024, bias=True)\n",
              "  (action_out_proj): Linear(in_features=1024, out_features=32, bias=True)\n",
              "  (time_mlp_in): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "  (time_mlp_out): Linear(in_features=1024, out_features=1024, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "MODEL_ID = \"lerobot/pi05_base\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "PALIGEMMA_EOS_TOKEN = 1\n",
        "max_decoding_steps = 40\n",
        "temperature = 0.1\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# FIX for AttributeError: 'GemmaRMSNorm' object has no attribute 'weight'\n",
        "# The custom transformers branch uses adaRMS which removes the weight parameter,\n",
        "# but the __repr__ method expects it. We monkey-patch it here.\n",
        "from transformers.models.gemma import modeling_gemma\n",
        "\n",
        "def fixed_extra_repr(self):\n",
        "    if hasattr(self, \"weight\"):\n",
        "        repr_str = f\"{tuple(self.weight.shape)}, eps={self.eps}\"\n",
        "    else:\n",
        "        repr_str = f\"eps={self.eps}\"\n",
        "    if getattr(self, \"dense\", None) is not None:\n",
        "        repr_str += f\", adaptive=True, cond_dim={self.cond_dim}\"\n",
        "    return repr_str\n",
        "\n",
        "modeling_gemma.GemmaRMSNorm.extra_repr = fixed_extra_repr\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "print(\"Loading policy...\")\n",
        "policy = PI05Policy.from_pretrained(MODEL_ID)\n",
        "policy.model.eval()\n",
        "policy.model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "y-rRAwC_PHNP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "img_share_path = '/content'\n",
        "img_name_list = ['top.png']\n",
        "img_list = []\n",
        "\n",
        "if img_share_path and os.path.exists(img_share_path):\n",
        "    # Load images from path\n",
        "    for img_name in img_name_list:\n",
        "        img_path = os.path.join(img_share_path, img_name)\n",
        "        if os.path.exists(img_path):\n",
        "            img = cv2.imread(img_path)\n",
        "            if img is not None:\n",
        "                # Convert BGR to RGB\n",
        "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "                img_list.append(img)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsWSWCu8PkXF",
        "outputId": "411f6470-2a89-4dfc-cf98-a78c0c40ffab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing prompt: 'Put the fruits in the basket\n",
            "Sub task:'\n"
          ]
        }
      ],
      "source": [
        "from lerobot.utils.constants import OBS_LANGUAGE_TOKENS, OBS_LANGUAGE_ATTENTION_MASK\n",
        "\n",
        "batch = {}\n",
        "if len(img_list) > 0:\n",
        "    # Prepare image tensor: [H, W, C] -> [1, H, W, C]\n",
        "    # Normalize to [0, 1] as expected by _preprocess_images (which then scales to [-1, 1])\n",
        "    img_tensor = torch.from_numpy(img_list[0]).float() / 255.0\n",
        "    img_tensor = img_tensor.unsqueeze(0) # Add batch dimension -> [1, H, W, C]\n",
        "\n",
        "    batch[\"observation.images.base_0_rgb\"] = img_tensor\n",
        "\n",
        "# Process Language Prompt\n",
        "# Note: PI05 automatically prepends image embeddings, so we DO NOT need an <image> token in the text.\n",
        "# However, we ensure the prompt ends with a newline for better model performance.\n",
        "prompt = \"Put the fruits in the basket\\nSub task: \"\n",
        "if not prompt.endswith(\"\\n\"):\n",
        "    prompt += \"\\n\"\n",
        "\n",
        "print(f\"Tokenizing prompt: '{prompt.strip()}'\")\n",
        "tokenized = tokenizer(\n",
        "    prompt,\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=512\n",
        ")\n",
        "\n",
        "# Add to batch (converting mask to bool for compatibility)\n",
        "batch[OBS_LANGUAGE_TOKENS] = tokenized[\"input_ids\"]\n",
        "batch[OBS_LANGUAGE_ATTENTION_MASK] = tokenized[\"attention_mask\"].bool()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "tj0a6aXwPp2m"
      },
      "outputs": [],
      "source": [
        "processed_images, img_masks = policy._preprocess_images(batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpG-FpqqQMSe",
        "outputId": "59dd8e99-a155-4eec-af25-473846f35daa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generation Start...\n",
            "\n",
            "Step 0:\n",
            "  Batch 0 Top 5:\n",
            "    1: 'Yes' (p=0.1366)\n",
            "    2: 'No' (p=0.1207)\n",
            "    3: 'pick' (p=0.0421)\n",
            "    4: 'no' (p=0.0367)\n",
            "    5: 'Sub' (p=0.0255)\n",
            "    6: 'place' (p=0.0249)\n",
            "    7: 'yes' (p=0.0162)\n",
            "    8: 'put' (p=0.0132)\n",
            "    9: 'adjust' (p=0.0068)\n",
            "    10: ' to' (p=0.0057)\n",
            "    11: 'hand' (p=0.0053)\n",
            "    12: 'move' (p=0.0050)\n",
            "    13: '  ' (p=0.0048)\n",
            "    14: '<eos>' (p=0.0047)\n",
            "    15: ' in' (p=0.0033)\n",
            "    16: ' the' (p=0.0032)\n",
            "    17: ' out' (p=0.0029)\n",
            "    18: ' on' (p=0.0026)\n",
            "    19: 'return' (p=0.0025)\n",
            "    20: ' ' (p=0.0023)\n",
            "\n",
            "Step 1:\n",
            "  Batch 0 Top 5:\n",
            "    1: '<eos>' (p=0.7305)\n",
            "    2: ' green' (p=0.0690)\n",
            "    3: ' blue' (p=0.0544)\n",
            "    4: '  ' (p=0.0133)\n",
            "    5: ' red' (p=0.0072)\n",
            "    6: ' the' (p=0.0062)\n",
            "    7: ' organizer' (p=0.0051)\n",
            "    8: ' water' (p=0.0048)\n",
            "    9: 'Yes' (p=0.0041)\n",
            "    10: ' orange' (p=0.0039)\n",
            "    11: 'â…¸' (p=0.0032)\n",
            "    12: '\n",
            "' (p=0.0026)\n",
            "    13: ' pink' (p=0.0020)\n",
            "    14: ' can' (p=0.0019)\n",
            "    15: ' basket' (p=0.0018)\n",
            "    16: '\n",
            "\n",
            "' (p=0.0017)\n",
            "    17: ' to' (p=0.0017)\n",
            "    18: '.' (p=0.0016)\n",
            "    19: ' ' (p=0.0014)\n",
            "    20: ' in' (p=0.0013)\n",
            "\n",
            "Output tokens shape: torch.Size([1, 40])\n",
            "Output tokens:\n",
            "tensor([[1294,    1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0]], device='cuda:0')\n",
            "\n",
            "================================================================================\n",
            "Decoded output:\n",
            "================================================================================\n",
            "Batch 0: No<eos>\n",
            "\n",
            "================================================================================\n",
            "Test completed successfully!\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    # FIX: Ensure images are in (B, C, H, W) format\n",
        "    # The error \"expected input... to have 3 channels, but got 224\" occurs because\n",
        "    # the input was (B, H, W, C) but the model expects (B, C, H, W).\n",
        "    fixed_processed_images = []\n",
        "    for img in processed_images:\n",
        "        # Check if shape is (B, H, W, C=3) instead of (B, C=3, H, W)\n",
        "        if img.ndim == 4 and img.shape[-1] == 3 and img.shape[1] != 3:\n",
        "            img = img.permute(0, 3, 1, 2)\n",
        "        fixed_processed_images.append(img)\n",
        "\n",
        "    # Retrieve tokens and masks from the batch dictionary and move them to the correct device\n",
        "    tokens = batch[OBS_LANGUAGE_TOKENS].to(device)\n",
        "    masks = batch[OBS_LANGUAGE_ATTENTION_MASK].to(device)\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Manual Sampling Loop to inspect Top 5 Tokens\n",
        "    # -------------------------------------------------------------------------\n",
        "    import torch.nn.functional as F\n",
        "\n",
        "    # 1. Embed prefix\n",
        "    prefix_embs, prefix_pad_masks, prefix_att_masks = policy.model.embed_prefix(\n",
        "        fixed_processed_images, img_masks, tokens, masks.bool()\n",
        "    )\n",
        "\n",
        "    # 2. Prepare masks\n",
        "    # make_att_2d_masks is in global scope from previous cell execution\n",
        "    prefix_att_2d_masks = make_att_2d_masks(prefix_pad_masks, prefix_att_masks)\n",
        "    prefix_position_ids = torch.cumsum(prefix_pad_masks, dim=1) - 1\n",
        "    prefix_att_2d_masks_4d = policy.model._prepare_attention_masks_4d(prefix_att_2d_masks)\n",
        "\n",
        "    # 3. Initial Forward Pass\n",
        "    lang_model = policy.model.paligemma_with_expert.paligemma.language_model\n",
        "    lang_model.config._attn_implementation = \"eager\"\n",
        "\n",
        "    embeddings, past_key_values = policy.model.paligemma_with_expert.forward(\n",
        "        attention_mask=prefix_att_2d_masks_4d,\n",
        "        position_ids=prefix_position_ids,\n",
        "        past_key_values=None,\n",
        "        inputs_embeds=[prefix_embs, None],\n",
        "        use_cache=True,\n",
        "    )\n",
        "\n",
        "    last_token_embed = embeddings[0][:, -1, :]\n",
        "    last_logits = policy.model.paligemma_with_expert.paligemma.lm_head(last_token_embed)\n",
        "\n",
        "    batch_size = last_logits.shape[0]\n",
        "    device = last_logits.device\n",
        "\n",
        "    prefix_valid_length = torch.sum(prefix_pad_masks, dim=1)\n",
        "    output_tokens = torch.zeros((batch_size, max_decoding_steps), dtype=torch.long, device=device)\n",
        "    all_eos = torch.zeros(batch_size, dtype=torch.bool, device=device)\n",
        "\n",
        "    running_attention_mask = prefix_pad_masks.clone()\n",
        "\n",
        "    print(\"Generation Start...\")\n",
        "\n",
        "    # Autoregressive Loop\n",
        "    for step in range(max_decoding_steps):\n",
        "        # --- Inspect Probabilities ---\n",
        "        probs_all = F.softmax(last_logits, dim=-1)\n",
        "        top_probs, top_indices = torch.topk(probs_all, 20, dim=-1)\n",
        "\n",
        "        print(f\"\\nStep {step}:\")\n",
        "        for b in range(batch_size):\n",
        "            print(f\"  Batch {b} Top 5:\")\n",
        "            for k in range(20):\n",
        "                token_idx = top_indices[b, k].item()\n",
        "                prob = top_probs[b, k].item()\n",
        "                token_str = tokenizer.decode([token_idx])\n",
        "                print(f\"    {k+1}: '{token_str}' (p={prob:.4f})\")\n",
        "        # -----------------------------\n",
        "\n",
        "        # Sample next token\n",
        "        current_temp = 0.3 # Using 0.7 as in the original cell call\n",
        "        if current_temp > 0.0:\n",
        "            probs = F.softmax(last_logits / current_temp, dim=-1)\n",
        "            token = torch.multinomial(probs, num_samples=1)\n",
        "        else:\n",
        "            token = torch.argmax(last_logits, dim=-1, keepdim=True)\n",
        "\n",
        "        output_tokens[:, step] = token.squeeze(-1)\n",
        "\n",
        "        # Check for EOS\n",
        "        all_eos |= (token.squeeze(-1) == PALIGEMMA_EOS_TOKEN)\n",
        "        if all_eos.all():\n",
        "            break\n",
        "\n",
        "        # Feed the new token back in the model\n",
        "        next_token_embeds = lang_model.embed_tokens(token)\n",
        "        position_ids = prefix_valid_length[:, None] + step\n",
        "\n",
        "        new_mask = torch.ones((batch_size, 1), dtype=running_attention_mask.dtype, device=device)\n",
        "        running_attention_mask = torch.cat([running_attention_mask, new_mask], dim=1)\n",
        "\n",
        "        embeds_list, past_key_values = policy.model.paligemma_with_expert.forward(\n",
        "            inputs_embeds=[next_token_embeds, None],\n",
        "            attention_mask=running_attention_mask,\n",
        "            position_ids=position_ids,\n",
        "            past_key_values=past_key_values,\n",
        "            use_cache=True,\n",
        "        )\n",
        "\n",
        "        prefix_output = embeds_list[0]\n",
        "        last_token_embed = prefix_output[:, -1, :]\n",
        "        last_logits = policy.model.paligemma_with_expert.paligemma.lm_head(last_token_embed)\n",
        "\n",
        "    print(f\"\\nOutput tokens shape: {output_tokens.shape}\")\n",
        "    print(f\"Output tokens:\\n{output_tokens}\")\n",
        "\n",
        "    # Decode the generated tokens\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    # The variable high_level_prompt is not defined. I'm removing it.\n",
        "    # print(\"High Level Prompt\\n\",high_level_prompt)\n",
        "    print(\"Decoded output:\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    for batch_idx in range(output_tokens.shape[0]):\n",
        "        batch_tokens = output_tokens[batch_idx]\n",
        "        # Remove padding (zeros) and decode\n",
        "        non_zero_tokens = batch_tokens[batch_tokens != 0]\n",
        "        if len(non_zero_tokens) > 0:\n",
        "            decoded_text = tokenizer.decode(non_zero_tokens, skip_special_tokens=False)\n",
        "            print(f\"Batch {batch_idx}: {decoded_text}\")\n",
        "        else:\n",
        "            print(f\"Batch {batch_idx}: (empty)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Test completed successfully!\")\n",
        "print(\"=\"*80)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b84c30d947024ef6b177a5a74662baba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_b7e098f36bfc40db8d65a63550ecec51"
          }
        },
        "e176c54725f64b699c7f560ef7c46554": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa4460f624334d889394e50e7ec3a941",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_39bc728c95ff43989132b27fee86ddb8",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "afd1802ae3d44acca45b1a5a2e6110c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_03da519921654a5bab87c85543184936",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_7dafafe80eda470c9aaa659d857d33ac",
            "value": ""
          }
        },
        "606fedc33f4f413ca824ad9c7ba84a34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_a7a19f850262426cac457d755ce3a4a8",
            "style": "IPY_MODEL_7a250fedde8447d2a6f8f44c7f21b092",
            "value": true
          }
        },
        "89f9756b355341f884e7b2d6d18b6e0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_f5d2f49c0de64ed68e8f0cbd1a5ee7e9",
            "style": "IPY_MODEL_c89e5230bb374acf9e1d4de38b49f353",
            "tooltip": ""
          }
        },
        "9805e7b102f44479a60dfe578a1b84cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_726b44720258460c9a8f25fdcfd6b744",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b566a841e6954c89acf6d0a0889fece0",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "b7e098f36bfc40db8d65a63550ecec51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "fa4460f624334d889394e50e7ec3a941": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39bc728c95ff43989132b27fee86ddb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "03da519921654a5bab87c85543184936": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7dafafe80eda470c9aaa659d857d33ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a7a19f850262426cac457d755ce3a4a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a250fedde8447d2a6f8f44c7f21b092": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f5d2f49c0de64ed68e8f0cbd1a5ee7e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c89e5230bb374acf9e1d4de38b49f353": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "726b44720258460c9a8f25fdcfd6b744": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b566a841e6954c89acf6d0a0889fece0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7e3a72644a3b4658b91dbe5aa0b10b24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_739ad6f705fb4596ac3466f3533eba68",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_7041584c531849adb3d526e1bb1ef679",
            "value": "Connecting..."
          }
        },
        "739ad6f705fb4596ac3466f3533eba68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7041584c531849adb3d526e1bb1ef679": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55b76b19e6d643f2863d49f8d17770fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_92fe0c4f171e4277bc92e2f76bd7b8ca",
              "IPY_MODEL_354b05db6c2742569690177416deedf2",
              "IPY_MODEL_21ba6cc5d39a4acf9d0c13f27ede9153"
            ],
            "layout": "IPY_MODEL_41bbe56b6fee46e2aee47b486365ea10"
          }
        },
        "92fe0c4f171e4277bc92e2f76bd7b8ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2eda561de0b4e19a7efba6a74876f7c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5aa1aa3d32b140d3bef1483cc74411d7",
            "value": "tokenizer_config.json:â€‡100%"
          }
        },
        "354b05db6c2742569690177416deedf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0b0a98f00e34039809a08ea17258840",
            "max": 39968,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3f188e0e975144598c7ea8cdc21a743c",
            "value": 39968
          }
        },
        "21ba6cc5d39a4acf9d0c13f27ede9153": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43ee1b1856534ea1859dd6a18ce8af3f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_32c721722edf4ecc986d6696076698a3",
            "value": "â€‡40.0k/40.0kâ€‡[00:00&lt;00:00,â€‡4.94MB/s]"
          }
        },
        "41bbe56b6fee46e2aee47b486365ea10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2eda561de0b4e19a7efba6a74876f7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5aa1aa3d32b140d3bef1483cc74411d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f0b0a98f00e34039809a08ea17258840": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f188e0e975144598c7ea8cdc21a743c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "43ee1b1856534ea1859dd6a18ce8af3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32c721722edf4ecc986d6696076698a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "46b46c92f71d44b0b95c1ae8f3e966bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b3afb172f8eb444f812320ab845027b5",
              "IPY_MODEL_b94200e0d7464d25b1efad9b15779066",
              "IPY_MODEL_1ea5697015a148a9b9221a405ddaf349"
            ],
            "layout": "IPY_MODEL_9d3d53b86c3d4064892a9762620c10e1"
          }
        },
        "b3afb172f8eb444f812320ab845027b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9fc1964f5634f82950605f286437ab6",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ba1bff96e27941be967a66e11080a617",
            "value": "tokenizer.model:â€‡100%"
          }
        },
        "b94200e0d7464d25b1efad9b15779066": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bee1037741574150ae70b0d9abd99472",
            "max": 4264023,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2c4f3fb3fd3e4e6a99d3f4eb68bd06bb",
            "value": 4264023
          }
        },
        "1ea5697015a148a9b9221a405ddaf349": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec0ea6ba423f434ba4bea34049ffdb6e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_dff9dedbe4eb4495af8c738d3aba9bf4",
            "value": "â€‡4.26M/4.26Mâ€‡[00:01&lt;00:00,â€‡3.92MB/s]"
          }
        },
        "9d3d53b86c3d4064892a9762620c10e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9fc1964f5634f82950605f286437ab6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba1bff96e27941be967a66e11080a617": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bee1037741574150ae70b0d9abd99472": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c4f3fb3fd3e4e6a99d3f4eb68bd06bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ec0ea6ba423f434ba4bea34049ffdb6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dff9dedbe4eb4495af8c738d3aba9bf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c1234854234944debfbff33bf19cf144": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0dd2b95b66d34bff91f0b188da1e8162",
              "IPY_MODEL_eb32da5921974aaaad0e2e61d18b3906",
              "IPY_MODEL_78d30d850e264f388663d7b53020ca27"
            ],
            "layout": "IPY_MODEL_cf3d25ff5dca411aaece66b24c80b542"
          }
        },
        "0dd2b95b66d34bff91f0b188da1e8162": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c0e368f3b06487d960ceda435762995",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_acb2a96b1cb04efb8191a40c5d6d1ec0",
            "value": "added_tokens.json:â€‡100%"
          }
        },
        "eb32da5921974aaaad0e2e61d18b3906": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34079fea82654bcc99f8b5f61961d642",
            "max": 24,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cf76f0e71eb742d18e3b925051ec6a9f",
            "value": 24
          }
        },
        "78d30d850e264f388663d7b53020ca27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_652a3773f83b486bac473b772678b303",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_4475ced03e674752a15b74b22675952b",
            "value": "â€‡24.0/24.0â€‡[00:00&lt;00:00,â€‡3.26kB/s]"
          }
        },
        "cf3d25ff5dca411aaece66b24c80b542": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c0e368f3b06487d960ceda435762995": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "acb2a96b1cb04efb8191a40c5d6d1ec0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "34079fea82654bcc99f8b5f61961d642": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf76f0e71eb742d18e3b925051ec6a9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "652a3773f83b486bac473b772678b303": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4475ced03e674752a15b74b22675952b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "562fbd46ca71495bbdb31485657ae0f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_71ceb5cea51440bd98c3dd2ffe396729",
              "IPY_MODEL_814709f8d5934b30994720d17aeabbd2",
              "IPY_MODEL_30e52ea0eaa64b31b4f097d7416b3f1d"
            ],
            "layout": "IPY_MODEL_642ed517c82c41d49b08a8f03f037fe1"
          }
        },
        "71ceb5cea51440bd98c3dd2ffe396729": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83e32c8c349f431d8ed3e623727d5baf",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_9c90b087c5164494983f7ddd091992e5",
            "value": "special_tokens_map.json:â€‡100%"
          }
        },
        "814709f8d5934b30994720d17aeabbd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8942cce372d4a15bd825c61b8c6b315",
            "max": 607,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0f6d37333dae4daf9bd168ee9fbb0bb4",
            "value": 607
          }
        },
        "30e52ea0eaa64b31b4f097d7416b3f1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ca81ad055bc4c8d9464493de77de4fd",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_aa6b17fcedce4ae1a8983f866fc1fa54",
            "value": "â€‡607/607â€‡[00:00&lt;00:00,â€‡70.2kB/s]"
          }
        },
        "642ed517c82c41d49b08a8f03f037fe1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83e32c8c349f431d8ed3e623727d5baf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c90b087c5164494983f7ddd091992e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d8942cce372d4a15bd825c61b8c6b315": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f6d37333dae4daf9bd168ee9fbb0bb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4ca81ad055bc4c8d9464493de77de4fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa6b17fcedce4ae1a8983f866fc1fa54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d202b67557024a71980a5fbac84c9ec7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b09458032fbd4cca9ec701dfdfa3b774",
              "IPY_MODEL_5b333a7d686c4dadb48e6183e3c567cb",
              "IPY_MODEL_bd8bee9872ba46ccb3fbf0f40f59fca3"
            ],
            "layout": "IPY_MODEL_90b115415a78498db76198fd5a963b59"
          }
        },
        "b09458032fbd4cca9ec701dfdfa3b774": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8becd8f994434975a596819814b00d26",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_8bb4561af6354bd7a79c37c9b77137e1",
            "value": "tokenizer.json:â€‡100%"
          }
        },
        "5b333a7d686c4dadb48e6183e3c567cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e62feeeced54508a8ac991104b6fd8b",
            "max": 17549604,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3f8eb292d9db4458857e18813964e61e",
            "value": 17549604
          }
        },
        "bd8bee9872ba46ccb3fbf0f40f59fca3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04d2f1b0da6c488ea2cce221e84a54af",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_7cc817872ea84a5e8c777d59aeff258b",
            "value": "â€‡17.5M/17.5Mâ€‡[00:00&lt;00:00,â€‡33.3MB/s]"
          }
        },
        "90b115415a78498db76198fd5a963b59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8becd8f994434975a596819814b00d26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8bb4561af6354bd7a79c37c9b77137e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e62feeeced54508a8ac991104b6fd8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f8eb292d9db4458857e18813964e61e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "04d2f1b0da6c488ea2cce221e84a54af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7cc817872ea84a5e8c777d59aeff258b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b71ed7b654a848e985b834f635650aa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_36bbb309d8e14bc9ab012d86420026f9",
              "IPY_MODEL_955feb894c4c41269506820f6e799849",
              "IPY_MODEL_a635d77b52d443d29181db64b334c0b3"
            ],
            "layout": "IPY_MODEL_8366641a841044b1aef8942241d18990"
          }
        },
        "36bbb309d8e14bc9ab012d86420026f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17a3df8269c94fe083c5973146a27ff0",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_1cfde02998f74bc682e522a3f809ca46",
            "value": "config.json:â€‡"
          }
        },
        "955feb894c4c41269506820f6e799849": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2cfce8b85b34226a0afe0d96b9e5c36",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_526e2e334c114a2ba051af1444e0580c",
            "value": 1
          }
        },
        "a635d77b52d443d29181db64b334c0b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b2513d690e243cdadee8b48579edfef",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_07fe7ec4b3a349c9bc9123d14ce2384b",
            "value": "â€‡1.90k/?â€‡[00:00&lt;00:00,â€‡200kB/s]"
          }
        },
        "8366641a841044b1aef8942241d18990": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17a3df8269c94fe083c5973146a27ff0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1cfde02998f74bc682e522a3f809ca46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d2cfce8b85b34226a0afe0d96b9e5c36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "526e2e334c114a2ba051af1444e0580c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9b2513d690e243cdadee8b48579edfef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07fe7ec4b3a349c9bc9123d14ce2384b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea9840b452e74ab084d18671bb73291e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f2218bbfbd844332bc77ed8aa0059039",
              "IPY_MODEL_60f93d5f91624226b10abae94e4e925b",
              "IPY_MODEL_21d364e25e70401a8a3d433501a9e6a0"
            ],
            "layout": "IPY_MODEL_d1f460488f784f9fa7c689ea6034ee8d"
          }
        },
        "f2218bbfbd844332bc77ed8aa0059039": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55dc82ad2762463096cd9171977e2362",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c51b7fb051cf4f4a8f39e1df2530bfb4",
            "value": "model.safetensors:â€‡100%"
          }
        },
        "60f93d5f91624226b10abae94e4e925b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e66f927e17f149b3b9158f7ebcd3d2f9",
            "max": 14467165872,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1995915675944791b1267f940e40fd34",
            "value": 14467165872
          }
        },
        "21d364e25e70401a8a3d433501a9e6a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d699649b439045229dfe5dafc2a70fdb",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a2f3aae9484d48c78d207b05b1f9a998",
            "value": "â€‡14.5G/14.5Gâ€‡[00:40&lt;00:00,â€‡513MB/s]"
          }
        },
        "d1f460488f784f9fa7c689ea6034ee8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55dc82ad2762463096cd9171977e2362": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c51b7fb051cf4f4a8f39e1df2530bfb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e66f927e17f149b3b9158f7ebcd3d2f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1995915675944791b1267f940e40fd34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d699649b439045229dfe5dafc2a70fdb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2f3aae9484d48c78d207b05b1f9a998": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}