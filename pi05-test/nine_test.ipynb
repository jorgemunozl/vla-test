{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nine Test\n",
    "\n",
    "This breaks the continuity that we have been following. But it's important because for debug properly we need a proper batch. The eight test continues being the important one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd XHUMAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install -e .[pi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from xhuman.configs.default import LerobotDatasetConfig\n",
    "\n",
    "dataset_config = LerobotDatasetConfig(\n",
    "    repo_id=\"NONHUMAN_RESEARCH/TEST-RECORD-ANNOTATIONS\",  \n",
    ")\n",
    "\n",
    "device = torch.device(policy_config.device if policy_config.device else \"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xhuman.configs.train import TrainPipelineConfigXHUMAN\n",
    "\n",
    "cfg = TrainPipelineConfigXHUMAN(\n",
    "    dataset=dataset_config,\n",
    "    policy=policy_config,\n",
    "    batch_size=50,\n",
    "    num_workers=4,\n",
    "    split_ratio=0.8,\n",
    ")\n",
    "\n",
    "cfg.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xhuman.policies.pi05.configuration_pi05 import PI05Config\n",
    "\n",
    "policy_config = PI05Config(\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from accelerate.utils import DistributedDataParallelKwargs\n",
    "\n",
    "ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)\n",
    "accelerator = Accelerator(step_scheduler_with_optimizer=False, kwargs_handlers=[ddp_kwargs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = make_dataset_xhuman(cfg)\n",
    "episodes = list(range(dataset.meta.total_episodes))\n",
    "train_episodes, _ = split_train_eval_episodes(episodes, split_ratio=cfg.split_ratio, seed=42)\n",
    "if 30 in train_episodes:\n",
    "    train_episodes.remove(30)\n",
    "\n",
    "# Eliminar dataset para liberar memoria (igual que en train_val_pi05.py)\n",
    "del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xhuman.policies.factory import make_xhuman_policy\n",
    "\n",
    "policy = make_xhuman_policy(cfg=cfg.policy, ds_meta=dataset.meta)\n",
    "\n",
    "processor_kwargs = {\"dataset_stats\": dataset.meta.stats}\n",
    "if cfg.policy.pretrained_path is not None:\n",
    "    processor_kwargs[\"preprocessor_overrides\"] = {\n",
    "        \"device_processor\": {\"device\": device.type},\n",
    "        \"normalizer_processor\": {\n",
    "            \"stats\": dataset.meta.stats,\n",
    "            \"features\": {**policy.config.input_features, **policy.config.output_features},\n",
    "            \"norm_map\": policy.config.normalization_mapping,\n",
    "        },\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xhuman.policies.pi05.processor_pi05 import (\n",
    "    make_pi05_pre_post_processors_ki,\n",
    ")\n",
    "\n",
    "preprocessor, _ = make_pi05_pre_post_processors_ki(\n",
    "    policy_cfg=cfg.policy,\n",
    "    pretrained_path=cfg.policy.pretrained_path,\n",
    "    **processor_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xhuman.datasets.factory import make_dataset_xhuman\n",
    "\n",
    "cfg.dataset.episodes = train_episodes\n",
    "dataset = make_dataset_xhuman(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lerobot.datasets.sampler import EpisodeAwareSampler\n",
    "\n",
    "# Crear dataloader\n",
    "if hasattr(cfg.policy, \"drop_n_last_frames\"):\n",
    "    sampler = EpisodeAwareSampler(\n",
    "        dataset.meta.episodes[\"dataset_from_index\"],\n",
    "        dataset.meta.episodes[\"dataset_to_index\"],\n",
    "        drop_n_last_frames=cfg.policy.drop_n_last_frames,\n",
    "        shuffle=True,\n",
    "    )\n",
    "else:\n",
    "    sampler = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    num_workers=cfg.num_workers,\n",
    "    batch_size=cfg.batch_size,\n",
    "    shuffle=(sampler is None) and not cfg.dataset.streaming,\n",
    "    sampler=sampler,\n",
    "    pin_memory=device.type == \"cuda\",\n",
    "    drop_last=False,\n",
    "    prefetch_factor=2 if cfg.num_workers > 0 else None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = accelerator.prepare(dataloader)\n",
    "dl_iter = cycle(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's inspect the raw batch BEFORE preprocessing to see the original structure\n",
    "print(\"=\" * 80)\n",
    "print(\"RAW BATCH (before preprocessor):\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get a fresh batch before preprocessing\n",
    "raw_batch = next(dl_iter)\n",
    "print(f\"\\nRaw batch keys: {list(raw_batch.keys())}\\n\")\n",
    "\n",
    "for key, value in raw_batch.items():\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        print(f\"  {key}: shape={value.shape}, dtype={value.dtype}\")\n",
    "    elif isinstance(value, (list, tuple)):\n",
    "        print(f\"  {key}: {type(value).__name__} of length {len(value)}\")\n",
    "        if len(value) > 0 and isinstance(value[0], torch.Tensor):\n",
    "            print(f\"    First element shape: {value[0].shape}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {type(value).__name__} = {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPARISON:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Raw batch has the original dataset structure\")\n",
    "print(\"Processed batch (after preprocessor) has the structure expected by the policy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding `cfg.steps` vs Epochs\n",
    "\n",
    "In `train_val_pi05.py`, the training loop uses:\n",
    "\n",
    "```python\n",
    "for _ in range(step, cfg.steps):\n",
    "    batch = next(dl_iter)  # Get next batch from infinite iterator\n",
    "    batch = preprocessor(batch)\n",
    "    # ... training step ...\n",
    "    step += 1\n",
    "```\n",
    "\n",
    "**Key points:**\n",
    "\n",
    "1. **`cfg.steps`** = Total number of training steps (gradient updates), NOT epochs\n",
    "2. **`cycle(dataloader)`** creates an infinite iterator that automatically restarts when exhausted\n",
    "3. This approach is common in RL/robotics because:\n",
    "   - You care more about number of gradient updates than epochs\n",
    "   - Datasets can be very large, so epochs might take too long\n",
    "   - You can easily control training duration with a step count\n",
    "\n",
    "**Example calculation:**\n",
    "- Dataset: 10,000 samples\n",
    "- Batch size: 50\n",
    "- Batches per epoch: 10,000 / 50 = 200 batches\n",
    "- If `cfg.steps = 1000`:\n",
    "  - We do 1000 gradient updates\n",
    "  - This equals 1000 / 200 = 5 epochs worth of data\n",
    "  - But we don't explicitly count epochs, just steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's inspect the raw batch BEFORE preprocessing to see the original structure\n",
    "print(\"=\" * 80)\n",
    "print(\"RAW BATCH (before preprocessor):\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get a fresh batch before preprocessing\n",
    "raw_batch = next(dl_iter)\n",
    "print(f\"\\nRaw batch keys: {list(raw_batch.keys())}\\n\")\n",
    "\n",
    "for key, value in raw_batch.items():\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        print(f\"  {key}: shape={value.shape}, dtype={value.dtype}\")\n",
    "    elif isinstance(value, (list, tuple)):\n",
    "        print(f\"  {key}: {type(value).__name__} of length {len(value)}\")\n",
    "        if len(value) > 0 and isinstance(value[0], torch.Tensor):\n",
    "            print(f\"    First element shape: {value[0].shape}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {type(value).__name__} = {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPARISON:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Raw batch has the original dataset structure\")\n",
    "print(\"Processed batch (after preprocessor) has the structure expected by the policy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener batch procesado (igual que next(dl_iter) en train_val_pi05.py)\n",
    "batch = next(dl_iter)\n",
    "batch = preprocessor(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El batch est√° listo para usar\n",
    "print(\"Batch keys:\", list(batch.keys()))\n",
    "for key, value in batch.items():\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        print(f\"  {key}: shape={value.shape}, dtype={value.dtype}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
