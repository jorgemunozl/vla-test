{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Core Training Notebook\n",
        "\n",
        "This notebook contains only the essential parts for training a policy:\n",
        "- Dataset loading\n",
        "- Policy updates\n",
        "- Core training loop\n",
        "\n",
        "Excludes: evaluation, WandB logging, checkpoint strategies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from contextlib import nullcontext\n",
        "from typing import Any\n",
        "\n",
        "import torch\n",
        "from accelerate import Accelerator\n",
        "from accelerate.utils import DistributedDataParallelKwargs\n",
        "from torch.optim import Optimizer\n",
        "\n",
        "from lerobot.configs import parser\n",
        "from lerobot.datasets.sampler import EpisodeAwareSampler\n",
        "from lerobot.datasets.utils import cycle\n",
        "from lerobot.optim.factory import make_optimizer_and_scheduler\n",
        "from lerobot.policies.pretrained import PreTrainedPolicy\n",
        "from lerobot.utils.logging_utils import AverageMeter, MetricsTracker\n",
        "from lerobot.utils.random_utils import set_seed\n",
        "from lerobot.utils.train_utils import load_training_state\n",
        "from lerobot.utils.utils import (\n",
        "    format_big_number,\n",
        "    has_method,\n",
        "    init_logging,\n",
        ")\n",
        "\n",
        "from xhuman.policies.factory import make_xhuman_policy, make_xhuman_pre_post_processors\n",
        "from xhuman.configs.train import TrainPipelineConfigXHUMAN\n",
        "from xhuman.datasets.factory import make_dataset_xhuman\n",
        "from xhuman.datasets.utils import split_train_eval_episodes\n",
        "from xhuman.logger import logger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_dataset(cfg: TrainPipelineConfigXHUMAN, episodes: list[int], is_main_process: bool = True, accelerator: Accelerator | None = None):\n",
        "    \"\"\"\n",
        "    Load the dataset for training and evaluation.\n",
        "    \"\"\"\n",
        "    # Dataset loading synchronization: main process downloads first to avoid race conditions\n",
        "    cfg.dataset.episodes = episodes\n",
        "\n",
        "    if is_main_process:\n",
        "        logger.info(\"Creating dataset\")\n",
        "        dataset = make_dataset_xhuman(cfg)\n",
        "\n",
        "    accelerator.wait_for_everyone()\n",
        "\n",
        "    # Now all other processes can safely load the dataset\n",
        "    if not is_main_process:\n",
        "        dataset = make_dataset_xhuman(cfg)\n",
        "    \n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def update_policy(\n",
        "    train_metrics: MetricsTracker,\n",
        "    policy: PreTrainedPolicy,\n",
        "    batch: Any,\n",
        "    optimizer: Optimizer,\n",
        "    grad_clip_norm: float,\n",
        "    accelerator: Accelerator,\n",
        "    lr_scheduler=None,\n",
        "    lock=None,\n",
        ") -> tuple[MetricsTracker, dict]:\n",
        "    \"\"\"\n",
        "    Performs a single training step to update the policy's weights.\n",
        "\n",
        "    This function executes the forward and backward passes, clips gradients, and steps the optimizer and\n",
        "    learning rate scheduler. Accelerator handles mixed-precision training automatically.\n",
        "\n",
        "    Args:\n",
        "        train_metrics: A MetricsTracker instance to record training statistics.\n",
        "        policy: The policy model to be trained.\n",
        "        batch: A batch of training data.\n",
        "        optimizer: The optimizer used to update the policy's parameters.\n",
        "        grad_clip_norm: The maximum norm for gradient clipping.\n",
        "        accelerator: The Accelerator instance for distributed training and mixed precision.\n",
        "        lr_scheduler: An optional learning rate scheduler.\n",
        "        lock: An optional lock for thread-safe optimizer updates.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - The updated MetricsTracker with new statistics for this step.\n",
        "        - A dictionary of outputs from the policy's forward pass, for logging purposes.\n",
        "    \"\"\"\n",
        "    start_time = time.perf_counter()\n",
        "    policy.train()\n",
        "\n",
        "    # Let accelerator handle mixed precision\n",
        "    with accelerator.autocast():\n",
        "        loss, output_dict = policy.forward(batch)\n",
        "\n",
        "    # Use accelerator's backward method\n",
        "    accelerator.backward(loss)\n",
        "\n",
        "    # Clip gradients if specified\n",
        "    if grad_clip_norm > 0:\n",
        "        grad_norm = accelerator.clip_grad_norm_(policy.parameters(), grad_clip_norm)\n",
        "    else:\n",
        "        grad_norm = torch.nn.utils.clip_grad_norm_(\n",
        "            policy.parameters(), float(\"inf\"), error_if_nonfinite=False\n",
        "        )\n",
        "\n",
        "    # Optimizer step\n",
        "    with lock if lock is not None else nullcontext():\n",
        "        optimizer.step()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Step through pytorch scheduler at every batch instead of epoch\n",
        "    if lr_scheduler is not None:\n",
        "        lr_scheduler.step()\n",
        "\n",
        "    # Update internal buffers if policy has update method\n",
        "    if has_method(accelerator.unwrap_model(policy, keep_fp32_wrapper=True), \"update\"):\n",
        "        accelerator.unwrap_model(policy, keep_fp32_wrapper=True).update()\n",
        "\n",
        "    train_metrics.loss = loss.item()\n",
        "    train_metrics.grad_norm = grad_norm.item()\n",
        "    train_metrics.lr = optimizer.param_groups[0][\"lr\"]\n",
        "    train_metrics.update_s = time.perf_counter() - start_time\n",
        "    return train_metrics, output_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load configuration (you can modify this to load from a config file or set directly)\n",
        "# Example: cfg = TrainPipelineConfigXHUMAN.from_dict({...})\n",
        "# For now, we'll use the parser to load from command line or config file\n",
        "# In notebook, you might want to set cfg directly\n",
        "\n",
        "# Uncomment and modify as needed:\n",
        "# @parser.wrap()\n",
        "# def get_config():\n",
        "#     return TrainPipelineConfigXHUMAN()\n",
        "\n",
        "# cfg = get_config()\n",
        "# cfg.validate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Accelerator\n",
        "# It will automatically detect if running in distributed mode or single-process mode\n",
        "# We set step_scheduler_with_optimizer=False to prevent accelerate from adjusting the lr_scheduler steps based on the num_processes\n",
        "# We set find_unused_parameters=True to handle models with conditional computation\n",
        "ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)\n",
        "accelerator = Accelerator(step_scheduler_with_optimizer=False, kwargs_handlers=[ddp_kwargs])\n",
        "\n",
        "init_logging(accelerator=accelerator)\n",
        "\n",
        "# Determine if this is the main process (for logging and checkpointing)\n",
        "is_main_process = accelerator.is_main_process\n",
        "\n",
        "# Set seed if specified\n",
        "if cfg.seed is not None:\n",
        "    set_seed(cfg.seed, accelerator=accelerator)\n",
        "\n",
        "# Use accelerator's device\n",
        "device = accelerator.device\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cuda.matmul.allow_tf32 = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "if is_main_process:\n",
        "    logger.info(\"Creating dataset\")\n",
        "    dataset = make_dataset_xhuman(cfg)\n",
        "\n",
        "accelerator.wait_for_everyone()\n",
        "\n",
        "if not is_main_process:\n",
        "    dataset = make_dataset_xhuman(cfg)\n",
        "\n",
        "# Split episodes\n",
        "episodes = list(range(dataset.meta.total_episodes))\n",
        "# Modify episode selection as needed\n",
        "# episodes = episodes[1700:]\n",
        "train_episodes, eval_episodes = split_train_eval_episodes(episodes, split_ratio=cfg.split_ratio, seed=42)\n",
        "\n",
        "del dataset\n",
        "\n",
        "# Load train dataset\n",
        "if is_main_process:\n",
        "    logger.info(f\"Loading train dataset with {len(train_episodes)} episodes\")\n",
        "dataset = load_dataset(cfg, train_episodes, is_main_process, accelerator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create policy\n",
        "if is_main_process:\n",
        "    logger.info(\"Creating policy\")\n",
        "policy = make_xhuman_policy(\n",
        "    cfg=cfg.policy,\n",
        "    ds_meta=dataset.meta,\n",
        ")\n",
        "\n",
        "accelerator.wait_for_everyone()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset and Model Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display dataset metadata and model configuration\n",
        "if is_main_process:\n",
        "    from pprint import pprint\n",
        "    \n",
        "    print(\"=\" * 80)\n",
        "    print(\"DATASET METADATA\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"\\nDataset Repository: {dataset.repo_id}\")\n",
        "    print(f\"Total Episodes: {dataset.meta.total_episodes}\")\n",
        "    print(f\"Training Episodes: {len(train_episodes)}\")\n",
        "    print(f\"Number of Frames: {dataset.num_frames:,}\")\n",
        "    print(f\"Number of Episodes (loaded): {dataset.num_episodes}\")\n",
        "    \n",
        "    print(f\"\\nFeatures:\")\n",
        "    for key, feature in dataset.meta.features.items():\n",
        "        print(f\"  - {key}: {feature.type.name} (shape: {feature.shape})\")\n",
        "    \n",
        "    print(f\"\\nCamera/Video Keys:\")\n",
        "    if hasattr(dataset.meta, 'camera_keys') and dataset.meta.camera_keys:\n",
        "        for key in dataset.meta.camera_keys:\n",
        "            print(f\"  - {key}\")\n",
        "    elif hasattr(dataset.meta, 'video_keys') and dataset.meta.video_keys:\n",
        "        for key in dataset.meta.video_keys:\n",
        "            print(f\"  - {key}\")\n",
        "    \n",
        "    print(f\"\\nDataset Statistics (normalization):\")\n",
        "    for key, stats in dataset.meta.stats.items():\n",
        "        if isinstance(stats, dict):\n",
        "            print(f\"  {key}:\")\n",
        "            for stat_name, stat_val in stats.items():\n",
        "                if hasattr(stat_val, 'shape'):\n",
        "                    if stat_val.numel() > 0:\n",
        "                        print(f\"    {stat_name}: shape={stat_val.shape}, \"\n",
        "                              f\"min={stat_val.min().item():.4f}, max={stat_val.max().item():.4f}, \"\n",
        "                              f\"mean={stat_val.mean().item():.4f}\")\n",
        "                elif isinstance(stat_val, (int, float)):\n",
        "                    print(f\"    {stat_name}: {stat_val}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"MODEL CONFIGURATION\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"\\nPolicy Type: {policy.config.type}\")\n",
        "    print(f\"Policy Class: {policy.__class__.__name__}\")\n",
        "    \n",
        "    print(f\"\\nInput Features:\")\n",
        "    for key, feature in policy.config.input_features.items():\n",
        "        print(f\"  - {key}: {feature.type.name} (shape: {feature.shape})\")\n",
        "    \n",
        "    print(f\"\\nOutput Features:\")\n",
        "    for key, feature in policy.config.output_features.items():\n",
        "        print(f\"  - {key}: {feature.type.name} (shape: {feature.shape})\")\n",
        "    \n",
        "    if hasattr(policy.config, 'normalization_mapping') and policy.config.normalization_mapping:\n",
        "        print(f\"\\nNormalization Mapping:\")\n",
        "        for key, value in policy.config.normalization_mapping.items():\n",
        "            print(f\"  {key} -> {value}\")\n",
        "    \n",
        "    # Display model-specific configuration\n",
        "    print(f\"\\nModel-Specific Configuration:\")\n",
        "    config_dict = policy.config.to_dict()\n",
        "    # Filter out common fields to show only model-specific ones\n",
        "    common_fields = {'type', 'device', 'input_features', 'output_features', 'normalization_mapping'}\n",
        "    model_specific = {k: v for k, v in config_dict.items() if k not in common_fields}\n",
        "    if model_specific:\n",
        "        pprint(model_specific, width=80, indent=2)\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create processors\n",
        "processor_kwargs = {}\n",
        "postprocessor_kwargs = {}\n",
        "if (cfg.policy.pretrained_path and not cfg.resume) or not cfg.policy.pretrained_path:\n",
        "    processor_kwargs[\"dataset_stats\"] = dataset.meta.stats\n",
        "\n",
        "if cfg.policy.pretrained_path is not None:\n",
        "    processor_kwargs[\"preprocessor_overrides\"] = {\n",
        "        \"device_processor\": {\"device\": device.type},\n",
        "        \"normalizer_processor\": {\n",
        "            \"stats\": dataset.meta.stats,\n",
        "            \"features\": {**policy.config.input_features, **policy.config.output_features},\n",
        "            \"norm_map\": policy.config.normalization_mapping,\n",
        "        },\n",
        "    }\n",
        "    postprocessor_kwargs[\"postprocessor_overrides\"] = {\n",
        "        \"unnormalizer_processor\": {\n",
        "            \"stats\": dataset.meta.stats,\n",
        "            \"features\": policy.config.output_features,\n",
        "            \"norm_map\": policy.config.normalization_mapping,\n",
        "        },\n",
        "    }\n",
        "\n",
        "preprocessor, postprocessor = make_xhuman_pre_post_processors(\n",
        "    policy_cfg=cfg.policy,\n",
        "    pretrained_path=cfg.policy.pretrained_path,\n",
        "    **processor_kwargs,\n",
        "    **postprocessor_kwargs,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create optimizer and scheduler\n",
        "if is_main_process:\n",
        "    logger.info(\"Creating optimizer and scheduler\")\n",
        "optimizer, lr_scheduler = make_optimizer_and_scheduler(cfg, policy)\n",
        "\n",
        "step = 0  # number of policy updates\n",
        "\n",
        "# Resume from checkpoint if needed\n",
        "if cfg.resume:\n",
        "    step, optimizer, lr_scheduler = load_training_state(cfg.checkpoint_path, optimizer, lr_scheduler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print training info\n",
        "if is_main_process:\n",
        "    num_learnable_params = sum(p.numel() for p in policy.parameters() if p.requires_grad)\n",
        "    num_total_params = sum(p.numel() for p in policy.parameters())\n",
        "    logger.info(f\"Output dir: {cfg.output_dir}\")\n",
        "    logger.info(f\"Steps: {cfg.steps} ({format_big_number(cfg.steps)})\")\n",
        "    logger.info(f\"Dataset frames: {dataset.num_frames} ({format_big_number(dataset.num_frames)})\")\n",
        "    logger.info(f\"Dataset episodes: {dataset.num_episodes}\")\n",
        "    num_processes = accelerator.num_processes\n",
        "    effective_bs = cfg.batch_size * num_processes\n",
        "    logger.info(f\"Effective batch size: {cfg.batch_size} x {num_processes} = {effective_bs}\")\n",
        "    logger.info(f\"Learnable params: {num_learnable_params} ({format_big_number(num_learnable_params)})\")\n",
        "    logger.info(f\"Total params: {num_total_params} ({format_big_number(num_total_params)})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create dataloader\n",
        "if hasattr(cfg.policy, \"drop_n_last_frames\"):\n",
        "    logger.info(f\"Dropping {cfg.policy.drop_n_last_frames} last frames\")\n",
        "    shuffle = False\n",
        "    sampler = EpisodeAwareSampler(\n",
        "        dataset.meta.episodes[\"dataset_from_index\"],\n",
        "        dataset.meta.episodes[\"dataset_to_index\"],\n",
        "        drop_n_last_frames=cfg.policy.drop_n_last_frames,\n",
        "        shuffle=True,\n",
        "    )\n",
        "else:\n",
        "    logger.info(\"Not dropping any frames\")\n",
        "    shuffle = True\n",
        "    sampler = None\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    num_workers=cfg.num_workers,\n",
        "    batch_size=cfg.batch_size,\n",
        "    shuffle=shuffle and not cfg.dataset.streaming,\n",
        "    sampler=sampler,\n",
        "    pin_memory=device.type == \"cuda\",\n",
        "    drop_last=False,\n",
        "    prefetch_factor=2 if cfg.num_workers > 0 else None,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare everything with accelerator\n",
        "accelerator.wait_for_everyone()\n",
        "policy, optimizer, dataloader, lr_scheduler = accelerator.prepare(\n",
        "    policy, optimizer, dataloader, lr_scheduler\n",
        ")\n",
        "dl_iter = cycle(dataloader)\n",
        "\n",
        "policy.train()\n",
        "\n",
        "# Setup metrics tracking\n",
        "train_metrics = {\n",
        "    \"loss\": AverageMeter(\"loss\", \":.3f\"),\n",
        "    \"grad_norm\": AverageMeter(\"grdn\", \":.3f\"),\n",
        "    \"lr\": AverageMeter(\"lr\", \":0.1e\"),\n",
        "    \"update_s\": AverageMeter(\"updt_s\", \":.3f\"),\n",
        "    \"dataloading_s\": AverageMeter(\"data_s\", \":.3f\"),\n",
        "}\n",
        "\n",
        "effective_batch_size = cfg.batch_size * accelerator.num_processes\n",
        "train_tracker = MetricsTracker(\n",
        "    effective_batch_size,\n",
        "    dataset.num_frames,\n",
        "    dataset.num_episodes,\n",
        "    train_metrics,\n",
        "    initial_step=step,\n",
        "    accelerator=accelerator,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if is_main_process:\n",
        "    logger.info(\"Start offline training on a fixed dataset\")\n",
        "    logger.info(f\"Train episodes: {len(train_episodes)}\")\n",
        "\n",
        "for _ in range(step, cfg.steps):\n",
        "    start_time = time.perf_counter()\n",
        "    batch = next(dl_iter)\n",
        "    batch = preprocessor(batch)\n",
        "    train_tracker.dataloading_s = time.perf_counter() - start_time\n",
        "\n",
        "    train_tracker, output_dict = update_policy(\n",
        "        train_tracker,\n",
        "        policy,\n",
        "        batch,\n",
        "        optimizer,\n",
        "        cfg.optimizer.grad_clip_norm,\n",
        "        accelerator=accelerator,\n",
        "        lr_scheduler=lr_scheduler,\n",
        "    )\n",
        "\n",
        "    step += 1\n",
        "    train_tracker.step()\n",
        "    \n",
        "    # Log to terminal at log_freq intervals\n",
        "    is_log_step = cfg.log_freq > 0 and step % cfg.log_freq == 0 and is_main_process\n",
        "    if is_log_step:\n",
        "        logger.info(train_tracker)\n",
        "        train_tracker.reset_averages()\n",
        "\n",
        "if is_main_process:\n",
        "    logger.info(\"End of training\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean up\n",
        "accelerator.wait_for_everyone()\n",
        "accelerator.end_training()"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
