{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "904efc82",
   "metadata": {},
   "source": [
    "AR Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25fb0b3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.models.gemma import modeling_gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc4487a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b31d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Gemma model and tokenizer\n",
    "model_name = \"google/gemma-2b\"  # or \"google/gemma-2-2b-it\" for instruction-tuned\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16 if device == \"cuda\" else torch.float32,\n",
    "    device_map=device,\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb67e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_att_2d_masks(pad_masks, att_masks):\n",
    "    \"\"\"\n",
    "    Tokens can attend to valid inputs tokens which have a cumulative mask_ar\n",
    "    smaller or equal to theirs. This way `mask_ar` int[B, N] can be used to\n",
    "    setup several types of attention, for example:\n",
    "\n",
    "      [[1 1 1 1 1 1]]: pure causal attention.\n",
    "\n",
    "      [[0 0 0 1 1 1]]: prefix-lm attention. The first 3 tokens can attend\n",
    "        between themselves and the last 3 tokens have a causal attention.\n",
    "\n",
    "    Args:\n",
    "        pad_masks: bool[B, N] true if its part of the input, false if padding.\n",
    "        att_masks: int[B, N] mask that's 1 where previous tokens\n",
    "            cannot depend on it and 0 where it shares the same\n",
    "            attention mask as the previous token.\n",
    "\n",
    "    Returns:\n",
    "        att_2d_masks: bool[B, N, N] 2D attention mask\n",
    "    \"\"\"\n",
    "    if att_masks.ndim != 2:\n",
    "        raise ValueError(f\"att_masks must be 2D, got {att_masks.ndim}D\")\n",
    "    if pad_masks.ndim != 2:\n",
    "        raise ValueError(f\"pad_masks must be 2D, got {pad_masks.ndim}D\")\n",
    "\n",
    "    # cumsum shape: (B, N)\n",
    "    cumsum = torch.cumsum(att_masks, dim=1)\n",
    "    # att_2d_masks shape: (B, N, N)\n",
    "    att_2d_masks = cumsum[:, None, :] <= cumsum[:, :, None]\n",
    "    # pad_2d_masks shape: (B, N, N)\n",
    "    pad_2d_masks = pad_masks[:, None, :] * pad_masks[:, :, None]\n",
    "    # att_2d_masks & pad_2d_masks shape: (B, N, N)\n",
    "    return att_2d_masks & pad_2d_masks\n",
    "\n",
    "\n",
    "def prepare_attention_masks_4d(att_2d_masks):\n",
    "    \"\"\"\n",
    "    Helper method to prepare 4D attention masks for transformer.\n",
    "    \"\"\"\n",
    "    OPENPI_ATTENTION_MASK_VALUE = -1e9\n",
    "    att_2d_masks_4d = att_2d_masks[:, None, :, :]\n",
    "    return torch.where(att_2d_masks_4d, 0.0, OPENPI_ATTENTION_MASK_VALUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1786c65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_autoregressive(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt: str,\n",
    "    max_decoding_steps: int = 100,\n",
    "    eos_token_id: Optional[int] = None,\n",
    "    temperature: float = 0.7,\n",
    "    device: str = \"cuda\",\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate tokens autoregressively from a Gemma model.\n",
    "    \n",
    "    Following the pattern from modeling_pi05ki.py sample_subtask method.\n",
    "    \n",
    "    Args:\n",
    "        model: Gemma model instance\n",
    "        tokenizer: Tokenizer instance\n",
    "        prompt: Input text prompt\n",
    "        max_decoding_steps: Maximum number of tokens to generate\n",
    "        eos_token_id: End-of-sequence token ID (defaults to tokenizer.eos_token_id)\n",
    "        temperature: Sampling temperature (0.0 for greedy, >0 for sampling)\n",
    "        device: Device to run on\n",
    "    \n",
    "    Returns:\n",
    "        Generated text string\n",
    "    \"\"\"\n",
    "    if eos_token_id is None:\n",
    "        eos_token_id = tokenizer.eos_token_id if tokenizer.eos_token_id is not None else 1\n",
    "    \n",
    "    # Tokenize prompt\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "    batch_size, seq_len = input_ids.shape\n",
    "    \n",
    "    # Create attention masks (all True for valid tokens)\n",
    "    pad_masks = torch.ones_like(input_ids, dtype=torch.bool)\n",
    "    # Causal attention mask (all zeros for causal attention)\n",
    "    att_masks = torch.zeros_like(input_ids, dtype=torch.bool)\n",
    "    \n",
    "    # Create 2D attention masks\n",
    "    att_2d_masks = make_att_2d_masks(pad_masks, att_masks)\n",
    "    \n",
    "    # Compute position IDs\n",
    "    position_ids = torch.cumsum(pad_masks, dim=1) - 1\n",
    "    \n",
    "    # Convert to 4D format expected by the model\n",
    "    att_2d_masks_4d = prepare_attention_masks_4d(att_2d_masks)\n",
    "    \n",
    "    # Set attention implementation to eager (for compatibility)\n",
    "    model.config._attn_implementation = \"eager\"\n",
    "    \n",
    "    # Get embeddings\n",
    "    inputs_embeds = model.model.embed_tokens(input_ids)\n",
    "    \n",
    "    # Initial forward pass to get past_key_values\n",
    "    outputs = model.model(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=att_2d_masks_4d,\n",
    "        position_ids=position_ids,\n",
    "        past_key_values=None,\n",
    "        use_cache=True,\n",
    "    )\n",
    "    \n",
    "    past_key_values = outputs.past_key_values\n",
    "    last_hidden_state = outputs.last_hidden_state\n",
    "    \n",
    "    # Extract last token embedding: (B, embd_dim)\n",
    "    last_token_embed = last_hidden_state[:, -1, :]\n",
    "    \n",
    "    # Convert to logits: (B, vocab_size)\n",
    "    last_logits = model.lm_head(last_token_embed)\n",
    "    \n",
    "    # Track valid length for position IDs\n",
    "    prefix_valid_length = torch.sum(pad_masks, dim=1)  # (B,)\n",
    "    \n",
    "    # Initialize output tokens\n",
    "    output_tokens = torch.zeros((batch_size, max_decoding_steps),\n",
    "                                dtype=torch.long, device=device)\n",
    "    all_eos = torch.zeros(batch_size, dtype=torch.bool, device=device)\n",
    "    \n",
    "    # Running attention mask (will grow as we generate)\n",
    "    running_attention_mask = pad_masks.clone()\n",
    "    \n",
    "    # Autoregressive loop\n",
    "    for step in range(max_decoding_steps):\n",
    "        # Sample next token\n",
    "        if temperature > 0.0:\n",
    "            probs = F.softmax(last_logits / temperature, dim=-1)\n",
    "            token = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "        else:\n",
    "            token = torch.argmax(last_logits, dim=-1, keepdim=True)  # (B, 1)\n",
    "        \n",
    "        output_tokens[:, step] = token.squeeze(-1)\n",
    "        \n",
    "        # Check for EOS\n",
    "        all_eos |= (token.squeeze(-1) == eos_token_id)\n",
    "        if all_eos.all():\n",
    "            break\n",
    "        \n",
    "        # Embed the new token\n",
    "        next_token_embeds = model.model.embed_tokens(token)  # (B, 1, embd_dim)\n",
    "        \n",
    "        # Create position IDs for the new token\n",
    "        position_ids = prefix_valid_length[:, None] + step + 1\n",
    "        \n",
    "        # Create attention mask for the new token\n",
    "        new_mask = torch.ones(\n",
    "            (batch_size, 1),\n",
    "            dtype=running_attention_mask.dtype,\n",
    "            device=device\n",
    "        )\n",
    "        running_attention_mask = torch.cat([running_attention_mask, new_mask], dim=1)\n",
    "        \n",
    "        # Create 2D attention mask for the extended sequence\n",
    "        extended_att_masks = torch.zeros_like(running_attention_mask, dtype=torch.bool)\n",
    "        extended_att_2d_masks = make_att_2d_masks(running_attention_mask, extended_att_masks)\n",
    "        extended_att_2d_masks_4d = prepare_attention_masks_4d(extended_att_2d_masks)\n",
    "        \n",
    "        # Forward pass with past_key_values\n",
    "        outputs = model.model(\n",
    "            inputs_embeds=next_token_embeds,\n",
    "            attention_mask=extended_att_2d_masks_4d,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=True,\n",
    "        )\n",
    "        \n",
    "        past_key_values = outputs.past_key_values\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        \n",
    "        # Extract last token embedding\n",
    "        last_token_embed = last_hidden_state[:, -1, :]\n",
    "        \n",
    "        # Get next logits\n",
    "        last_logits = model.lm_head(last_token_embed)\n",
    "    \n",
    "    # Decode generated tokens\n",
    "    generated_ids = output_tokens\n",
    "    # Filter out padding (0) and EOS tokens for cleaner output\n",
    "    generated_text = tokenizer.decode(\n",
    "        generated_ids[0].tolist(),\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81695542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "prompt = \"The capital of France is\"\n",
    "print(f\"Prompt: {prompt}\")\n",
    "\n",
    "generated_text = generate_autoregressive(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompt=prompt,\n",
    "    max_decoding_steps=50,\n",
    "    temperature=0.7,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "print(f\"\\nGenerated: {generated_text}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
