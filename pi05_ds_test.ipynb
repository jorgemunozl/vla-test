{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PI05 Policy Test: Subtask Generation and Action Prediction\n",
        "\n",
        "This notebook tests the PI05 policy with subtask generation and action prediction, visualizing prompts, subtasks, and actions simulating the inference process with a dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import torch\n",
        "from lerobot.datasets.lerobot_dataset import LeRobotDataset\n",
        "from lerobot.processor.core import TransitionKey, EnvTransition\n",
        "from lerobot.utils.constants import OBS_LANGUAGE_TOKENS\n",
        "from transformers import AutoTokenizer\n",
        "from xhuman.policies.pi05.processor_pi05 import make_pi05_pre_post_processors_ki\n",
        "from xhuman.policies.factory import make_xhuman_policy\n",
        "from xhuman.policies.pi05.configuration_pi05 import PI05Config\n",
        "\n",
        "print(\"‚úì Imports loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Constants\n",
        "DS_ID = \"NONHUMAN-RESEARCH/TEST_RECORD_ANNOTATIONS\"\n",
        "PRETRAINED_PATH = \"lerobot/pi05_base\"\n",
        "TOKENIZER_NAME = \"google/paligemma-3b-pt-224\"\n",
        "\n",
        "print(f\"Dataset ID: {DS_ID}\")\n",
        "print(f\"Pretrained path: {PRETRAINED_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper Functions\n",
        "def decode_tokens(tokens: torch.Tensor, tokenizer) -> str:\n",
        "    \"\"\"Decode tokens to visualize the prompt.\"\"\"\n",
        "    if tokens.dim() == 2:\n",
        "        tokens = tokens[0]  # Take first sample from batch\n",
        "    \n",
        "    # Remove padding (token id 0)\n",
        "    tokens = tokens[tokens != 0]\n",
        "    \n",
        "    return tokenizer.decode(tokens, skip_special_tokens=False)\n",
        "\n",
        "\n",
        "def visualize_prompt(batch: dict, tokenizer, step: int, prompt_type: str):\n",
        "    \"\"\"Visualize the prompt being sent to the model.\"\"\"\n",
        "    tokens = batch[OBS_LANGUAGE_TOKENS]\n",
        "    decoded = decode_tokens(tokens, tokenizer)\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Step {step}: {prompt_type}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Prompt: {decoded}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "print(\"‚úì Helper functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "subtask_prediction_frequency = 50  # Generate subtask every N steps\n",
        "max_steps = 20  # Test for N steps\n",
        "episode_index = 0  # Which episode to test\n",
        "\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Subtask prediction frequency: {subtask_prediction_frequency}\")\n",
        "print(f\"Max steps: {max_steps}\")\n",
        "print(f\"Episode index: {episode_index}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Dataset\n",
        "print(\"Loading dataset...\")\n",
        "dataset = LeRobotDataset(DS_ID)\n",
        "print(f\"‚úì Dataset loaded: {len(dataset)} samples\")\n",
        "print(f\"Features: {list(dataset.features.keys())}\")\n",
        "\n",
        "# Get a sample episode\n",
        "episode_data = dataset[episode_index]\n",
        "print(f\"\\n‚úì Episode {episode_index} loaded\")\n",
        "print(f\"Episode keys: {list(episode_data.keys())}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Policy Config\n",
        "print(\"Creating policy config...\")\n",
        "policy_config = PI05Config(\n",
        "    pretrained_path=PRETRAINED_PATH,  # Set pretrained path so factory loads weights\n",
        "    device=device,\n",
        ")\n",
        "print(f\"‚úì Config created\")\n",
        "print(f\"  - Type: {policy_config.type}\")\n",
        "print(f\"  - Device: {policy_config.device}\")\n",
        "print(f\"  - Chunk size: {policy_config.chunk_size}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Policy (factory populates input_features/output_features from dataset)\n",
        "print(\"Loading policy from pretrained...\")\n",
        "policy = make_xhuman_policy(\n",
        "    cfg=policy_config,\n",
        "    ds_meta=dataset.meta,\n",
        ")\n",
        "\n",
        "print(f\"‚úì Policy loaded: {policy.name}\")\n",
        "print(f\"  - Input features: {list(policy.config.input_features.keys())}\")\n",
        "print(f\"  - Output features: {list(policy.config.output_features.keys())}\")\n",
        "action_dim = policy.config.output_features['action'].shape[0]\n",
        "print(f\"  - Action dimension: {action_dim}\")\n",
        "print(f\"  - Chunk size: {policy.config.chunk_size}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Preprocessor and Postprocessor\n",
        "# Note: policy.config now has input_features and output_features set\n",
        "preprocessor, postprocessor = make_pi05_pre_post_processors_ki(\n",
        "    policy.config,\n",
        "    dataset_stats=dataset.meta.stats,  # Important: pass dataset stats for normalization (use .meta.stats)\n",
        ")\n",
        "\n",
        "print(\"‚úì Preprocessor and postprocessor created\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Tokenizer for Visualization\n",
        "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)\n",
        "print(f\"‚úì Tokenizer loaded: {TOKENIZER_NAME}\")\n",
        "\n",
        "# Get task from dataset metadata\n",
        "task = (\n",
        "    dataset.tasks[0]\n",
        "    if hasattr(dataset, 'tasks') and len(dataset.tasks) > 0\n",
        "    else \"pick up object\"\n",
        ")\n",
        "print(f\"‚úì Task: {task}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference Loop\n",
        "\n",
        "The loop simulates the inference process:\n",
        "- Every `subtask_prediction_frequency` steps: generate a new subtask\n",
        "- Every step: generate actions using the cached subtask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize inference loop\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STARTING INFERENCE LOOP\")\n",
        "print(\"=\"*80 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Single step example - you can modify time_index to test specific steps\n",
        "time_index = 0  # Change this to test different time steps\n",
        "\n",
        "print(f\"\\n{'‚îÄ'*80}\")\n",
        "print(f\"TIME INDEX: {time_index}\")\n",
        "print(f\"{'‚îÄ'*80}\")\n",
        "\n",
        "# Check what input features the policy expects\n",
        "print(f\"Policy input features: {list(policy.config.input_features.keys())}\")\n",
        "\n",
        "# Prepare observation - ensure tensors are properly formatted\n",
        "# Get raw data and convert to tensors if needed\n",
        "img_data = episode_data[\"observation.images.top\"][time_index:time_index+1]\n",
        "state_data = episode_data[\"observation.state\"][time_index:time_index+1]\n",
        "\n",
        "# Convert to tensors if they're numpy arrays\n",
        "if not isinstance(img_data, torch.Tensor):\n",
        "    img_data = torch.from_numpy(img_data)\n",
        "if not isinstance(state_data, torch.Tensor):\n",
        "    state_data = torch.from_numpy(state_data)\n",
        "\n",
        "# Ensure proper dtype\n",
        "if img_data.dtype != torch.float32:\n",
        "    img_data = img_data.float()\n",
        "if state_data.dtype != torch.float32:\n",
        "    state_data = state_data.float()\n",
        "\n",
        "# Build observation dict with keys matching policy input_features\n",
        "obs = {}\n",
        "for key in policy.config.input_features.keys():\n",
        "    if key == \"observation.images.top\":\n",
        "        obs[key] = img_data\n",
        "    elif key == \"observation.state\":\n",
        "        obs[key] = state_data\n",
        "    else:\n",
        "        # For other keys, try to get from episode_data or use defaults\n",
        "        if key in episode_data:\n",
        "            val = episode_data[key][time_index:time_index+1]\n",
        "            if not isinstance(val, torch.Tensor):\n",
        "                val = torch.from_numpy(val) if hasattr(val, '__array__') else torch.tensor(val)\n",
        "            obs[key] = val\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è  Warning: Missing observation key '{key}' in episode_data\")\n",
        "\n",
        "# Add complementary data\n",
        "complementary_data = {\n",
        "    \"task\": task,\n",
        "    \"time_index\": time_index,\n",
        "    \"subtask\": policy.cached_subtask,  # Use cached subtask\n",
        "}\n",
        "\n",
        "print(f\"‚úì Observation prepared for time_index={time_index}\")\n",
        "print(f\"  - Observation keys: {list(obs.keys())}\")\n",
        "for key, val in obs.items():\n",
        "    if isinstance(val, torch.Tensor):\n",
        "        print(f\"  - {key}: shape={val.shape}, dtype={val.dtype}\")\n",
        "    else:\n",
        "        print(f\"  - {key}: {type(val)}\")\n",
        "print(f\"  - Current cached subtask: '{policy.cached_subtask}'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if we should generate subtask\n",
        "should_generate_subtask = (\n",
        "    subtask_prediction_frequency > 0\n",
        "    and time_index % subtask_prediction_frequency == 0\n",
        ")\n",
        "\n",
        "if should_generate_subtask:\n",
        "    print(f\"üîÑ GENERATING NEW SUBTASK at step {time_index}\")\n",
        "    \n",
        "    # Prepare batch for subtask generation\n",
        "    # The processor will create prompt: \"Task: X. Subtask: \"\n",
        "    obs_subtask = {**obs}  # Copy observation dict\n",
        "    complementary_data_subtask = {\n",
        "        \"task\": task,\n",
        "        \"time_index\": time_index,\n",
        "        \"subtask\": None,  # Force subtask generation prompt\n",
        "    }\n",
        "    \n",
        "    # Create transition as EnvTransition (dict with TransitionKey)\n",
        "    # The preprocessor expects this structure\n",
        "    transition_subtask: EnvTransition = {\n",
        "        TransitionKey.OBSERVATION: obs_subtask,\n",
        "        TransitionKey.COMPLEMENTARY_DATA: complementary_data_subtask,\n",
        "    }\n",
        "    \n",
        "    print(f\"  - Transition type: {type(transition_subtask)}\")\n",
        "    print(f\"  - Transition keys: {list(transition_subtask.keys())}\")\n",
        "    print(f\"  - Observation keys: {list(transition_subtask[TransitionKey.OBSERVATION].keys())}\")\n",
        "    print(f\"  - Complementary data keys: {list(transition_subtask[TransitionKey.COMPLEMENTARY_DATA].keys())}\")\n",
        "    \n",
        "    # Verify observations are present\n",
        "    if TransitionKey.OBSERVATION not in transition_subtask:\n",
        "        raise ValueError(\"Transition missing OBSERVATION key\")\n",
        "    if not transition_subtask[TransitionKey.OBSERVATION]:\n",
        "        raise ValueError(\"Transition OBSERVATION is empty\")\n",
        "    \n",
        "    # Preprocess\n",
        "    try:\n",
        "        batch_subtask = preprocessor(transition_subtask)\n",
        "        \n",
        "        # Visualize subtask generation prompt\n",
        "        visualize_prompt(batch_subtask, tokenizer, time_index, \"SUBTASK GENERATION\")\n",
        "        \n",
        "        # Generate subtask\n",
        "        policy.update_subtask(batch_subtask)\n",
        "        \n",
        "        print(f\"‚úÖ Generated subtask: '{policy.cached_subtask}'\")\n",
        "        \n",
        "        # Update complementary data with new subtask\n",
        "        complementary_data[\"subtask\"] = policy.cached_subtask\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error in subtask generation: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        raise\n",
        "else:\n",
        "    print(f\"‚è≠Ô∏è  Skipping subtask generation (frequency={subtask_prediction_frequency})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate Actions\n",
        "print(f\"\\nüìç GENERATING ACTIONS at step {time_index}\")\n",
        "print(f\"   Using cached subtask: '{policy.cached_subtask}'\")\n",
        "\n",
        "# Prepare batch for action generation\n",
        "# Processor creates: \"Task: X. Subtask: Y, State: Z;\\nAction: \"\n",
        "transition_action: EnvTransition = {\n",
        "    TransitionKey.OBSERVATION: obs,\n",
        "    TransitionKey.COMPLEMENTARY_DATA: complementary_data,\n",
        "}\n",
        "\n",
        "print(f\"  - Transition type: {type(transition_action)}\")\n",
        "print(f\"  - Transition keys: {list(transition_action.keys())}\")\n",
        "print(f\"  - Observation keys: {list(transition_action[TransitionKey.OBSERVATION].keys())}\")\n",
        "print(f\"  - Complementary data keys: {list(transition_action[TransitionKey.COMPLEMENTARY_DATA].keys())}\")\n",
        "\n",
        "# Verify observations are present\n",
        "if TransitionKey.OBSERVATION not in transition_action:\n",
        "    raise ValueError(\"Transition missing OBSERVATION key\")\n",
        "if not transition_action[TransitionKey.OBSERVATION]:\n",
        "    raise ValueError(\"Transition OBSERVATION is empty\")\n",
        "\n",
        "# Preprocess\n",
        "try:\n",
        "    batch_action = preprocessor(transition_action)\n",
        "    \n",
        "    # Visualize action generation prompt\n",
        "    visualize_prompt(batch_action, tokenizer, time_index, \"ACTION GENERATION\")\n",
        "    \n",
        "    # Generate actions\n",
        "    actions = policy.predict_action_chunk(batch_action)\n",
        "    \n",
        "    print(f\"‚úÖ Generated actions (chunk_size={policy.config.chunk_size})\")\n",
        "    print(f\"   Actions shape: {actions.shape}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error in action generation: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize Actions (run this after cell 14)\n",
        "if 'actions' in locals():\n",
        "    actions_np = actions[0].cpu().numpy()  # First batch\n",
        "    \n",
        "    # Show first 3 actions in the chunk\n",
        "    num_to_show = min(3, len(actions_np))\n",
        "    for i in range(num_to_show):\n",
        "        action_str = \", \".join([f\"{x:.3f}\" for x in actions_np[i]])\n",
        "        print(f\"   Action[{i}]: [{action_str}]\")\n",
        "    \n",
        "    if len(actions_np) > num_to_show:\n",
        "        remaining = len(actions_np) - num_to_show\n",
        "        print(f\"   ... and {remaining} more actions\")\n",
        "    \n",
        "    # Show action statistics\n",
        "    print(\"\\n   Action stats:\")\n",
        "    print(f\"   - Mean: {actions_np.mean():.3f}\")\n",
        "    print(f\"   - Std:  {actions_np.std():.3f}\")\n",
        "    print(f\"   - Min:  {actions_np.min():.3f}\")\n",
        "    print(f\"   - Max:  {actions_np.max():.3f}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Run cell 14 first to generate actions\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
