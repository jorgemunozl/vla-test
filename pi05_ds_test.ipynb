{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PI05 Policy Test: Subtask Generation and Action Prediction\n",
        "\n",
        "This notebook tests the PI05 policy with subtask generation and action prediction, visualizing prompts, subtasks, and actions simulating the inference process with a dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import torch\n",
        "from lerobot.datasets.lerobot_dataset import LeRobotDataset\n",
        "from lerobot.processor.core import TransitionKey\n",
        "from lerobot.utils.constants import OBS_LANGUAGE_TOKENS\n",
        "from transformers import AutoTokenizer\n",
        "from xhuman.policies.pi05.processor_pi05 import make_pi05_pre_post_processors_ki\n",
        "from xhuman.policies.factory import make_xhuman_policy\n",
        "from xhuman.policies.pi05.configuration_pi05 import PI05Config\n",
        "\n",
        "print(\"‚úì Imports loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Constants\n",
        "DS_ID = \"NONHUMAN-RESEARCH/TEST_RECORD_ANNOTATIONS\"\n",
        "PRETRAINED_PATH = \"lerobot/pi05_base\"\n",
        "TOKENIZER_NAME = \"google/paligemma-3b-pt-224\"\n",
        "\n",
        "print(f\"Dataset ID: {DS_ID}\")\n",
        "print(f\"Pretrained path: {PRETRAINED_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper Functions\n",
        "def decode_tokens(tokens: torch.Tensor, tokenizer) -> str:\n",
        "    \"\"\"Decode tokens to visualize the prompt.\"\"\"\n",
        "    if tokens.dim() == 2:\n",
        "        tokens = tokens[0]  # Take first sample from batch\n",
        "    \n",
        "    # Remove padding (token id 0)\n",
        "    tokens = tokens[tokens != 0]\n",
        "    \n",
        "    return tokenizer.decode(tokens, skip_special_tokens=False)\n",
        "\n",
        "\n",
        "def visualize_prompt(batch: dict, tokenizer, step: int, prompt_type: str):\n",
        "    \"\"\"Visualize the prompt being sent to the model.\"\"\"\n",
        "    tokens = batch[OBS_LANGUAGE_TOKENS]\n",
        "    decoded = decode_tokens(tokens, tokenizer)\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Step {step}: {prompt_type}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Prompt: {decoded}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "print(\"‚úì Helper functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "subtask_prediction_frequency = 50  # Generate subtask every N steps\n",
        "max_steps = 20  # Test for N steps\n",
        "episode_index = 0  # Which episode to test\n",
        "\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Subtask prediction frequency: {subtask_prediction_frequency}\")\n",
        "print(f\"Max steps: {max_steps}\")\n",
        "print(f\"Episode index: {episode_index}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Dataset\n",
        "print(\"Loading dataset...\")\n",
        "dataset = LeRobotDataset(DS_ID)\n",
        "print(f\"‚úì Dataset loaded: {len(dataset)} samples\")\n",
        "print(f\"Features: {list(dataset.features.keys())}\")\n",
        "\n",
        "# Get a sample episode\n",
        "episode_data = dataset[episode_index]\n",
        "print(f\"\\n‚úì Episode {episode_index} loaded\")\n",
        "print(f\"Episode keys: {list(episode_data.keys())}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Policy Config\n",
        "print(\"Creating policy config...\")\n",
        "policy_config = PI05Config(\n",
        "    pretrained_path=PRETRAINED_PATH,  # Set pretrained path so factory loads weights\n",
        "    device=device,\n",
        ")\n",
        "print(f\"‚úì Config created\")\n",
        "print(f\"  - Type: {policy_config.type}\")\n",
        "print(f\"  - Device: {policy_config.device}\")\n",
        "print(f\"  - Chunk size: {policy_config.chunk_size}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Policy (factory populates input_features/output_features from dataset)\n",
        "print(\"Loading policy from pretrained...\")\n",
        "policy = make_xhuman_policy(\n",
        "    cfg=policy_config,\n",
        "    ds_meta=dataset.meta,\n",
        ")\n",
        "\n",
        "print(f\"‚úì Policy loaded: {policy.name}\")\n",
        "print(f\"  - Input features: {list(policy.config.input_features.keys())}\")\n",
        "print(f\"  - Output features: {list(policy.config.output_features.keys())}\")\n",
        "action_dim = policy.config.output_features['action'].shape[0]\n",
        "print(f\"  - Action dimension: {action_dim}\")\n",
        "print(f\"  - Chunk size: {policy.config.chunk_size}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Preprocessor and Postprocessor\n",
        "# Note: policy.config now has input_features and output_features set\n",
        "preprocessor, postprocessor = make_pi05_pre_post_processors_ki(\n",
        "    policy.config,\n",
        "    dataset_stats=dataset.stats,  # Important: pass dataset stats for normalization\n",
        ")\n",
        "\n",
        "print(\"‚úì Preprocessor and postprocessor created\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Tokenizer for Visualization\n",
        "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)\n",
        "print(f\"‚úì Tokenizer loaded: {TOKENIZER_NAME}\")\n",
        "\n",
        "# Get task from dataset metadata\n",
        "task = (\n",
        "    dataset.tasks[0]\n",
        "    if hasattr(dataset, 'tasks') and len(dataset.tasks) > 0\n",
        "    else \"pick up object\"\n",
        ")\n",
        "print(f\"‚úì Task: {task}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference Loop\n",
        "\n",
        "The loop simulates the inference process:\n",
        "- Every `subtask_prediction_frequency` steps: generate a new subtask\n",
        "- Every step: generate actions using the cached subtask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize inference loop\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STARTING INFERENCE LOOP\")\n",
        "print(\"=\"*80 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Single step example - you can modify time_index to test specific steps\n",
        "time_index = 0  # Change this to test different time steps\n",
        "\n",
        "print(f\"\\n{'‚îÄ'*80}\")\n",
        "print(f\"TIME INDEX: {time_index}\")\n",
        "print(f\"{'‚îÄ'*80}\")\n",
        "\n",
        "# Prepare observation\n",
        "obs = {\n",
        "    \"observation.images.top\": episode_data[\"observation.images.top\"][time_index:time_index+1],\n",
        "    \"observation.state\": episode_data[\"observation.state\"][time_index:time_index+1],\n",
        "}\n",
        "\n",
        "# Add complementary data\n",
        "complementary_data = {\n",
        "    \"task\": task,\n",
        "    \"time_index\": time_index,\n",
        "    \"subtask\": policy.cached_subtask,  # Use cached subtask\n",
        "}\n",
        "\n",
        "print(f\"‚úì Observation prepared for time_index={time_index}\")\n",
        "print(f\"  - Image shape: {obs['observation.images.top'].shape}\")\n",
        "print(f\"  - State shape: {obs['observation.state'].shape}\")\n",
        "print(f\"  - Current cached subtask: '{policy.cached_subtask}'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if we should generate subtask\n",
        "should_generate_subtask = (\n",
        "    subtask_prediction_frequency > 0\n",
        "    and time_index % subtask_prediction_frequency == 0\n",
        ")\n",
        "\n",
        "if should_generate_subtask:\n",
        "    print(f\"üîÑ GENERATING NEW SUBTASK at step {time_index}\")\n",
        "    \n",
        "    # Prepare batch for subtask generation\n",
        "    # The processor will create prompt: \"Task: X. Subtask: \"\n",
        "    obs_subtask = {**obs}\n",
        "    complementary_data_subtask = {\n",
        "        \"task\": task,\n",
        "        \"time_index\": time_index,\n",
        "        \"subtask\": None,  # Force subtask generation prompt\n",
        "    }\n",
        "    \n",
        "    # Note: The preprocessor expects transition format\n",
        "    transition_subtask = {\n",
        "        TransitionKey.OBSERVATION: obs_subtask,\n",
        "        TransitionKey.COMPLEMENTARY_DATA: complementary_data_subtask,\n",
        "    }\n",
        "    \n",
        "    # Preprocess\n",
        "    batch_subtask = preprocessor(transition_subtask)\n",
        "    \n",
        "    # Visualize subtask generation prompt\n",
        "    visualize_prompt(batch_subtask, tokenizer, time_index, \"SUBTASK GENERATION\")\n",
        "    \n",
        "    # Generate subtask\n",
        "    policy.update_subtask(batch_subtask)\n",
        "    \n",
        "    print(f\"‚úÖ Generated subtask: '{policy.cached_subtask}'\")\n",
        "    \n",
        "    # Update complementary data with new subtask\n",
        "    complementary_data[\"subtask\"] = policy.cached_subtask\n",
        "else:\n",
        "    print(f\"‚è≠Ô∏è  Skipping subtask generation (frequency={subtask_prediction_frequency})\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
